<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Mono3D Detection 整理 | Untitled.</title>

  
  <meta name="author" content="Jingyi Yu">
  

  
  <meta name="description" content="本文整理了单目3D目标检测的相关工作">
  

  
  
  <meta name="keywords" content="Mono3D">
  

  <meta id="viewport" name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">

  <meta property="og:title" content="Mono3D Detection 整理"/>

  <meta property="og:site_name" content="Untitled."/>

  
  <meta property="og:image" content="/favicon.ico"/>
  

  <link href="/favicon.ico" rel="icon">
  <link rel="alternate" href="/atom.xml" title="Untitled." type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
<div class="blog">
  <div class="content">

    <header>
  <div class="site-branding">
    <h1 class="site-title">
      <a href="/">Untitled.</a>
    </h1>
    <p class="site-description"></p>
  </div>
  <nav class="site-navigation">
    <ul>
      
    </ul>
  </nav>
</header>

    <main class="site-main posts-loop">
    <article>

  
    
    <h3 class="article-title"><span>Mono3D Detection 整理</span></h3>
    
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2022/12/12/Paper-Mono3D paper路线梳理/" rel="bookmark">
        <time class="entry-date published" datetime="2022-12-12T03:58:31.000Z">
          2022-12-12
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
        <p>本文整理了单目3D目标检测的相关工作</p>
<span id="more"></span>


<h1 id="0-Introduction-of-3D-Object-Detection"><a href="#0-Introduction-of-3D-Object-Detection" class="headerlink" title="0.Introduction of 3D Object Detection"></a>0.Introduction of 3D Object Detection</h1><p>在缺乏深度信息的情况下，根据RGB图像恢复3D结构是一个不适定问题。但是在提供先验信息的情况下，这个任务也能取得不错的效果。<br>在具体的方案实现上，总体可以分为四条路线：</p>
<ol>
<li>Proposal-based的方案，先生成3D proposal ，再根据设定的方案对proposal进行排序，从而得到最后的结果</li>
<li>RGB-only的方案，将3D框的各组成项解耦，直接从RGB中回归各组成项再组合成3D框</li>
<li>Perspective transformation的方案，将原始的输入图像换一种表达形式（此处不包含借助depth进行3D结构恢复的方案）</li>
<li>Pseudo-LiDAR的方案 ，本质上都是在RGB图像上进行深度估计，恢复3D结构，然后在视觉点云上进行3D检测</li>
</ol>
<p>以下将一些代表性的论文粗略分为上述四个类别，按照时间线进行简单的回顾。</p>
<h1 id="1-Proposal-based"><a href="#1-Proposal-based" class="headerlink" title="1.Proposal-based"></a>1.Proposal-based</h1><h2 id="2016–CVPR–Mono3D"><a href="#2016–CVPR–Mono3D" class="headerlink" title="2016–CVPR–Mono3D"></a>2016–CVPR–Mono3D</h2><p>[<a target="_blank" rel="noopener" href="https://www.cs.toronto.edu/~urtasun/publications/chen_etal_cvpr16.pdf">Paper</a>]     [Code]</p>
<blockquote>
<p>Mono3D first samples candidates based on the ground prior and scores them with semantic&#x2F;instance segmentation, contextual information, object shape, and location prior.<br>MonoFlex</p>
</blockquote>
<p>在3D空间中生成proposal，投影回2D后结合分割、形状、位置先验进行评分，最后保留的作为结果。</p>
<h2 id="2019–ICIP-Shift-RCNN"><a href="#2019–ICIP-Shift-RCNN" class="headerlink" title="2019–ICIP-Shift RCNN"></a>2019–ICIP-Shift RCNN</h2><p>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1905.09970.pdf">Paper</a>]     [Code]</p>
<blockquote>
<p>avoids dense proposal sampling by “actively” regressing the offset from the Deep3DBox proposal. They feed all the known 2D and 3D bbox info into a fast and simple fully connected network called ShiftNet and refine the 3D location</p>
</blockquote>
<h2 id="2019–ICCV–MonoDIS"><a href="#2019–ICCV–MonoDIS" class="headerlink" title="2019–ICCV–MonoDIS"></a>2019–ICCV–MonoDIS</h2><p>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1905.09970.pdf">Paper</a>]     [Code]</p>
<blockquote>
<p>Instead of directly supervising each of the components of 2D and 3D bbox output, it takes a holistic view of bbox regression and uses 2D (signed) IoU loss and 3D corner loss. These losses are hard to train in general, so it proposes a disentanglement technique by fixing all elements but one group (including one or more elements) to ground truth and calculate the loss, essentially training only parameters in that group.</p>
</blockquote>
<h2 id="2019–CVPR–MonoPSR"><a href="#2019–CVPR–MonoPSR" class="headerlink" title="2019–CVPR–MonoPSR"></a>2019–CVPR–MonoPSR</h2><blockquote>
<p>It generates 3D proposal first and then reconstructing local point cloud of dynamic objects.<br>The reconstruction branch regresses a local point cloud of the object and compares it with the GT in point cloud and camera (after projection).</p>
</blockquote>
<h2 id="2020–CVPR–D4LCN"><a href="#2020–CVPR–D4LCN" class="headerlink" title="2020–CVPR–D4LCN"></a>2020–CVPR–D4LCN</h2><blockquote>
<p>took the idea of depth-aware convolution from M3D-RPN even further by introducing a dynamic filter prediction branch. This additional branch which takes in the depth prediction as input and generates a filter feature volume, which generates different filters for each specific location in terms of both weights and dilation rates.</p>
</blockquote>
<h1 id="RGB-Image-only"><a href="#RGB-Image-only" class="headerlink" title="RGB Image only"></a>RGB Image only</h1><p>基于RGB图像直接得到3D框通常是通过借助关键点、目标形状、2D-3D的几何一致性约束等来进行3D框的解耦从而得到最后的结果。<br>关注的目标大尺寸大多有相似的尺寸，这个信息对于估计到目标的距离有很大的帮助。<br>很多方法延续2D的方法来预测关键点。</p>
<h2 id="2016–CVPR–Deep3DBox"><a href="#2016–CVPR–Deep3DBox" class="headerlink" title="2016–CVPR–Deep3DBox"></a>2016–CVPR–Deep3DBox</h2><p>对于3D框的回归，分成了三个分支。一个分支预测目标框的size相当于一类目标的均值的偏差，另外两个分支用于预测角度。角度的回归使用的是MultiBin的方法，预测每个bin的置信度以及角度的正弦和余弦。得到2D的结果和3D的结果后，使用最小二乘法通过最小化重投影误差计算目标框的位置。<br>缺点在于非常依赖于2D框的精度。<br><img src="https://cdn.nlark.com/yuque/0/2022/png/478741/1668950168695-bbc010be-f9ea-4d70-b406-27cbf1950c11.png#averageHue=%23f7f6e6&clientId=u9c117383-1941-4&from=paste&height=329&id=u1fd5cfaa&originHeight=657&originWidth=816&originalType=binary&ratio=1&rotation=0&showTitle=false&size=167856&status=done&style=none&taskId=ueb74e161-5352-4983-86d0-e65a9687d6e&title=&width=408" alt="image.png"></p>
<h2 id="2017–CVPR–Deep-MANTA"><a href="#2017–CVPR–Deep-MANTA" class="headerlink" title="2017–CVPR–Deep MANTA"></a>2017–CVPR–Deep MANTA</h2><p>本文是这一方案的先驱。首先使用级联的faster RCNN的架构来回归2D框、分类和模板的相似度。然后使用预先选择好的3D CAD模型，使用EpnP算法进行2D&#x2F;3D的匹配。</p>
<h2 id="2018–IROS–The-Earth-ain’t-Flat"><a href="#2018–IROS–The-Earth-ain’t-Flat" class="headerlink" title="2018–IROS–The Earth ain’t Flat"></a>2018–IROS–The Earth ain’t Flat</h2><p>本文目标在陡峭和分级的道路上对车辆进行单目重建。一个关键设计在于从单目图像中估计3D shape和6自由度。本文参考Deep MANTA进行3D model的匹配，但不是从所有可能的 3D 形状模板中挑选出最好的一个，而是使用基本向量和变形系数来捕捉车辆的形状。</p>
<h2 id="2019–CVPR–RoI-10D"><a href="#2019–CVPR–RoI-10D" class="headerlink" title="2019–CVPR–RoI-10D"></a>2019–CVPR–RoI-10D</h2><p>10D&#x3D;6DoF pose + 3DoF size + 1D shape space</p>
<h2 id="2019-AAAI-MonoGRNet"><a href="#2019-AAAI-MonoGRNet" class="headerlink" title="2019-AAAI-MonoGRNet"></a>2019-AAAI-MonoGRNet</h2><p>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1811.10247.pdf">Paper</a>]     [<a target="_blank" rel="noopener" href="https://github.com/Zengyi-Qin/MonoGRNet">Code</a>]<br>本文是由多个子网构成的统一网络，包括2D目标检测、实例深度估计、3D定位和局部角回归。<br>pixel-wise的深度估计最小化的是整张图像的所有像素得到一个平均最优估计，一些只占据了很小面积的目标就会被忽略了。<br>本文的核心思想是将3D定位问题解耦成几个循序渐进的子任务。<br>本文回归的是3D中心点的投影和粗糙的实例深度，然后使用两者估计粗糙的3D位置。它强调了2D框的中心点和3D框中心点在2D图像上的投影的不同。<br>本文没有直接回归更好观察的角度，而是回归了8个顶点相对于3D中心点的偏移。</p>
<h2 id="2019-ICCV-MVRA"><a href="#2019-ICCV-MVRA" class="headerlink" title="2019-ICCV-MVRA"></a>2019-ICCV-MVRA</h2><blockquote>
<p>It introduces a 3D reconstruction layer to lift 2D to 3D instead of solving an over-constrained equation, with two losses in two different spaces: 1) IoU loss in perspective view, between the reprojected 3D bbox and the 2d bbox in IoU, and 2) L2 loss in BEV loss between estimated distance and gt distance.<br>It recognizes that deep3DBox does not handle truncated box well, as not four sides of the bounding box now correspond to the real physical extent of the vehicle.</p>
</blockquote>
<h2 id="2019–CVPR–CenterNet–Objects-as-Points"><a href="#2019–CVPR–CenterNet–Objects-as-Points" class="headerlink" title="2019–CVPR–CenterNet–Objects as Points"></a>2019–CVPR–CenterNet–Objects as Points</h2><p>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1904.07850.pdf">Paper</a>]     [<a target="_blank" rel="noopener" href="https://github.com/xingyizhou/CenterNet">Code</a>]<br>本文认为许多方法列出全部潜在的目标框是一个计算冗余且不够高效的方案，本文将目标视作single point，利用关键点估计来回归每个目标框的中心点及其其他属性，如size、3D位置、朝向、甚至姿态。<br>在head部分的设计，是通过heatmap得到目标的位置，因此不需要NMS之类的后处理操作，也无需进行目标框的组合。对比anchor-based的方案来说，CenterNet要简单方便许多。</p>
<h2 id="GPP–Ground-Plane-Polling"><a href="#GPP–Ground-Plane-Polling" class="headerlink" title="GPP–Ground Plane Polling"></a>GPP–Ground Plane Polling</h2><p>本文使用3D框标注生成虚拟的2D关键点。</p>
<h2 id="2020–RTM3D–Real-time-Monocular-3D-Detection-from-Object-Keypoints-for-Autonomous-Driving"><a href="#2020–RTM3D–Real-time-Monocular-3D-Detection-from-Object-Keypoints-for-Autonomous-Driving" class="headerlink" title="2020–RTM3D–Real-time Monocular 3D Detection from Object Keypoints for Autonomous Driving"></a>2020–RTM3D–Real-time Monocular 3D Detection from Object Keypoints for Autonomous Driving</h2><blockquote>
<p>This article uses virtual keypoints and use CenterNet-like structure to directly detect the 2d projection of all 8 cuboid vertices + cuboid center. The paper also directly regresses distance, orientation, size. Instead of using these values to form cuboids directly, these values are used as the initial values (priors) to initialize the offline optimizer to generate 3D bboxes.</p>
</blockquote>
<h2 id="2020–CVPR–MonoPair–MonoPair-Monocular-3D-Object-Detection-Using-Pairwise-Spatial-Relationships"><a href="#2020–CVPR–MonoPair–MonoPair-Monocular-3D-Object-Detection-Using-Pairwise-Spatial-Relationships" class="headerlink" title="2020–CVPR–MonoPair–MonoPair: Monocular 3D Object Detection Using Pairwise Spatial Relationships"></a>2020–CVPR–MonoPair–MonoPair: Monocular 3D Object Detection Using Pairwise Spatial Relationships</h2><blockquote>
<p>MonoPair considers the pair-wise relationships between neighboring objects, which are utilized as spatial constraints to optimize the results of detection.</p>
</blockquote>
<p>有针对occluded目标进行处理<br>本文考虑了成对的目标之间的空间关系，加入了uncertainty的考虑。<br>Homography Loss for Monocular 3D Object Detection是本文的衍生方案，都使用了graph的方案，考虑了多目标之间的空间约束</p>
<h2 id="2020–CVPRW–SMOKE"><a href="#2020–CVPRW–SMOKE" class="headerlink" title="2020–CVPRW–SMOKE"></a>2020–CVPRW–SMOKE</h2><p>本文完全消除了 2D bbox 的回归并直接预测 3D bbox<br>显著降低了60米以内的distance error</p>
<h2 id="2020–AAAI–Monocular-3D-Object-Detection-with-Decoupled-Structured-Polygon-Estimation-and-Height-Guided-Depth-Estimation"><a href="#2020–AAAI–Monocular-3D-Object-Detection-with-Decoupled-Structured-Polygon-Estimation-and-Height-Guided-Depth-Estimation" class="headerlink" title="2020–AAAI–Monocular 3D Object Detection with Decoupled Structured Polygon Estimation and Height-Guided Depth Estimation"></a>2020–AAAI–Monocular 3D Object Detection with Decoupled Structured Polygon Estimation and Height-Guided Depth Estimation</h2><blockquote>
<p>The first work to clearly state that the estimation of the 2D projection of the 3D vertices is totally decoupled from the depth estimation.<br>It uses a similar method as RTM3D to regress the eight projected points of the cuboid, then uses vertical edge height as a strong prior to guide distance estimation. This generates a coarse 3D cuboid.</p>
</blockquote>
<h2 id="2021–CVPR–FCOS3D"><a href="#2021–CVPR–FCOS3D" class="headerlink" title="2021–CVPR–FCOS3D"></a>2021–CVPR–FCOS3D</h2><h1 id="Pesudo-LiDAR-and-BEV"><a href="#Pesudo-LiDAR-and-BEV" class="headerlink" title="Pesudo LiDAR and BEV"></a>Pesudo LiDAR and BEV</h1><p>在透视视角下，对于纯视觉的方案存在着遮挡和尺度变化的挑战。一些方法通过转换输入图像的表示形式来应对这个问题。<br>基于Pesudo-LiDAR和BEV的方案实际上都是将输入图像换了一种表示方法，近年来的主流BEV方法也多是借助Pseudo LiDAR的思想再过渡到BEV的表示方式，因此此处合并在一个类别下。</p>
<h2 id="BEV-without-Pseudo-LiDAR"><a href="#BEV-without-Pseudo-LiDAR" class="headerlink" title="BEV without Pseudo LiDAR"></a>BEV without Pseudo LiDAR</h2><p>BEV的表示下，不同的车辆之间不会出现重叠的现象。过去常使用IPM的方式得到BEV的图像，但是这种方案假设所有的像素都在地面上，并且需要获得相机精准的内外参。对于在路上行进的车辆而已，外参是实时变化的，其精度可能无法达到IPM的要求。</p>
<h3 id="2019–IV–Deep-Learning-based-Vehicle-Position-and-Orientation-Estimation-via-Inverse-Perspective-Mapping-Image"><a href="#2019–IV–Deep-Learning-based-Vehicle-Position-and-Orientation-Estimation-via-Inverse-Perspective-Mapping-Image" class="headerlink" title="2019–IV–Deep Learning based Vehicle Position and Orientation Estimation via Inverse Perspective Mapping Image"></a>2019–IV–Deep Learning based Vehicle Position and Orientation Estimation via Inverse Perspective Mapping Image</h3><p>这篇文章使用IMU数据来进行外参的在线标定从而得到更精确的IPM图像，然后在此之上进行目标检测。</p>
<h3 id="2019–BMVC–OFT–Orthographic-Feature-Transform-for-Monocular-3D-Object-Detection"><a href="#2019–BMVC–OFT–Orthographic-Feature-Transform-for-Monocular-3D-Object-Detection" class="headerlink" title="2019–BMVC–OFT–Orthographic Feature Transform for Monocular 3D Object Detection"></a>2019–BMVC–OFT–Orthographic Feature Transform for Monocular 3D Object Detection</h3><p>本文是将透视图转到BEV视角的另一种方式。其思想是利用正交特征变换来将透视视角的图像特征转到正交BEV下。然后通过在投影的voxel area上累加图像特征从而得到voxel-based的特征，再沿着垂直维度折叠voxel feature以产生正交地平面的特征。<br><img src="https://cdn.nlark.com/yuque/0/2022/png/478741/1668776253342-b70b7601-3e96-4dac-8a64-cf251da049cd.png#averageHue=%23f0f0ef&clientId=ud79661c2-ad60-4&from=paste&height=252&id=u1b5fb897&originHeight=252&originWidth=845&originalType=binary&ratio=1&rotation=0&showTitle=false&size=121370&status=done&style=none&taskId=ue8ddc226-f280-4f9d-b12f-27682a739f7&title=&width=845" alt="image.png"><br>OFT的思路非常简单，效果也很好。</p>
<blockquote>
<p>Although the back-projection step could have been improved by using some heuristics for better initialization of the voxel-based features, rather than naively do a back-projection.<br>review的作者的观点，还需要熟悉一下文章体会一下。</p>
</blockquote>
<h2 id="Pseudo-LiDAR"><a href="#Pseudo-LiDAR" class="headerlink" title="Pseudo LiDAR"></a>Pseudo LiDAR</h2><p>基于Pseudo-LiDAR的方案的实质是根据2D图像估计深度，借助单目深度估计的方案生成点云。</p>
<h3 id="2017–CVPR–MonoDepth–Unsupervised-Monocular-Depth-Estimation-with-Left-Right-Consistency"><a href="#2017–CVPR–MonoDepth–Unsupervised-Monocular-Depth-Estimation-with-Left-Right-Consistency" class="headerlink" title="2017–CVPR–MonoDepth–Unsupervised Monocular Depth Estimation with Left-Right Consistency"></a>2017–CVPR–MonoDepth–Unsupervised Monocular Depth Estimation with Left-Right Consistency</h3><h3 id="2018–CVPR–MLF–Multi-Level-Fusion-based-3D-Object-Detection-from-Monocular-Images"><a href="#2018–CVPR–MLF–Multi-Level-Fusion-based-3D-Object-Detection-from-Monocular-Images" class="headerlink" title="2018–CVPR–MLF–Multi-Level Fusion based 3D Object Detection from Monocular Images"></a>2018–CVPR–MLF–Multi-Level Fusion based 3D Object Detection from Monocular Images</h3><p>本文是严格意义上第一篇提出将估计的深度信息lift到3D的，使用估计的深度信息将每个像素从RGB图像上投影到3D空间，然后再将得到的点云特征和图像特征融合起来得到3D目标框。<br><img src="https://cdn.nlark.com/yuque/0/2022/png/478741/1668777488770-0e5506ce-bde1-4af6-9959-c91a488564c3.png#averageHue=%23e2d2b7&clientId=ud79661c2-ad60-4&from=paste&height=347&id=u3d33257d&originHeight=347&originWidth=777&originalType=binary&ratio=1&rotation=0&showTitle=false&size=167932&status=done&style=none&taskId=u30e094d5-b500-41b0-ad6e-b52b5c07eec&title=&width=777" alt="image.png"></p>
<h3 id="2018–CVPR–Pseudo-LiDAR-from-Visual-Depth-Estimation-Bridging-the-Gap-in-3D-Object-Detection-for-Autonomous-Driving"><a href="#2018–CVPR–Pseudo-LiDAR-from-Visual-Depth-Estimation-Bridging-the-Gap-in-3D-Object-Detection-for-Autonomous-Driving" class="headerlink" title="2018–CVPR–Pseudo-LiDAR from Visual Depth Estimation: Bridging the Gap in 3D Object Detection for Autonomous Driving"></a>2018–CVPR–Pseudo-LiDAR from Visual Depth Estimation: Bridging the Gap in 3D Object Detection for Autonomous Driving</h3><p>本文在得到Pseudo-LiDAR的特征后，直接使用最新的lidar-based的3D检测器。</p>
<blockquote>
<p> The authors argue that representation matters, and convolution on depth map does not make sense, as neighboring pixels on depth images may be physically far away in 3D space.<br>review</p>
</blockquote>
<h3 id="2018–CVPR–Frustum-PointNets-for-3D-Object-Detection-from-RGB-D-Data"><a href="#2018–CVPR–Frustum-PointNets-for-3D-Object-Detection-from-RGB-D-Data" class="headerlink" title="2018–CVPR–Frustum PointNets for 3D Object Detection from RGB-D Data"></a>2018–CVPR–Frustum PointNets for 3D Object Detection from RGB-D Data</h3><p><img src="https://cdn.nlark.com/yuque/0/2022/png/478741/1668848902425-3d8dbb09-3d3b-4de1-81d4-d22cf6ddf968.png#averageHue=%23d4d0cb&clientId=uf53aa924-d691-4&from=paste&height=263&id=u51387129&originHeight=263&originWidth=661&originalType=binary&ratio=1&rotation=0&showTitle=false&size=176617&status=done&style=none&taskId=u884520b7-fa04-4ac1-9341-8dc357a6e72&title=&width=661" alt="image.png"></p>
<h3 id="2019–ICCV–Monocular-3D-Object-Detection-with-Pseudo-LiDAR-Point-Cloud"><a href="#2019–ICCV–Monocular-3D-Object-Detection-with-Pseudo-LiDAR-Point-Cloud" class="headerlink" title="2019–ICCV–Monocular 3D Object Detection with Pseudo-LiDAR Point Cloud"></a>2019–ICCV–Monocular 3D Object Detection with Pseudo-LiDAR Point Cloud</h3><p>本文提出了pseudo-lidar的方案的两个缺点：不准确的深度估计导致的local misalignment 和目标外围深度伪影导致的long tail。提出了实例分割mask，并引入了2D-3D框的consistency loss。<br><img src="https://cdn.nlark.com/yuque/0/2022/png/478741/1668848848450-f7427826-075c-413b-8f55-1ad7bd4487f1.png#averageHue=%23faf2cd&clientId=uf53aa924-d691-4&from=paste&height=414&id=u3315ef35&originHeight=827&originWidth=508&originalType=binary&ratio=1&rotation=0&showTitle=false&size=418515&status=done&style=none&taskId=uf6b88a11-2464-4136-a612-36d07177c01&title=&width=254" alt="image.png"></p>
<h3 id="ForeSeE"><a href="#ForeSeE" class="headerlink" title="ForeSeE"></a>ForeSeE</h3><p>ForeSeE 也注意到了这些缺点，它们提出不是所有的pixel在深度估计中都有相同的重要性。因此它们训练了两个不同的深度估计器，一个用于前景、一个用于背景。在推理的时候自适应的融合深度图。</p>
<h3 id="2019–ICCV–AM3D–Accurate-Monocular-3D-Object-Detection-via-Color-Embedded-3D-Reconstruction-for-Autonomous-Driving"><a href="#2019–ICCV–AM3D–Accurate-Monocular-3D-Object-Detection-via-Color-Embedded-3D-Reconstruction-for-Autonomous-Driving" class="headerlink" title="2019–ICCV–AM3D–Accurate Monocular 3D Object Detection via Color-Embedded 3D Reconstruction for Autonomous Driving"></a>2019–ICCV–AM3D–Accurate Monocular 3D Object Detection via Color-Embedded 3D Reconstruction for Autonomous Driving</h3><blockquote>
<p>AM3D proposes a multi-modal fusion module to enhance the pseudo-LiDAR with color information</p>
</blockquote>
<h3 id=""><a href="#" class="headerlink" title=""></a></h3><h3 id="202008–ECCV2020–PatchNet–Rethinking-Pseudo-LiDAR-Representation"><a href="#202008–ECCV2020–PatchNet–Rethinking-Pseudo-LiDAR-Representation" class="headerlink" title="202008–ECCV2020–PatchNet–Rethinking Pseudo-LiDAR Representation"></a>202008–ECCV2020–PatchNet–Rethinking Pseudo-LiDAR Representation</h3><blockquote>
<p>PatchNet organizes pseudo-LiDAR into the image representation and utilizes powerful 2D CNN to boost the detection performance.<br>MonoFlex</p>
</blockquote>
<h3 id="202103–ICCV–Are-we-Missing-Confidence-in-Pseudo-LiDAR-Methods-for-Monocular-3D-Object-Detection"><a href="#202103–ICCV–Are-we-Missing-Confidence-in-Pseudo-LiDAR-Methods-for-Monocular-3D-Object-Detection" class="headerlink" title="202103–ICCV–Are we Missing Confidence in Pseudo-LiDAR Methods for Monocular 3D Object Detection?"></a>202103–ICCV–Are we Missing Confidence in Pseudo-LiDAR Methods for Monocular 3D Object Detection?</h3><p>本文认为虽然PL-based的方法表现出比RGB image only的方法更好的指标，但是并不意味着PL-based的方法是完全优于后者的，其中还有KITTI数据集中存在的问题。<br><img src="https://cdn.nlark.com/yuque/0/2022/png/478741/1668346439612-8e3d2178-e242-48a5-9737-8bc6d41d4143.png#averageHue=%2375e692&clientId=u137eae7a-36b8-4&from=paste&height=574&id=uc9dccc1a&originHeight=574&originWidth=678&originalType=binary&ratio=1&rotation=0&showTitle=false&size=141185&status=done&style=none&taskId=u5d0519b3-35da-44f3-9086-715356b9b33&title=&width=678" alt="image.png"></p>
<h3 id="2021–CVPR–Objects-are-Different-Flexible-Monocular-3D-Object-Detection"><a href="#2021–CVPR–Objects-are-Different-Flexible-Monocular-3D-Object-Detection" class="headerlink" title="2021–CVPR–Objects are Different: Flexible Monocular 3D Object Detection"></a>2021–CVPR–Objects are Different: Flexible Monocular 3D Object Detection</h3><p>MonoFlex针对截断目标设计了不同的3D目标框解耦方式</p>
<p>DD3D</p>
<h3 id="2022–CVPR–Homography-Loss-for-Monocular-3D-Object-Detection"><a href="#2022–CVPR–Homography-Loss-for-Monocular-3D-Object-Detection" class="headerlink" title="2022–CVPR–Homography Loss for Monocular 3D Object Detection"></a>2022–CVPR–Homography Loss for Monocular 3D Object Detection</h3><p><img src="https://cdn.nlark.com/yuque/0/2022/png/478741/1668655897144-c7dab5f8-4ebe-4bad-b72b-c24c85c71505.png#averageHue=%23f6f6f5&clientId=udc62ad13-1138-4&from=drop&id=ub50b52a1&originHeight=312&originWidth=1174&originalType=binary&ratio=1&rotation=0&showTitle=false&size=149118&status=done&style=none&taskId=uc423b1ad-7ac0-4635-b41d-40a973f2216&title=" alt="截屏2022-11-17 11.31.32.png"></p>
<h1 id="Related-Review"><a href="#Related-Review" class="headerlink" title="Related Review"></a>Related Review</h1><p>XPeng—Patrick Langechuan Liu—2020<br><a target="_blank" rel="noopener" href="https://towardsdatascience.com/monocular-3d-object-detection-in-autonomous-driving-2476a3c7f57e">Monocular 3D Object Detection in Autonomous Driving — A Review</a></p>

      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

    
      

    <span class="post-categories">
      <i class="icon-categories"></i>
        <a href="/categories/Paper/">Paper</a>
    </span>
    

    
    

    <span class="post-tags">
      <i class="icon-tags"></i>
        <a href="/tags/Mono3D/">Mono3D</a>
    </span>
    

    </div>

    
  </div>
</article>

  









    </main>

    <footer class="site-footer">
  <p class="site-info">
    Proudly powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and
    Theme by <a href="https://github.com/CodeDaraW/Hacker" target="_blank">Hacker</a>
    </br>
    
    &copy; 2023 Jingyi Yu
    
  </p>
</footer>
    
    
  </div>
</div>
</body>
</html>