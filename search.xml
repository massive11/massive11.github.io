<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>KAN论文与代码解读</title>
      <link href="/2024/05/09/paper-kan-lun-wen-yu-dai-ma-jie-du/"/>
      <url>/2024/05/09/paper-kan-lun-wen-yu-dai-ma-jie-du/</url>
      
        <content type="html"><![CDATA[<p>本文对KAN的论文和代码进行了解读。<br><span id="more"></span></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> General AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MapTR V2论文与代码解读</title>
      <link href="/2023/08/27/paper-maptr-v2-lun-wen-yu-dai-ma-jie-du/"/>
      <url>/2023/08/27/paper-maptr-v2-lun-wen-yu-dai-ma-jie-du/</url>
      
        <content type="html"><![CDATA[<p>本文对MapTR V2的论文和代码进行了解读。<br><span id="more"></span></p><p>MapTR V2是在MapTR（详情可戳<a href="https://massive11.github.io/2022/12/10/paper-maptr-lun-wen-yu-dai-ma-jie-du/">MapTR论文与代码解读</a>）的基础上进行了进行了一些小改动实现了性能提升，整体框架和MapTR差距不大，其改动在于：</p><ol><li>解耦了自注意力机制，降低了显存开销</li><li>引入了一对多的集合预测分支来加速收敛</li><li>在pv和bev上都增加了密集的监督，显著提升了性能</li><li>增加了center line的学习，这对于下游任务更加友好</li><li>将整个框架拓展到了3D地图重建</li></ol>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Multi-view 3D </tag>
            
            <tag> HD Map </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Panoptic Visual Odometry论文解读</title>
      <link href="/2023/08/13/paper-pvo-lun-wen-yu-dai-ma-jie-du/"/>
      <url>/2023/08/13/paper-pvo-lun-wen-yu-dai-ma-jie-du/</url>
      
        <content type="html"><![CDATA[<p>Panoptic Visual Odometry论文解读<br><span id="more"></span></p><h2 id="PVO整体结构"><a href="#PVO整体结构" class="headerlink" title="PVO整体结构"></a>PVO整体结构</h2><p>PVO提出了一个全景视觉里程计框架，同时处理视觉里程计和视频全景分割两项任务。视觉里程计（VO）是基于静态场景假设，通过单目图像计算相机的位姿。对于动态场景，需要过滤动态目标。视频全景分割（VPS）则是跟踪场景中的动态实例。通过两个任务之间的循环迭代优化，PVO同时提升了两个任务的精度。整体思路概括而言就是VPS可以给VO提供每个像素的权重信息，VO可以提供位姿信息从而将目标追踪从2D空间提升到3D空间。PVO的网络结构如图1所示。</p><p><img src="/images/pvo.png"/></p><center>图1 PVO网络结构</center><p>PVO由三个模块组成，image panoptic segmentation module，Panoptic-Enhanced VO Module 以及VO-Enhanced VPS Module。</p><h3 id="Image-panoptic-segmentation-module"><a href="#Image-panoptic-segmentation-module" class="headerlink" title="Image panoptic segmentation module"></a>Image panoptic segmentation module</h3><blockquote><p>To exploit the power of multi-resolution features, the imageview encoder includes a backbone for high-level feature extraction and a neck for multi-resolution feature fusion.</p></blockquote><p>这个部分就是把输入的原始图像编码成特征，通常直接遵循2D检测的pipeline。为权衡精度和速度，常见的模块为ResNet/EfficientNet + FPN。</p><h3 id="Panoptic-Enhanced-VO-Module"><a href="#Panoptic-Enhanced-VO-Module" class="headerlink" title="Panoptic-Enhanced VO Module"></a>Panoptic-Enhanced VO Module</h3><h3 id="VO-Enhanced-VPS-Module"><a href="#VO-Enhanced-VPS-Module" class="headerlink" title="VO-Enhanced VPS Module"></a>VO-Enhanced VPS Module</h3><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://arxiv.org/pdf/2207.01610.pdf">PVO Paper</a><br><a href="https://github.com/zju3dv/pvo">PVO Code</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Visual Odometry </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>排序算法重温</title>
      <link href="/2023/06/08/code-pai-xu-suan-fa-chong-wen/"/>
      <url>/2023/06/08/code-pai-xu-suan-fa-chong-wen/</url>
      
        <content type="html"><![CDATA[<p>时隔两年多的排序算法重温。<br><span id="more"></span></p><h2 id="快排"><a href="#快排" class="headerlink" title="快排"></a>快排</h2><p>原理：快排通过分治的方式实现排序<br>过程：</p><ol><li>将数列划分为两部分（要求保证相对大小关系）。<br>保证前一个子数列中的数都小于后一个子数列中的数。为了保证平均时间复杂度，一般是随机选择一个数 $m$ 来当做两个子数列的分界。</li><li>递归到两个子序列中分别进行快速排序。<br>维护一前一后两个指针 $p$ 和 $q$，依次考虑当前的数是否放在了应该放的位置（前还是后）。如果当前的数没放对，比如说如果后面的指针 $q$ 遇到了一个比 $m$ 小的数，那么可以交换 $p$ 和 $q$ 位置上的数，再把 $p$ 向后移一位。当前的数的位置全放对后，再移动指针继续处理，直到两个指针相遇。</li><li>无需合并，此时数列已经完全有序。</li></ol><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><pre class="line-numbers language-Python" data-language="Python"><code class="language-Python">def quick_sort(alist, first, last):    if first &gt;&#x3D; last:        return    mid_value &#x3D; alist[first]    low &#x3D; first    high &#x3D; last    while low &lt; high:        while low &lt; high and alist[high] &gt;&#x3D; mid_value:            high -&#x3D; 1        alist[low] &#x3D; alist[high]        while low &lt; high and alist[low] &lt; mid_value:            low +&#x3D; 1        alist[high] &#x3D; alist[low]    alist[low] &#x3D; mid_value    quick_sort(alist, first, low - 1)    quick_sort(alist, low + 1, last)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h3><p>快速排序是在冒泡排序的基础上改进而来的，冒泡排序每次只能交换相邻的两个元素，而快速排序是跳跃式的交换，交换的距离很大，因此总的比较和交换次数少了很多，速度也快了不少。<br>稳定性：不稳定（经过排序之后，相同值的元素的相对位置可能发生改变）<br>时间复杂度：最优时间复杂度和平均时间复杂度为${O(n logn)}$，最坏时间复杂度为${O(n^2)}$<br>对于最优情况，每一次选择的分界值都是序列的中位数;<br>对于最坏情况，每一次选择的分界值都是序列的最值。<br>在实践中，几乎不可能达到最坏情况，而快速排序的内存访问遵循局部性原理，所以多数情况下快速排序的表现大幅优于堆排序等其他复杂度为 O(n \log n) 的排序算法。</p><p>空间复杂度：快速排序只是使用数组原本的空间进行排序，所以所占用的空间应该是常量级的，但是由于每次划分之后是递归调用，所以递归调用在运行的过程中会消耗一定的空间，在一般情况下的空间复杂度为 O(logn)，在最差的情况下，若每次只完成了一个元素，那么空间复杂度为 O(n)。所以一般认为快速排序的空间复杂度为 O(logn)。</p><h3 id="经典题型实战"><a href="#经典题型实战" class="headerlink" title="经典题型实战"></a>经典题型实战</h3><blockquote><p><a href="https://leetcode.cn/problems/xx4gT2/">剑指 Offer II 076. 数组中的第 k 大的数字</a><br>给定整数数组 nums 和整数 k，请返回数组中第 k 个最大的元素。请注意，你需要找的是数组排序后的第 k 个最大的元素，而不是第 k 个不同的元素。</p></blockquote><p>Python 题解<br><pre class="line-numbers language-Python" data-language="Python"><code class="language-Python">class Solution:    def findKthLargest(self, nums: List[int], k: int) -&gt; int:        def quicksort(nums, first, last, k):            mid &#x3D; nums[first]            low &#x3D; first            hign &#x3D; last            while low &lt; hign:                while low &lt; hign and nums[hign] &gt;&#x3D; mid:                    hign -&#x3D; 1                nums[low] &#x3D; nums[hign]                while low &lt; hign and nums[low] &lt; mid:                    low +&#x3D; 1                nums[hign] &#x3D; nums[low]            nums[low] &#x3D; mid            if low &#x3D;&#x3D; k:                return mid            elif low &gt; k:                return quicksort(nums, first, low - 1, k)            else:                return quicksort(nums, low+1, last, k)        return quicksort(nums, 0, len(nums)-1, len(nums)-k)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></p><h2 id="堆排"><a href="#堆排" class="headerlink" title="堆排"></a>堆排</h2><p>原理：利用二叉堆这种数据结构而设计的一种排序算法，其本质是建立在堆上的选择排序。<br>堆：堆是具有以下性质的完全二叉树：每个结点的值都大于或等于其左右孩子结点的值，称为大顶堆；或者每个结点的值都小于或等于其左右孩子结点的值，称为小顶堆。<br>堆的可视化如下图所示（<a href="https://www.cnblogs.com/chengxiao/p/6129630.html">图源</a>）<br><img src="/images/Heapsort.png"/><br>按层编号堆中的节点，这种逻辑结构映射到数组中为（<a href="https://www.cnblogs.com/chengxiao/p/6129630.html">图源</a>）<br><img src="/images/Heapsort2.png"/></p><p>基本思想：将待排序序列构造成一个大顶堆，取出堆顶的根节点。将其与末尾元素进行交换，此时末尾就为最大值，维持残余堆的性质。之后将堆顶的元素取出，作为次大值，与数组倒数第二位元素交换，并维持残余堆的性质。以此类推，在第 $n-1$ 次操作后，整个数组就完成了排序。</p><h3 id="代码实现-1"><a href="#代码实现-1" class="headerlink" title="代码实现"></a>代码实现</h3><pre class="line-numbers language-Python" data-language="Python"><code class="language-Python">def sift_down(arr, start, end):    # 计算父结点和子结点的下标    parent &#x3D; int(start)    child &#x3D; int(parent * 2 + 1)    while child &lt;&#x3D; end: # 子结点下标在范围内才做比较        # 先比较两个子结点大小，选择最大的        if child + 1 &lt;&#x3D; end and arr[child] &lt; arr[child + 1]:            child +&#x3D; 1        # 如果父结点比子结点大，代表调整完毕，直接跳出函数        if arr[parent] &gt;&#x3D; arr[child]:            return        else: # 否则交换父子内容，子结点再和孙结点比较            arr[parent], arr[child] &#x3D; arr[child], arr[parent]            parent &#x3D; child            child &#x3D; int(parent * 2 + 1)def heap_sort(arr, len):  # 从最后一个节点的父节点开始 sift down 以完成堆化 (heapify)    i &#x3D; (len - 1 - 1) &#x2F; 2    while(i &gt;&#x3D; 0):        sift_down(arr, i, len - 1)        i -&#x3D; 1  # 先将第一个元素和已经排好的元素前一位做交换，再重新调整（刚调整的元素之前的元素），直到排序完毕    i &#x3D; len - 1    while(i &gt; 0):        arr[0], arr[i] &#x3D; arr[i], arr[0]        sift_down(arr, 0, i - 1)        i -&#x3D; 1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="特点-1"><a href="#特点-1" class="headerlink" title="特点"></a>特点</h3><p>稳定性：不稳定<br>时间复杂度：最坏，最好，平均时间复杂度均为${O(n logn)}$<br>空间复杂度：由于可以在输入数组上建立堆，所以这是一个原地算法。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://oi-wiki.org/basic/quick-sort/">https://oi-wiki.org/basic/quick-sort/</a><br><a href="https://oi-wiki.org/basic/heap-sort/">https://oi-wiki.org/basic/heap-sort/</a><br><a href="https://www.cnblogs.com/chengxiao/p/6129630.html">https://www.cnblogs.com/chengxiao/p/6129630.html</a></p>]]></content>
      
      
      <categories>
          
          <category> 原理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Algorithm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Git基础使用梳理</title>
      <link href="/2023/06/02/tech-git-ji-chu-shi-yong/"/>
      <url>/2023/06/02/tech-git-ji-chu-shi-yong/</url>
      
        <content type="html"><![CDATA[<p>Git基础用法<br><span id="more"></span><br>Git 的三个分区分别是：working directory，stage area (index area)，commit history。<br>working directory 是「工作目录」，也是我们肉眼能够看到的文件。<br>当我们在 work dir 中执行 git add 相关命令后，就会把 work dir 中的修改添加到「暂存区」stage area 中去。<br>当 stage 中存在修改时，我们使用 git commit 相关命令之后，就会把 stage 中的修改保存到「提交历史」 commit history 中，也就是 HEAD 指针指向的位置。<br>关于 commit history ，任何修改只要进入 commit history，基本可以认为永远不会丢失了。每个 commit 都有一个唯一的 Hash 值，我们经常说的 HEAD 或者 master 分支，都可以理解为一个指向某个 commit 的指针。<br>work dir 和 stage 区域的状态，可以通过命令 git status 来查看，history 区域的提交历史可以通过 git log 命令来查看。</p><h2 id="1-如何将work-dir中的修改加入stage"><a href="#1-如何将work-dir中的修改加入stage" class="headerlink" title="1.如何将work dir中的修改加入stage"></a>1.如何将work dir中的修改加入stage</h2><p>使用 git add 相关的命令就行了。顺便一提，add 有个别名叫做 stage，也就是说你可能见到 git stage 相关的命令，这个命令和 git add 命令是完全一样的。</p><h2 id="2-如何将stage中的修改还原到work-dir中"><a href="#2-如何将stage中的修改还原到work-dir中" class="headerlink" title="2.如何将stage中的修改还原到work dir中"></a>2.如何将stage中的修改还原到work dir中</h2><p>使用 checkout 命令</p><h2 id="3-将-stage-区的文件添加到-history-区"><a href="#3-将-stage-区的文件添加到-history-区" class="headerlink" title="3.将 stage 区的文件添加到 history 区"></a>3.将 stage 区的文件添加到 history 区</h2><p>git commit 相关的命令<br><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">git</span> commit <span class="token parameter variable">-m</span> <span class="token string">'一些描述'</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></p><h2 id="4-将-history-区的文件还原到-stage-区"><a href="#4-将-history-区的文件还原到-stage-区" class="headerlink" title="4.将 history 区的文件还原到 stage 区"></a>4.将 history 区的文件还原到 stage 区</h2><p>可以使用 git reset 命令</p><h2 id="5-将-work-dir-的修改提交到-history-区"><a href="#5-将-work-dir-的修改提交到-history-区" class="headerlink" title="5.将 work dir 的修改提交到 history 区"></a>5.将 work dir 的修改提交到 history 区</h2><p>先 git add 然后 git commit 就行了，或者一个快捷方法是使用命令 git commit -a</p><h2 id="6-将-history-区的历史提交还原到-work-dir-中"><a href="#6-将-history-区的历史提交还原到-work-dir-中" class="headerlink" title="6.将 history 区的历史提交还原到 work dir 中"></a>6.将 history 区的历史提交还原到 work dir 中</h2><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://labuladong.github.io/algo/di-si-zhan-4baf4/wo-yong-si-ad48a/">https://labuladong.github.io/algo/di-si-zhan-4baf4/wo-yong-si-ad48a/</a></p>]]></content>
      
      
      <categories>
          
          <category> 实战 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Tech </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>BEV模型常用性能评价指标总结</title>
      <link href="/2023/04/12/principle-bev-mo-xing-chang-yong-zhi-biao-zong-jie/"/>
      <url>/2023/04/12/principle-bev-mo-xing-chang-yong-zhi-biao-zong-jie/</url>
      
        <content type="html"><![CDATA[<p>BEV模型常用性能评价指标总结<br><span id="more"></span></p><h2 id="混淆矩阵"><a href="#混淆矩阵" class="headerlink" title="混淆矩阵"></a>混淆矩阵</h2><p>混淆矩阵（confusion matrix）是一种特定的矩阵用来呈现算法性能的可视化效果，其每一列代表预测值，每一行代表的是实际的类别。这个名字来源于它可以非常容易的表明多个类别是否有混淆（也就是一个class被预测成另一个class）。下面是二分类的混淆矩阵：<br><img src="/images/confusion_matrix.png" width="80%" height="80%"/><br>预测值与真实值相同为True，反之则为False。混淆矩阵的对角线是判断正确的，期望TP和TN越大越好，FN和FP越小越好。</p><h3 id="查准率-Precision"><a href="#查准率-Precision" class="headerlink" title="查准率 Precision"></a>查准率 Precision</h3><p>表示模型预测为正例的所有样本中，预测正确（真实标签为正）样本的占比：</p><script type="math/tex; mode=display">Precision = \frac{TP}{TP+FP}</script><h3 id="查全率-Recall"><a href="#查全率-Recall" class="headerlink" title="查全率 Recall"></a>查全率 Recall</h3><p>表示所有真实标签为正的样本，有多大百分比被预测出来:</p><script type="math/tex; mode=display">Recall = \frac{TP}{TP + FN}</script><h3 id="F1-score"><a href="#F1-score" class="headerlink" title="F1-score"></a>F1-score</h3><p>表示precision和recall的调和平均数，具体公式如下：</p><script type="math/tex; mode=display">F_1 = \frac{2 * Precision * Recall}{Precision + Recall}</script><h2 id="语义分割"><a href="#语义分割" class="headerlink" title="语义分割"></a>语义分割</h2><p>语义分割的本质任务是分类任务，常规分类任务的对象是图像中的物体，而语义分割的对象是图像中像素点。</p><h3 id="IoU"><a href="#IoU" class="headerlink" title="IoU"></a>IoU</h3><p><img src="/images/iou_visual.png" width="60%" height="60%"/></p><script type="math/tex; mode=display">IoU = \frac{X\cap Y}{X\cup Y} = \frac{TP}{TP+FN+FP}</script><p>IoU计算实现<br><pre class="line-numbers language-PYTHON" data-language="PYTHON"><code class="language-PYTHON">def iou(input, target, classes&#x3D;1):    &quot;&quot;&quot;  compute the value of iou    :param input:  2d array, int, prediction    :param target: 2d array, int, ground truth    :param classes: int, the number of class    :return:        iou: float, the value of iou    &quot;&quot;&quot;    intersection &#x3D; np.logical_and(target &#x3D;&#x3D; classes, input &#x3D;&#x3D; classes)    # print(intersection.any())    union &#x3D; np.logical_or(target &#x3D;&#x3D; classes, input &#x3D;&#x3D; classes)    iou &#x3D; np.sum(intersection) &#x2F; np.sum(union)    return iou<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></p><h2 id="2D目标检测"><a href="#2D目标检测" class="headerlink" title="2D目标检测"></a>2D目标检测</h2><h3 id="IoU-1"><a href="#IoU-1" class="headerlink" title="IoU"></a>IoU</h3><p>2D检测框的IoU实现<br><pre class="line-numbers language-PYTHON" data-language="PYTHON"><code class="language-PYTHON">x1,y1,x2,y2 &#x3D; box1 #box1的左上角坐标、右下角坐标x3,y3,x4,y4 &#x3D; box2 #box1的左上角坐标、右下角坐标#计算交集的坐标x_inter1 &#x3D; max(x1,x3) #union的左上角xy_inter1 &#x3D; max(y1,y3) #union的左上角yx_inter2 &#x3D; min(x2,x4) #union的右下角xy_inter2 &#x3D; min(y2,y4) #union的右下角y# 计算交集部分面积，因为图像是像素点，所以计算图像的长度需要加一# 比如有两个像素点(0,0)、(1,0)，那么图像的长度是1-0+1&#x3D;2，而不是1-0&#x3D;1interArea &#x3D; max(0,x_inter2-x_inter1+1)*max(0,y_inter2-y_inter1+1)# 分别计算两个box的面积area_box1 &#x3D; (x2-x1+1)*(y2-y1+1)area_box2 &#x3D; (x4-x3+1)*(y4-y3+1)#计算IOU，交集比并集，并集面积&#x3D;两个矩形框面积和-交集面积iou &#x3D; interArea&#x2F;(area_box1+area_box2-interArea)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></p><h3 id="GIoU-Generalized-Intersection-over-Union"><a href="#GIoU-Generalized-Intersection-over-Union" class="headerlink" title="GIoU (Generalized Intersection over Union)"></a>GIoU (Generalized Intersection over Union)</h3><p>GIoU相较于IoU多了一个‘Generalized’，这也意味着它能在更广义的层面上计算IoU，并解决两个图像没有相交时，无法比较两个图像的距离远近的问题，公式定义如下</p><script type="math/tex; mode=display">GIoU = IoU - \frac{|C-(A\cup B)|}{|C|}</script><p>其中C代表两个图像的最小包庇面积，也可以理解为这两个图像的最小外接矩形的面积。<br>原有IoU取值区间为[0,1]，而GIoU的取值区间为[-1,1]；在两个图像完全重叠时，IoU=GIoU=1，在两个图像距离无限远时，IoU=0而GIoU=-1<br><pre class="line-numbers language-PYTHON" data-language="PYTHON"><code class="language-PYTHON"># 分别是第一个矩形左右上下的坐标x1,y1,x2,y2 &#x3D; box1 #box1的左上角坐标、右下角坐标x3,y3,x4,y4 &#x3D; box2 #box1的左上角坐标、右下角坐标iou &#x3D; Iou(box1,box2)area_C &#x3D; (max(x1,x2,x3,x4)-min(x1,x2,x3,x4))*(max(y1,y2,y3,y4)-min(y1,y2,y3,y4))area_1 &#x3D; (x2-x1)*(y1-y2)area_2 &#x3D; (x4-x3)*(y3-y4)sum_area &#x3D; area_1 + area_2# 第一个矩形的宽w1 &#x3D; x2 - x1# 第二个矩形的宽w2 &#x3D; x4 - x3   h1 &#x3D; y1 - y2h2 &#x3D; y3 - y4# 交叉部分的宽W &#x3D; min(x1,x2,x3,x4)+w1+w2-max(x1,x2,x3,x4) # 交叉部分的高  H &#x3D; min(y1,y2,y3,y4)+h1+h2-max(y1,y2,y3,y4)  # 交叉的面积  Area &#x3D; W*H    # 两矩形并集的面积add_area &#x3D; sum_area - Area  # 闭包区域中不属于两个框的区域占闭包区域的比重end_area &#x3D; (area_C - add_area)&#x2F;area_C    giou &#x3D; iou - end_area<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></p><h3 id="AP-amp-mAP"><a href="#AP-amp-mAP" class="headerlink" title="AP &amp; mAP"></a>AP &amp; mAP</h3><p>PR曲线: Precision-Recall曲线<br>AP: PR曲线下面积解<br>mAP: mean Average Precision, 即各类别AP的平均值</p><p>在混淆矩阵的部分，我们已经介绍过TP、FP等定义。在计算AP的过程中，我们首先需要确定如何判断推理结果属于哪个类别。<br>以单张图像为例，首先遍历图片中ground truth对象，然后提取我们要计算的某类别的gt objects，之后读取我们通过检测器检测出的这种类别的检测框（其他类别的先不管），接着过滤掉置信度分数低于置信度阈值的框（也有的未设置信度阈值），将剩下的检测框按置信度分数从高到低排序，最先判断置信度分数最高的检测框与gt bbox的iou是否大于iou阈值，若iou大于设定的iou阈值即判断为TP，将此gt_bbox标记为已检测（后续的同一个GT的多余检测框都视为FP,这就是为什么先要按照置信度分数从高到低排序，置信度分数最高的检测框最先去与iou阈值比较，若大于iou阈值，视为TP，后续的同一个gt对象的检测框都视为FP），iou小于阈值的，直接规划到FP中去。代码实现如下<br><pre class="line-numbers language-PYTHON" data-language="PYTHON"><code class="language-PYTHON">if ovmax &gt; ovthresh:             # 若iou大于阈值  if not R[&#39;difficult&#39;][jmax]:   # 且要检测的gt对象非difficult类型    if not R[&#39;det&#39;][jmax]:       # 且gt对象暂未被检测      tp[d] &#x3D; 1.                 # 此检测框记为TP      R[&#39;det&#39;][jmax] &#x3D; 1         # 并将此gt对象标记为已检测    else:                        # 若gt对象已被检测，那么此检测框为FP      fp[d] &#x3D; 1.else:                            # iou&lt;&#x3D;阈值，此检测框为FP  fp[d] &#x3D; 1.<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br>关于图片中FN的统计就比较简单了，图片中某类别一共有多少个gt我们是知道的，减去TP的个数，剩下的就是FN的个数了。</p><p>现在我们已经能够计算得到某一类别的percision和recall。PR曲线与x轴所围成的面积，即为当前类别的AP值。AP值计算有3种方式：</p><ol><li>在VOC2010以前，只需要选取当Recall &gt;= 0, 0.1, 0.2, …, 1共11个点时的Precision最大值，然后AP就是这11个Precision的平均值。</li><li>在VOC2010及以后，需要针对每一个不同的Recall值（包括0和1），选取其大于等于这些Recall值时的Precision最大值，然后计算PR曲线下面积作为AP值。</li><li>设定多个IOU阈值（0.5-0.95,0.05为步长），在每一个IOU阈值下都有某一类别的AP值，然后求不同IOU阈值下的AP平均，就是所求的最终的某类别的AP值。</li></ol><h2 id="3D目标检测（以nuScenes为例）"><a href="#3D目标检测（以nuScenes为例）" class="headerlink" title="3D目标检测（以nuScenes为例）"></a>3D目标检测（以nuScenes为例）</h2><h3 id="NDS"><a href="#NDS" class="headerlink" title="NDS"></a>NDS</h3><blockquote><p>Our final score is a weighted sum of mean Average Precision (mAP) and several True Positive (TP) metrics.<br>nuScenes detection score (NDS): We consolidate the above metrics by computing a weighted sum: mAP, mATE, mASE, mAOE, mAVE and mAAE. As a first step we convert the TP errors to TP scores as TP_score = max(1 - TP_error, 0.0). We then assign a weight of 5 to mAP and 1 to each of the 5 TP scores and calculate the normalized sum.</p></blockquote><h3 id="mAP"><a href="#mAP" class="headerlink" title="mAP"></a>mAP</h3><blockquote><p>We use the well-known Average Precision metric, but define a match by considering the 2D center distance on the ground plane rather than intersection over union based affinities. Specifically, we match predictions with the ground truth objects that have the smallest center-distance up to a certain threshold. For a given match threshold we calculate average precision (AP) by integrating the recall vs precision curve for recalls and precisions &gt; 0.1. We finally average over match thresholds of {0.5, 1, 2, 4} meters and compute the mean across classes.</p></blockquote><p>AP的阈值匹配不使用IoU来计算，而使用在地平面上的2D中心距离d来计算。解耦了物体的尺寸和方向对AP计算的影响。nuScenes的mAP计算设置的阈值为$\{0.5, 1, 2, 4\}$米。</p><h3 id="several-True-Positive-TP-metrics"><a href="#several-True-Positive-TP-metrics" class="headerlink" title="several True Positive (TP) metrics"></a>several True Positive (TP) metrics</h3><blockquote><p>Here we define metrics for a set of true positives (TP) that measure translation / scale / orientation / velocity and attribute errors. All TP metrics are calculated using a threshold of 2m center distance during matching, and they are all designed to be positive scalars.</p></blockquote><ol><li>Average Translation Error (ATE): Euclidean center distance in 2D in meters.</li><li>Average Scale Error (ASE): Calculated as 1 - IOU after aligning centers and orientation.</li><li>Average Orientation Error (AOE): Smallest yaw angle difference between prediction and ground-truth in radians. Orientation error is evaluated at 360 degree for all classes except barriers where it is only evaluated at 180 degrees. Orientation errors for cones are ignored.</li><li>Average Velocity Error (AVE): Absolute velocity error in m/s. Velocity error for barriers and cones are ignored.</li><li>Average Attribute Error (AAE): Calculated as 1 - acc, where acc is the attribute classification accuracy. Attribute error for barriers and cones are ignored.</li></ol><h2 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h2><h3 id="NMS实现"><a href="#NMS实现" class="headerlink" title="NMS实现"></a>NMS实现</h3><p>对于一张图片中的每一个预测框来说，模型为其每一个类别都预测了一个置信度分数（一般多分类，模型输出后接softmax，每一个类别都得到了一个置信度分数，包括背景类）我们取置信度最高的那一个类别作为预测框中对象所属的类别。</p><ol><li>首先我们将置信度分数低于置信度阈值a的所有预测框去掉 。</li><li>然后在同一张图片上，我们按照类别（除开背景类，因为背景类不需要进行NMS），将所有预测框按照置信度从高到低排序，将置信度最高的框作为我们要保留的此类别的第1个预测框，</li><li>然后按照顺序计算剩下其他预测框与其的IoU，</li><li>去掉与其IoU大于IoU阈值b的预测框（其实代码实现里是将这些要去掉的预测框其置信度分数置为0），</li><li>第一次迭代结束，我们已经剔除了与第一个框重合度较高的框。</li></ol><p>NMS手写实现<br><pre class="line-numbers language-PYTHON" data-language="PYTHON"><code class="language-PYTHON">import numpy as npdef nms(dets, thresh):    &quot;&quot;&quot;Pure Python NMS baseline.&quot;&quot;&quot;    x1 &#x3D; dets[:, 0]    y1 &#x3D; dets[:, 1]    x2 &#x3D; dets[:, 2]    y2 &#x3D; dets[:, 3]    scores &#x3D; dets[:, 4]    areas &#x3D; (x2 - x1 + 1) * (y2 - y1 + 1)    order &#x3D; scores.argsort()[::-1]  # 置信度从高到低排序    keep &#x3D; []    while order.size &gt; 0:        i &#x3D; order[0]    # 此类别中置信度最高的预测框的索引        keep.append(i)  # 将其作为保留下来的第1个预测框        xx1 &#x3D; np.maximum(x1[i], x1[order[1:]])        yy1 &#x3D; np.maximum(y1[i], y1[order[1:]])        xx2 &#x3D; np.minimum(x2[i], x2[order[1:]])        yy2 &#x3D; np.minimum(y2[i], y2[order[1:]])        w &#x3D; np.maximum(0.0, xx2 - xx1 + 1)        h &#x3D; np.maximum(0.0, yy2 - yy1 + 1)        inter &#x3D; w * h        ovr &#x3D; inter &#x2F; (areas[i] + areas[order[1:]] - inter) # 计算其余预测框与置信度最高的预测框的IoU        inds &#x3D; np.where(ovr &lt;&#x3D; thresh)[0]  # 记录下第1个与其Iou&lt;阈值的预测框，也就是与其Iou&lt;阈值的预测框中置信度最高的        order &#x3D; order[inds + 1]     # 将与保留下来的第1个预测框Iou&lt;阈值的预测框中置信度分数最高的预测框作为第2个要保留的    return keep  # 所有经过NMS后保留下来的框<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.cnblogs.com/Fish0403/p/17117778.html">https://www.cnblogs.com/Fish0403/p/17117778.html</a><br><a href="https://zhuanlan.zhihu.com/p/75348108">https://zhuanlan.zhihu.com/p/75348108</a><br><a href="https://zhuanlan.zhihu.com/p/70306015">https://zhuanlan.zhihu.com/p/70306015</a><br><a href="https://zhuanlan.zhihu.com/p/64423753">https://zhuanlan.zhihu.com/p/64423753</a></p>]]></content>
      
      
      <categories>
          
          <category> 原理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> BEV Perception </tag>
            
            <tag> Pytorch </tag>
            
            <tag> Metrics </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>BEV模型常用LOSS总结</title>
      <link href="/2023/04/08/principle-bev-mo-xing-chang-yong-loss-zong-jie/"/>
      <url>/2023/04/08/principle-bev-mo-xing-chang-yong-loss-zong-jie/</url>
      
        <content type="html"><![CDATA[<p>BEV模型常用LOSS总结，以Pytorch用法为例进行介绍<br><span id="more"></span></p><h2 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h2><h3 id="CrossEntropyLoss-交叉熵"><a href="#CrossEntropyLoss-交叉熵" class="headerlink" title="CrossEntropyLoss 交叉熵"></a>CrossEntropyLoss 交叉熵</h3><blockquote><p>torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=- 100, reduce=None, reduction=’mean’, label_smoothing=0.0)</p></blockquote><p><img src="/images/cross_entropy_loss.png"/></p><p>交叉熵主要用于处理分类任务。当类别不均衡的时候，可以通过给不同的类别添加不同的权重进行平衡。</p><pre class="line-numbers language-PYTHON" data-language="PYTHON"><code class="language-PYTHON"># Example of target with class indicesloss &#x3D; nn.CrossEntropyLoss()input &#x3D; torch.randn(3, 5, requires_grad&#x3D;True)target &#x3D; torch.empty(3, dtype&#x3D;torch.long).random_(5)output &#x3D; loss(input, target)output.backward()# Example of target with class probabilitiesinput &#x3D; torch.randn(3, 5, requires_grad&#x3D;True)target &#x3D; torch.randn(3, 5).softmax(dim&#x3D;1)output &#x3D; loss(input, target)output.backward()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="Focal-loss"><a href="#Focal-loss" class="headerlink" title="Focal loss"></a>Focal loss</h3><p>Focal Loss 就是一个解决分类问题中类别不平衡、分类难度差异的一个 loss，它实际上一个基于交叉熵的改进版本。以二分类为例，公式定义如下</p><script type="math/tex; mode=display">FL(p_t) = -\alpha_t(1- p_t)^{\gamma}log(p_t)</script><p><img src="/images/focal_loss.png" width="70%" height="70%"/></p><p>Pytorch实现<br><pre class="line-numbers language-PYTHON" data-language="PYTHON"><code class="language-PYTHON">import torchimport torch.nn as nnimport torch.optim as optimfrom torch.utils.data import DataLoader, Datasetimport torchvisionimport torchvision.transforms as Ffrom IPython.display import displayclass FocalLoss(nn.Module):    def __init__(self, weight&#x3D;None, reduction&#x3D;&#39;mean&#39;, gamma&#x3D;0, eps&#x3D;1e-7):        super(FocalLoss, self).__init__()        self.gamma &#x3D; gamma        self.eps &#x3D; eps        self.ce &#x3D; torch.nn.CrossEntropyLoss(weight&#x3D;weight, reduction&#x3D;reduction)    def forward(self, input, target):        logp &#x3D; self.ce(input, target)        p &#x3D; torch.exp(-logp)        loss &#x3D; (1 - p) ** self.gamma * logp        return loss.mean()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></p><h3 id="KLDivLoss-KL散度"><a href="#KLDivLoss-KL散度" class="headerlink" title="KLDivLoss KL散度"></a>KLDivLoss KL散度</h3><blockquote><p>torch.nn.KLDivLoss(size_average=None, reduce=None, reduction=’mean’, log_target=False)</p></blockquote><p><img src="/images/kl_loss.png"/></p><p>KL散度的取值范围是$[0, +\infty)$，当两个分布接近相同的时候KL散度取值为0。KL散度和交叉熵之间相差一个常数，这个常数是信息熵。<br>不对称性。尽管KL散度从直观上是个度量或距离函数，但它并不是一个真正的度量或者距离，因为它不具有对称性，即$D_{KL}(p||q) \ne D_{KL}(q||p)$</p><pre class="line-numbers language-PYTHON" data-language="PYTHON"><code class="language-PYTHON">&gt;&gt;&gt; import torch.nn.functional as F&gt;&gt;&gt; kl_loss &#x3D; nn.KLDivLoss(reduction&#x3D;&quot;batchmean&quot;)&gt;&gt;&gt; # input should be a distribution in the log space&gt;&gt;&gt; input &#x3D; F.log_softmax(torch.randn(3, 5, requires_grad&#x3D;True), dim&#x3D;1)&gt;&gt;&gt; # Sample a batch of distributions. Usually this would come from the dataset&gt;&gt;&gt; target &#x3D; F.softmax(torch.rand(3, 5), dim&#x3D;1)&gt;&gt;&gt; output &#x3D; kl_loss(input, target)&gt;&gt;&gt; kl_loss &#x3D; nn.KLDivLoss(reduction&#x3D;&quot;batchmean&quot;, log_target&#x3D;True)&gt;&gt;&gt; log_target &#x3D; F.log_softmax(torch.rand(3, 5), dim&#x3D;1)&gt;&gt;&gt; output &#x3D; kl_loss(input, log_target)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="回归"><a href="#回归" class="headerlink" title="回归"></a>回归</h2><h3 id="L1-loss-MAE-loss"><a href="#L1-loss-MAE-loss" class="headerlink" title="L1 loss (MAE loss)"></a>L1 loss (MAE loss)</h3><blockquote><p>torch.nn.L1Loss(size_average=None, reduce=None, reduction=’mean’)</p></blockquote><p><img src="/images/l1_loss.png" width="70%" height="70%"/></p><pre class="line-numbers language-PYTHON" data-language="PYTHON"><code class="language-PYTHON">&gt;&gt;&gt; loss &#x3D; nn.L1Loss()&gt;&gt;&gt; input &#x3D; torch.randn(3, 5, requires_grad&#x3D;True)&gt;&gt;&gt; target &#x3D; torch.randn(3, 5)&gt;&gt;&gt; output &#x3D; loss(input, target)&gt;&gt;&gt; output.backward()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="L2-loss-MSE-loss"><a href="#L2-loss-MSE-loss" class="headerlink" title="L2 loss (MSE loss)"></a>L2 loss (MSE loss)</h3><blockquote><p>torch.nn.MSELoss(size_average=None, reduce=None, reduction=’mean’)</p></blockquote><p><img src="/images/MSE_loss.png"/></p><pre class="line-numbers language-PYTHON" data-language="PYTHON"><code class="language-PYTHON">&gt;&gt;&gt; loss &#x3D; nn.MSELoss()&gt;&gt;&gt; input &#x3D; torch.randn(3, 5, requires_grad&#x3D;True)&gt;&gt;&gt; target &#x3D; torch.randn(3, 5)&gt;&gt;&gt; output &#x3D; loss(input, target)&gt;&gt;&gt; output.backward()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="Smooth-L1-loss"><a href="#Smooth-L1-loss" class="headerlink" title="Smooth L1 loss"></a>Smooth L1 loss</h3><blockquote><p>torch.nn.SmoothL1Loss(size_average=None, reduce=None, reduction=’mean’, beta=1.0)</p></blockquote><p><img src="/images/smooth_l1.png" width="70%" height="70%"/></p><p>优点：Smooth L1 loss主要是在预测结果和ground-truth相差大时起到限制梯度的作用。在$x$较小时，对$x$的梯度也会变小，而在$x$很大时，对$x$的梯度的绝对值达到上限 1，也不会太大以至于破坏网络参数。完美地避开了$L_1$和$L_2$损失的缺陷，不会导致梯度爆炸。其函数图像如下<br><img src="/images/smooth_l1_visual.png"/></p><h3 id="曼哈顿距离"><a href="#曼哈顿距离" class="headerlink" title="曼哈顿距离"></a>曼哈顿距离</h3><p>曼哈顿距离实际上就是两个点在标准坐标系上的绝对轴距总和。</p><h3 id="GIoU-loss-Generalized-Intersection-over-Union"><a href="#GIoU-loss-Generalized-Intersection-over-Union" class="headerlink" title="GIoU loss (Generalized Intersection over Union)"></a>GIoU loss (Generalized Intersection over Union)</h3><p>GIoU loss即泛化的IoU损失。当预测框与真实框之间没有任何重叠时，两个边框的交集（分子）为0，此时IoU损失为0，因此IoU无法算出两者之间的距离（重叠度）。另一方面，由于IoU损失为零，意味着梯度无法有效地反向传播和更新，即出现梯度消失的现象，致使网络无法给出一个优化的方向。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://pytorch.org/docs/">https://pytorch.org/docs/</a><br><a href="https://www.zhihu.com/question/58200555/answer/621174180">https://www.zhihu.com/question/58200555/answer/621174180</a><br><a href="https://zhuanlan.zhihu.com/p/552132010">https://zhuanlan.zhihu.com/p/552132010</a></p>]]></content>
      
      
      <categories>
          
          <category> 原理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> BEV Perception </tag>
            
            <tag> Pytorch </tag>
            
            <tag> Loss </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Online HD Map Construction 整理</title>
      <link href="/2023/03/05/paper-online-hdmap-construction-zheng-li/"/>
      <url>/2023/03/05/paper-online-hdmap-construction-zheng-li/</url>
      
        <content type="html"><![CDATA[<p>本文总结了当前在线地图构造任务的相关工作<br><span id="more"></span></p><h2 id="方向简介"><a href="#方向简介" class="headerlink" title="方向简介"></a>方向简介</h2><p>高精地图构建的传统方案需要事先采集点云，使用SLAM方法离线构造全局一致的地图，并在地图上标注语义信息。这样的流程虽然能制作精确的高精地图，但采集、标注和维护都需要大量的人力。近年来，虽然有一些车道线检测的工作是基于车载传感器实现的，但车道线检测只是地图生成的一个子任务，并且相关数据集往往只提供front-view的数据，且只提供线性元素的标注数据，相关的研究工作也非常受限。恰逢BEV和多传感器融合技术的发展，一些工作尝试使用车载传感器进行在线的局部地图学习。<br>由于传感器输出的数据和目标地图位于不同的坐标系，仅使用车载传感器作为模型输入尤其颇具挑战性。起初也是把在线地图学习作为一个分割任务来学习，主要操作是将地图栅格化为像素，然后给每个像素分配一个类别标签。<br>但是栅格地图并不是自动驾驶任务最合适的表示形式：</p><ol><li>栅格地图缺乏instance信息，对于区分具有相同类标签但语义不同的地图元素存在困难（如左边界和右边界）</li><li>难以在预测的栅格化地图中强制执行空间一致性，例如，邻近的像素的语义或者几何形状可能是冲突的</li><li>与下游任务不兼容，自动驾驶中后续的预测和规划模块多使用instance级别的2D/3D矢量化地图</li></ol><p>近期出现了一些工作<strong>使用车载传感器实时生成矢量化的局部语义地图</strong>。这种工作并不旨在取代全局高清地图重建，而是提供一种简单的方法来预测局部语义地图，用于实时运动预测和规划。<br><img src="/images/map_construction_comparison.png"/></p><center>图1 全局高精地图构建和局部语义地图构建的对比</center>矢量化的高精地图不需要密集的语义像素，它将地图元素表示为一组折线，这些折线与自动驾驶场景的下游任务密切相关，如运动预测等。可以通过图2清楚的看到三种不同方案的对比效果<img src="/images/three_methods_comparison.png"/><center>图2 三种地图构造方案的对比</center><p>具体来说，使用多段线表示地图元素的优点如下：</p><ol><li>高精地图通常由不同几何图形混合组成，例如点、线、曲线和多边形。折线是一种灵活的基元，可以有效地表示这些几何元素</li><li>折线顶点的顺序是编码地图元素方向的自然方式，这对驾驶至关重要</li><li>折线表示已经广泛应用于下游自动驾驶模块，如运动预测等</li></ol><p>因此，地图学习任务也可以归结为从传感器数据中预测一个稀疏的折线集合（set prediction）。本文将对这个领域的相关论文进行具体的介绍。</p><h2 id="现有主要论文"><a href="#现有主要论文" class="headerlink" title="现有主要论文"></a>现有主要论文</h2><h3 id="202203-HDMapNet"><a href="#202203-HDMapNet" class="headerlink" title="[202203] HDMapNet"></a>[202203] HDMapNet</h3><p><strong>Title</strong>：HDMapNet: An Online HD Map Construction and Evaluation Framework<br><strong>TL;DR</strong>: HDMapNet遵循常规的语义分割pipeline的密集预测方案，在后处理中通过启发式的聚类算法得到矢量化的地图元素。<br><strong>Motivation</strong>: 高精地图生成的传统方案需要大量的人力物力去标注和维护地图的语义信息，限制了其规模。<br><strong>Result</strong>:<br><img src="/images/hdmapnet_result.png"/></p><center>图3 HDMapNet 在nuScenes val set的效果</center><h4 id="Pipeline"><a href="#Pipeline" class="headerlink" title="Pipeline"></a>Pipeline</h4><p>HDMapNet本质上还是将在线地图学习任务看作是一个语义分割任务，这种任务非常适合使用卷积+全连接网络进行求解。模型Pipeline如图4所示<br><img src="/images/hdmapnet.png"/></p><center>图4 HDMapNet Pipeline</center><ol><li>PV Encoder<br>在视角转换部分，HDMapNet是使用MLP讲特征从图像坐标系转到相机坐标系，然后使用IPM投影将其转到自车坐标系</li><li>LiDar Encoder<br>参考的是PointPillar的方案</li><li>BEV Decoder<br>通过三个卷积block得到语义图、实例图和方向图三个输出。<ul><li>语义图是pixel-level的分类结果。</li><li>实例图是。</li><li>方向图预测的是pixel-level地图元素的方向。方向共划分了$N_d$个类别。</li></ul></li><li>通过手工设计的后处理算法将这三种地图矢量化。</li></ol><h4 id="Loss设计"><a href="#Loss设计" class="headerlink" title="Loss设计"></a>Loss设计</h4><script type="math/tex; mode=display">L=L_{seg}+L_{ins\_embedding}</script><p>$L_{seg}$是semantic prediction模块中使用的CE loss。<br>$L_{ins_embedding}$是instance embedding模块中对每个bev embedding进行聚类的损失，$L_{var}$和$L_{dist}$分别是方差和距离的损失，二者都是L2 loss</p><script type="math/tex; mode=display">L_{ins\_embedding}=L_{var}+L_{dist}</script><p>对于点集中点的匹配关系，使用的是Chamfer distance进行度量</p><script type="math/tex; mode=display">CD_{Dir}(S_1, S_2)=\frac{1}{S_1}\sum_{x \in S_1}min_{y\in S_2}||x-y||_2</script><script type="math/tex; mode=display">CD(S_1, S_2)=CD_{Dir}(S_1, S_2) + CD_{Dir}(S_2, S_1)</script><h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><ol><li>率先提出了矢量化的在线高精地图构造思路</li></ol><h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4><ol><li>实际上还是依赖栅格化的地图预测</li><li>启发式后处理步骤使整个pipeline变得复杂，也非常耗时，限制了模型的可伸缩性和性能</li></ol><h3 id="202206-VectorMapNet"><a href="#202206-VectorMapNet" class="headerlink" title="[202206] VectorMapNet"></a>[202206] VectorMapNet</h3><p><strong>Title</strong>：VectorMapNet: End-to-end Vectorized HD Map Learning<br><strong>TL;DR</strong>：VectorMapNet提出了第一个端到端的矢量地图构造方案，无需任何后处理操作。<br><strong>Motivation</strong>: 现有的在线地图学习方案要么是基于栅格地图预测的，没有instance信息；要么就需要复杂的后处理步骤。<br><strong>Result</strong>:<br><img src="/images/vectormapnet_result.png"/></p><center>图5 VectorMapNet 在nuScenes val set的效果(阈值为$\{0.5, 1.0, 1.5\}$)</center><h4 id="Pipeline-1"><a href="#Pipeline-1" class="headerlink" title="Pipeline"></a>Pipeline</h4><p>VectorMapNet将在线地图生成任务描述为一个稀疏的集合任务，这种任务非常适合使用Transformer进行求解，这个任务也可以顺理成章地转换成检测任务进行处理。</p><p>VectorMapNet是一个两阶段的方案，第一阶段是一个set prediction任务，预测策略的关键点；第二阶段是一个sequence generation任务，按顺序预测地图元素的下一个点。VectorMapNet的pipeline如图6所示<br><img src="/images/vectormapnet_pipeline.png"/></p><center>图6 VectorMapNet Pipeline</center><p>VectorMapNet首先将不同模态的数据整合到BEV特征空间，然后基于可学习的query检测地图元素的位置和类别，最后将每个地图元素解码成polyline。每个模块具体实现的拆解如下</p><ol><li>BEV Feature Extractor<br>将传感器得到的数据（相机图像）转换到鸟瞰图视角（BEV）中得到BEV features。这里使用的也是IPM的方案。</li><li>Map Element Detector<br>主要的目的就是定位每一个地图元素的位置，大致形状以及所属类别。这里论文中称其将每一个地图元素用多个关键点（Keypoints）来表示，实际上就是用bounding box封装了一下</li><li>Polyline Generator<br>这个模块根据检测模块检测到的关键点和类别信息来预测每个地图元素的具体的局部几何形状，将每一个地图元素补充完整。模块推断用公式描述为<script type="math/tex; mode=display">p(V_i^{poly}|a_i,l_i,\mathcal{F}_{BEV};\theta)= \prod_{n=1}^{2N_v}p(v_{i, n}^{f}|v_{i,<n}^{f},a_i,l_i,\mathcal{F}_{BEV})</script></li></ol><h4 id="loss设计"><a href="#loss设计" class="headerlink" title="loss设计"></a>loss设计</h4><p>loss设计包括两个部分，分别是检测损失和生成损失</p><script type="math/tex; mode=display">\mathcal{L} = \mathcal{L}_{det} + \mathcal{L}_{gen}</script><p>其中，$\mathcal{L}_{det}$是由两个个部分组成的损失函数，包括关键点回归损失和分类损失，$\mathcal{L}_{gen}$则是交叉熵损失。<br>由于生成器是一个序列生成模型，采用教师指导（teacher-forcing）训练方法进行训练。</p><h4 id="优点-1"><a href="#优点-1" class="headerlink" title="优点"></a>优点</h4><ol><li>无须复杂的后处理过程，能够直接端到端的得到矢量地图元素</li><li>将地图构造问题转成set prediction的问题，使得基于transformer的方案能够轻松移植过来</li></ol><h4 id="缺点-1"><a href="#缺点-1" class="headerlink" title="缺点"></a>缺点</h4><ol><li>基于box的封装方式，一方面不能准确表示元素的形状；另一方面，box对于线性目标会退化（尤其是驾驶场景中元素常与坐标轴平行）</li><li>有向折线表示地图元素和顺序预测点会引起定义歧义</li><li>以循环的方式预测点，并采用级联的由粗到细框架，导致较长的推理时间和对实时场景的有限可伸缩性。且自回归解码器的周期性导致了累积误差的问题，需要更长的训练时间才能收敛</li></ol><h3 id="202208-MapTR"><a href="#202208-MapTR" class="headerlink" title="[202208]MapTR"></a>[202208]MapTR</h3><p><strong>Title</strong>：MapTR: Structured Modeling and Learning for Online Vectorized HD Map Construction<br><strong>TL;DR</strong>: MapTR直接建模地图元素的元素-点的关系，采用类似DETR的方式直接回归每个元素的每个点，并提出了针对地图元素的等价定义问题。<br><strong>Motivation</strong>: 针对当下地图元素的定义中存在的歧义性，提出了基于排列的方案；遵循DETR的范式，将在线地图构造任务提到了实时级的水平。<br><strong>Result</strong>:<br><img src="/images/maptr_result.png"/></p><center>图7 MapTR 在nuScenes val set的效果(阈值为$\{0.5, 1.0, 1.5\}$)</center><p>注：MapTR是目前Online HD Map Construction任务的SOTA，另外写了一篇<a href="https://massive11.github.io/2022/12/10/paper-maptr-lun-wen-yu-dai-ma-jie-du/">MapTR论文与代码解读</a>给出了详细的介绍。</p><h4 id="Pipeline-2"><a href="#Pipeline-2" class="headerlink" title="Pipeline"></a>Pipeline</h4><p>MapTR是一个完全的set prediction任务，几乎可以看作是BEVFormer+DETR head的方案。<br>MapTR将每个地图元素视为一个有一组等价排列方式的点集，首先采用常规的view transformation方案将6帧图像转到BEV视角下，然后使用层级式的二分匹配设计，依次进行instance-level、point-level的匹配，其pipeline如图8所示<br><img src="/images/maptr_structure.png"/></p><center>图8 MapTR Pipeline</center><ol><li>Map Encoder<br>这里用的应该是作者实验室团队的GKT，原理和BEVFormer的Spatial cross attention基本一样。实际上可以根据自己的需要替换成其他模块，作者在消融实验中也给出了不同的视角转换模块的性能对比，比较有参考意义<br><img src="/images/maptr_ablation1.png" width="60%" height="60%"/></li><li>Map Decoder<br>DETR的Decoder设计，比较特别的是MapTR在初始化每个Query的时候就直接赋予了结构化关系，也即文中所说的hierarchical query，因此能够直接回归每个地图元素的每个点。其注意力操作中的cross attention也是比较常规的deformable attention，即每个query只与其对应的参考点位置的BEV特征进行交互。这样做的好处就是避免了类似VectorMapNet中需要两阶段才能得到的完整的矢量化的地图数据。</li><li>Hierarchical Match<br>根据之前的操作，已经能够得到多个由有序的点集组成的地图元素，这个模块就是解决有序点集与真值的匹配问题。如前文所述，每个Query在初始化的时候就已经固定了其所属的instance信息，因此这里的匹配包含两部分内容：元素匹配和元素中点的匹配。由于真值和推理结果都是有序的点集，可以直接将元素中所有对应点的距离累加作为两个元素之间的差异。<br>在这个部分，MapTR提出了它们的核心思路，也就是对于地图构造任务来说，真值中有序点集的顺序并不是唯一正确的顺序。因此，MapTR扩展了单个有序的真值点集为其等价定义类。根据预测值与等价定义类的二分匹配结果确定最终的匹配。</li></ol><h4 id="loss"><a href="#loss" class="headerlink" title="loss"></a>loss</h4><p>MapTR损失由分类损失、点对点的损失和方向损失三部分组成</p><script type="math/tex; mode=display">\mathcal{L}=\lambda\mathcal{L}_{cls}+\alpha\mathcal{L}_{p2p}+\beta\mathcal{L}_{dir}</script><p>分类损失是Focal loss。点对点的损失限制的是每个点的位置，使用的是Manhattan距离。点对点的损失仅限制折线和多边形的节点，不考虑边(相邻点之间的连接线)。但是对于地图元素来说，边的方向也同样重要，方向损失使用余弦相似度进行度量。</p><h4 id="优点-2"><a href="#优点-2" class="headerlink" title="优点"></a>优点</h4><ol><li>SOTA，精度高的同时速度非常快</li><li>提出了地图构造问题中组成每个元素的有序点集的排列等价性，从实验中可以看出有效提升了模型性能。</li></ol><h4 id="缺点-2"><a href="#缺点-2" class="headerlink" title="缺点"></a>缺点</h4><ol><li>等数量插值，模型性能会受每个元素的点集数量的影响</li><li>元素之间、元素内部的结构关系约束比较弱，可能因此导致模型收敛速度慢</li></ol><h3 id="202301-InstaGraM"><a href="#202301-InstaGraM" class="headerlink" title="[202301]InstaGraM"></a>[202301]InstaGraM</h3><p><strong>Title</strong>：InstaGraM: Instance-level Graph Modeling for Vectorized HD Map Learning<br><strong>TL;DR</strong>: 分别预测vertex map和edge map，然后通过instance-level的图建模构造矢量化地图元素。<br><strong>Motivation</strong>: 现有的方法要么依赖后处理，要么依赖自回归，速度都比较慢。（本文没有提及MapTR，实际上二者速度应该是差不多的，但是MapTR的精度更高）<br><strong>Result</strong>:<br><img src="/images/instagram_result.png"/></p><center>图9 InstaGraM 在nuScenes val set的效果(阈值为$\{0.5, 1.0, 1.5\}$)</center><h4 id="Pipeline-3"><a href="#Pipeline-3" class="headerlink" title="Pipeline"></a>Pipeline</h4><p><img src="/images/instagram_pipeline.png"/></p><center>图10 InstaGraM Pipeline</center><ol><li>BEV feature extractor<br>参考HDMapNet的IPM方式</li><li>Element detector heads<br>使用两个CNN decoder分别提取vertecies和edge的特征信息。vertecies预测的是顶点的heatmap，edge预测的是distance transform map</li><li>Association<br>这部分是使用GNN关联上一个得到的vertecies和edge的信息。</li></ol><h4 id="优点-3"><a href="#优点-3" class="headerlink" title="优点"></a>优点</h4><ol><li>提出图建模的思想解决矢量地图构造问题</li><li>速度非常快</li></ol><h4 id="缺点-3"><a href="#缺点-3" class="headerlink" title="缺点"></a>缺点</h4><ol><li>先检测点，再通过图连接，这种二阶段方式在交通密集的场景效果会比较差，缺乏一定的关联推理能力</li><li>代码未开源</li></ol><h2 id="性能评价指标"><a href="#性能评价指标" class="headerlink" title="性能评价指标"></a>性能评价指标</h2><p>早期基于分割的方案使用的是IoU的评价指标，自VectorMapNet将该问题描述为检测问题后，比较常用的评价指标是基于距离的评价指标mAP。<br>判断正负样例使用的是几何相似性，使用的chamfer distance。chamfer distance衡量两个无序点集的相似性，考虑了元素点集的所有排列方式，公式定义如下</p><script type="math/tex; mode=display">D_{chamfer}(S_1, S_2)=\frac{1}{2}(\frac{1}{|S_1|}\sum_{p\in S_1}min_{q\in S_2}||p,q||_2+\frac{1}{|S_2|}\sum_{q\in S_2}min_{p\in S_1}||q,p||_2)</script><p>推理结果和真值都采样成100个点，分别在$\{0.5, 1.0, 1.5\}$距离阈值下计算AP。取各类别在各阈值下的平均值作为最终的结果。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://arxiv.org/pdf/2107.06307.pdf">HDMapNet Paper</a><br><a href="https://arxiv.org/pdf/2206.08920.pdf">VectorMapNet Paper</a><br><a href="https://arxiv.org/pdf/2208.14437.pdf">MapTR Paper</a><br><a href="https://arxiv.org/pdf/2301.04470.pdf">InstaGraM Paper</a><br><a href="https://zhuanlan.zhihu.com/p/574687904">https://zhuanlan.zhihu.com/p/574687904</a></p>]]></content>
      
      
      <categories>
          
          <category> 系统梳理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Multi-view 3D </tag>
            
            <tag> HD Map </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Fast-BEV论文解读</title>
      <link href="/2023/02/12/paper-fast-bev-lun-wen-jie-du/"/>
      <url>/2023/02/12/paper-fast-bev-lun-wen-jie-du/</url>
      
        <content type="html"><![CDATA[<p>Fast-BEV论文解读<br><span id="more"></span></p><h2 id="Fast-BEV"><a href="#Fast-BEV" class="headerlink" title="Fast-BEV"></a>Fast-BEV</h2><p>Fast-BEV的整体结构如图1所示<br><img src="/images/fast_bev.png"/></p><center>图1 Fast-BEV网络结构</center><p>Fast-BEV是在M$^2$BEV的基础上，进行了三方面的改进</p><ol><li>为了防止过拟合，在图像空间和BEV空间都进行了有力的数据增强策略</li><li>利用时序信息的多帧融合机制</li><li>部署友好的视角转换方案</li></ol><p>前两方面改进实际上都是比较常见的，文中主要是第三点改进能够显著提高2D图像到3D特征的视角转换速度。M$^2$BEV假设沿着光线的深度分布是均匀的。基于此，一旦得到了相机的内部/外部参数，就可以很容易地知道2D到3D投影。由于这里没有使用可学习的参数，可以很容易地计算2D特征图上的点与BEV特征图之间的对应矩阵。<br>投影索引是从2D图像空间到3D体素空间的映射索引。由于该方案不依赖于数据相关的深度预测或变换器，所以投影索引对于每个输入都是相同的。因此，可以预先计算固定的投影索引并存储它。在推理阶段，可以通过查询查询表获得投影索引，这在边缘设备上是非常“便宜”的操作。此外，如果从单帧扩展到多帧，也可以轻松预先计算内在和外在参数并将其预先对齐到当前帧。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://arxiv.org/pdf/2301.07870.pdf">Fast-BEV Paper</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Multi-view 3D </tag>
            
            <tag> BEV Perception </tag>
            
            <tag> Attention </tag>
            
            <tag> Deploy </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Persformer论文与代码解读</title>
      <link href="/2023/02/03/paper-persformer-lun-wen-jie-du/"/>
      <url>/2023/02/03/paper-persformer-lun-wen-jie-du/</url>
      
        <content type="html"><![CDATA[<p>Persformer论文与代码解读<br><span id="more"></span></p><h2 id="Persformer"><a href="#Persformer" class="headerlink" title="Persformer"></a>Persformer</h2><p>Persformer是一个针对3D车道线检测的模型，在介绍具体模型设计之前，由于3D Lane和HD Map Construction在一定程度上比较接近，顺便插播一下二者的异同<br>同：</p><ol><li>task的对象都包括车道线；</li><li>都得到的是车载坐标系下的结果，即包含从2D-3D的一个转换过程，而不同于传统的图像上的2D车道线检测/分割任务。</li></ol><p>异：</p><ol><li>3D Lane的数据集和HD Map Construction的数据集不太一样。前者主要是OpenLane等3D车道线数据集，这些数据集通常仅提供前视单目的图像，数据类别通常只有车道线、路沿线等长线性类别。后者主要是nuScenes数据集，nuScenes数据集提供周视多相机的图像，能够覆盖360度的视角，数据类别包括车道线、路沿线、人行横道等；</li><li>受制于上述数据集差异，3D Lane仅针对3D车道线，输出结果是单目视角的车道线每个位置的3D坐标；HD Map Construction通常包括车道线、路沿线和人性横道，输出结果是360度范围内每个元素每个位置的2D坐标（a.因为nuScenes只提供了BEV的2D坐标，并没有高度；b.Argoverse数据集虽然提供了周视3D地图元素标注，但是元素类别存在重合，即一个元素可能即是车道分界线也是路沿线，对模型结果有一定影响）；</li><li>由于3D Lane仅针对3D车道线，基于这个task的模型常常采用基于anchor的方案，因为单个车道线不会与y轴同一位置有两个不同的交点，元素形状实际上比较理想的。但是HD Map Construction的地图元素十分“自由”，路沿线和人行横道都包括封闭图形，检测的难度更大，很难使用这种anchor封装；</li><li>3D Lane的数据集可以得到在图像上的2D标注结果，因此3D Lane的模型常常借助2D和3D联合检测的结果。正如前文所示，nuScenes只提供了BEV的2D坐标，并没有高度，因此无法得到图像上的2D信息，基本没有采用联合检测的方案。</li></ol><p>以上差异导致两个task的模型设计非常不一样。</p><p>Persformer的整体思路简单来说就是使用 Transformer 进行2D 和 3D 车道线的联合检测，其整体结构如图1所示<br><img src="/images/persformer.png"/></p><center>图1 Persformer网络结构</center><p>Persformer的核心是从前视图到 BEV 空间的空间特征变换，以便通过关注参考点周围的局部上下文，在目标点生成的 BEV 特征将更具代表性。Persformer由三个部分组成，backbone、Perspective Transformer和lane detection heads。backbone仍然是常规的ResNet，以下将详细介绍后两个部分。</p><h3 id="Perspective-Transformer"><a href="#Perspective-Transformer" class="headerlink" title="Perspective Transformer"></a>Perspective Transformer</h3><p>Persformer的视角转换模块依赖相机的内外参，这个模块和BEVFormer的思想基本一致。初始化BEV Queries，通过deformable attention的方式实现BEV Queries和对应的2D特征之间的交互。实现方式也遵循DETR的范式，包括self-attention和cross-attention两个部分。<br>cross-attention模块中，BEV空间中的点$(x, y)$通过中间态$(x’, y’)$投射前视图中的对应点$(u, v)$；通过学习偏移量，网络学习从绿色矩框到黄色目标参考点之间的映射，以及相关的蓝色框作为Transformer的key。图解如下图所示<br><img src="/images/persformer_cross_attention.png" width="60%" height="60%"/></p><h3 id="Lane-detection-Head"><a href="#Lane-detection-Head" class="headerlink" title="Lane detection Head"></a>Lane detection Head</h3><h4 id="Unified-Anchor-Design"><a href="#Unified-Anchor-Design" class="headerlink" title="Unified Anchor Design"></a>Unified Anchor Design</h4><p><img src="/images/persformer_unified_anchor.png" width="60%" height="60%"/></p><blockquote><p>Unifying anchor design in 2D and 3D. We first put curated anchors (red) in the BEV space (left), then project them to the front view (right). Offset $x^i_k$ and $u^i_k$ (dashed line) are predicted to match ground truth (yellow and green) to anchors. The correspondence is thus built, and features are optimized together.</p></blockquote><p>这个部分对于3D和2D的统一设计的方式写的有点模糊，官方的知乎文章也是有点粗糙的。博主本人也没仔细研究这一块的代码，就不做展开介绍了。</p><h4 id="3D-Lane-Head"><a href="#3D-Lane-Head" class="headerlink" title="3D Lane Head"></a>3D Lane Head</h4><p>2D/3D 检测头参考的是LaneATT和3D-LaneNet，这里着重介绍3D lane head。</p><div class="table-container"><table><thead><tr><th>组件</th><th>类型</th><th>tensor size</th></tr></thead><tbody><tr><td>卷积层</td><td>3*Conv2d(kerner_size=3)+9*Conv2d(kerner_size=5)</td><td>[B, 64, 50, 25]—&gt;—[B, 64, 8, 25]</td></tr><tr><td>reshape</td><td>x = x.reshape(sizes[0], sizes[1] * sizes[2], sizes[3], 1)</td><td>[B, 64, 8, 25]—&gt;—[B, 512, 25, 1]</td></tr><tr><td>卷积层</td><td>2*Conv2d(kerner_size=5)</td><td>[B, 512, 25, 1]—&gt;—[B, 30, 25, 1]</td></tr><tr><td>reshape</td><td>x = x.squeeze(-1).transpose(1, 2)</td><td>[B, 30, 25, 1]—&gt;—[B, 25, 30]</td></tr><tr><td>sigmoid</td><td>x[:, :, :2 <em> self.num_y_steps] = torch.sigmoid(x[:, :, :2 </em> self.num_y_steps])</td><td>[B, 25, 30]</td></tr></tbody></table></div><h4 id="3D-Lane-Head-Loss"><a href="#3D-Lane-Head-Loss" class="headerlink" title="3D Lane Head Loss"></a>3D Lane Head Loss</h4><ol><li>针对车道线坐标偏移的预测，计算L1 loss（normalized的结果）</li><li>针对visibility的预测，计算CE loss</li><li>针对车道线类别的预测，计算CE loss</li></ol><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://arxiv.org/pdf/2203.11089.pdf">Persformer Paper</a><br><a href="https://github.com/OpenDriveLab/PersFormer_3DLane">Persformer Code</a><br><a href="https://patrick-llgc.github.io/Learning-Deep-Learning/paper_notes/persformer.html">patrick-llgc blog</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> BEV Perception </tag>
            
            <tag> Mono 3D </tag>
            
            <tag> 3D-Lane </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MapTR论文与代码解读</title>
      <link href="/2022/12/10/paper-maptr-lun-wen-yu-dai-ma-jie-du/"/>
      <url>/2022/12/10/paper-maptr-lun-wen-yu-dai-ma-jie-du/</url>
      
        <content type="html"><![CDATA[<p>本文对MapTR的论文和代码进行了解读。<br><span id="more"></span></p><p>本文主要改进的地方在于针对地图元素的数据结构进行了instance-point点集的层级式设计，模型直接预测了每个地图元素的点集，而非HDMapNet类的方法去预测分割的tensor。Nuscenes Map所提供的数据本身也就是点集的形式，以下将首先对Nuscenes Map数据集进行简要介绍。</p><h2 id="Nuscenes-Map"><a href="#Nuscenes-Map" class="headerlink" title="Nuscenes Map"></a>Nuscenes Map</h2><p>Nuscenes地图分为两个图层，几何图层和非几何图层，几何图层包括（polygon, line, node）,　非几何图层包括(drivable_area, road_segment, road_block, lane, ped_crossing, walkway, stop_line, ‘carpark_area’, ‘road divider’, ‘lane divider’, ‘traffic light’)<br>下载<a href="https://www.nuscenes.org/nuscenes#download">Nuscenes Map expansion</a>可以看到地图元素的细节描述，以下以expansion/boston-seaport.json为例进行具体分析<br><pre class="line-numbers language-json" data-language="json"><code class="language-json"><span class="token punctuation">&#123;</span>  <span class="token property">"polygon"</span><span class="token operator">:</span> <span class="token punctuation">[</span>    <span class="token punctuation">&#123;</span>      <span class="token property">"token"</span><span class="token operator">:</span> <span class="token string">"1b161e64-fe37-4f3f-96db-299edefe9f8c"</span><span class="token punctuation">,</span>       <span class="token property">"exterior_node_tokens"</span><span class="token operator">:</span> <span class="token punctuation">[</span>        <span class="token string">"ee85f1a6-38c4-4491-8a3b-93e1d5bf2911"</span><span class="token punctuation">,</span>         ...      <span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token property">"holes"</span><span class="token operator">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token punctuation">&#125;</span>  <span class="token punctuation">]</span><span class="token punctuation">,</span>   <span class="token property">"line"</span><span class="token operator">:</span> <span class="token punctuation">[</span>    <span class="token punctuation">&#123;</span>      <span class="token property">"token"</span><span class="token operator">:</span> <span class="token string">"7a8fcfed-9c66-475a-8dcf-9efe983acc98"</span><span class="token punctuation">,</span>       <span class="token property">"node_tokens"</span><span class="token operator">:</span> <span class="token punctuation">[</span>        <span class="token string">"fde59d81-e200-4a78-80e7-b8044d417b02"</span><span class="token punctuation">,</span>        <span class="token string">"0cffed4b-c5b7-4cd2-95ec-0d4252a1c896"</span>      <span class="token punctuation">]</span>    <span class="token punctuation">&#125;</span>  <span class="token punctuation">]</span><span class="token punctuation">,</span>   <span class="token property">"node"</span><span class="token operator">:</span> <span class="token punctuation">[</span>    <span class="token punctuation">&#123;</span>      <span class="token property">"token"</span><span class="token operator">:</span> <span class="token string">"16af4f78-e195-4954-bf2a-889e9fa4d751"</span><span class="token punctuation">,</span>      <span class="token property">"x"</span><span class="token operator">:</span> <span class="token number">525.7599033618623</span><span class="token punctuation">,</span>      <span class="token property">"y"</span><span class="token operator">:</span> <span class="token number">752.3132939140847</span>    <span class="token punctuation">&#125;</span>  <span class="token punctuation">]</span><span class="token punctuation">,</span>   ...<span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></p><h2 id="MapTR"><a href="#MapTR" class="headerlink" title="MapTR"></a>MapTR</h2><blockquote><p>We present Map TRansformer, for efficient online vectorized HD map construction. MapTR is a unified permutation-based modeling approach, i.e., modeling map element as a point set with a group of equivalent permutations, which avoids the definition ambiguity of map element and eases learning.</p></blockquote><p>MapTR是一个基于排列的HD Map构造方法，它将每个地图元素视为有着一组等价排列方式的点集，通过这样的方式避免地图元素定义上的歧义。</p><h3 id="The-definition-ambiguity-of-map-element"><a href="#The-definition-ambiguity-of-map-element" class="headerlink" title="The definition ambiguity of map element"></a>The definition ambiguity of map element</h3><p>地图元素可以抽象为多段线（车道线等）和多边形（人行横道等）两种。对于多段线而言，两个端点都可以视作起始点；对于多边形而言，从点集中任何一点以任意方向排列都是合理的。<br><img src="/images/ambiguity_maptr.png"/></p><center>图1 The definition ambiguity of map element</center><h3 id="Permutation-based-modeling-of-MapTR"><a href="#Permutation-based-modeling-of-MapTR" class="headerlink" title="Permutation-based modeling of MapTR"></a>Permutation-based modeling of MapTR</h3><p>为了弥补这种差距，MapTR使用$\mathcal{V}=(V,\Gamma)$建模每个地图元素，$V=\{v_j\}_{j=0}^{N_v-1}$表示该地图元素的点集，$\Gamma=\{\gamma_k \}$表示这个点集的一组等效的排列，包含所有可能的组织序列。<br>对于多段线而言，有两种等价的排列方式；对于多边形而言，有$2N_v$种等价的配列方式。<br><img src="/images/maptr_equal.png"/></p><center>图2 llustration of permutation-based modeling of MapTR</center><h3 id="Hierarchical-matching"><a href="#Hierarchical-matching" class="headerlink" title="Hierarchical matching"></a>Hierarchical matching</h3><p>在此基础上，MapTR进一步引入了层级式的二分匹配，依次执行instance-level和point-level的匹配。<br>instance-level的匹配是在预测的instance和真值的instance之间寻找一个最优的标签分配，参考DETR的方法使用的是匈牙利匹配算法。<br>得到instance-level的结果之后，使用point-level找到最优的点对点的匹配，利用 Manhattan 距离度量。</p><h3 id="Overall-architecture-of-MapTR"><a href="#Overall-architecture-of-MapTR" class="headerlink" title="Overall architecture of MapTR"></a>Overall architecture of MapTR</h3><p>MapTR的整体结构<br><img src="/images/maptr_structure.png"/></p><center>图3 The overall architecture of MapTR</center><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><h3 id="代码模块划分"><a href="#代码模块划分" class="headerlink" title="代码模块划分"></a>代码模块划分</h3><div class="table-container"><table><thead><tr><th>论文模块</th><th>代码模块</th><th>子模块</th><th>transformer layer</th><th>具体类型</th><th>输入维度</th><th>输出维度</th></tr></thead><tbody><tr><td>MapEncoder</td><td>img_backbone</td><td>-</td><td>-</td><td>ResNet50</td><td>[B, N, 3, 480, 800]</td><td>[B, N, 2048,15,25]</td></tr><tr><td></td><td>img_neck</td><td>-</td><td>-</td><td>FPN</td><td>[B, N, 2048,15,25]</td><td>[B, N, 256, 15, 25]</td></tr><tr><td></td><td>MapTRHead</td><td>BEVFormerEncoder</td><td>BEVFormerLayer</td><td>BEVFormer</td><td>[N, 15*25, B, 256][B, 20000, 256]</td><td>[B, 20000, 256]</td></tr><tr><td>MapDecoder</td><td></td><td>MapTRDecoder</td><td>DETRDecoderLayer</td><td>MultiHead Attention</td><td>[50*20, B, 256] [B, 20000, 256]</td><td>[N, 50*20, B, 256]</td></tr><tr><td></td><td></td><td></td><td></td><td>Deformable Attention</td><td></td></tr></tbody></table></div><h3 id="BEVFormerEncoder"><a href="#BEVFormerEncoder" class="headerlink" title="BEVFormerEncoder"></a>BEVFormerEncoder</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> mlvl_feats<span class="token punctuation">,</span> img_metas<span class="token punctuation">,</span> prev_bev<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>  only_bev<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    bs<span class="token punctuation">,</span> num_cam<span class="token punctuation">,</span> _<span class="token punctuation">,</span> _<span class="token punctuation">,</span> _ <span class="token operator">=</span> mlvl_feats<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape  <span class="token comment"># [B, N, 256, 15, 25]</span>    pts_embeds <span class="token operator">=</span> self<span class="token punctuation">.</span>pts_embedding<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>  <span class="token comment"># [1, 20, 512]</span>    instance_embeds <span class="token operator">=</span> self<span class="token punctuation">.</span>instance_embedding<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># [50, 1, 512]</span>    object_query_embeds <span class="token operator">=</span> <span class="token punctuation">(</span>pts_embeds <span class="token operator">+</span> instance_embeds<span class="token punctuation">)</span><span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment"># [1000, 512]</span>        bev_queries <span class="token operator">=</span> self<span class="token punctuation">.</span>bev_embedding<span class="token punctuation">.</span>weight  <span class="token comment"># [200*100, 256]</span>    bev_queries <span class="token operator">=</span> bev_queries<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>repeat<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> bs<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># [200*100, 2, 256]</span>    bev_mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>bs<span class="token punctuation">,</span> self<span class="token punctuation">.</span>bev_h<span class="token punctuation">,</span> self<span class="token punctuation">.</span>bev_w<span class="token punctuation">)</span><span class="token punctuation">,</span> device<span class="token operator">=</span>bev_queries<span class="token punctuation">.</span>device<span class="token punctuation">)</span>  <span class="token comment"># [B, 200, 100]</span>    bev_pos <span class="token operator">=</span> self<span class="token punctuation">.</span>positional_encoding<span class="token punctuation">(</span>bev_mask<span class="token punctuation">)</span><span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># [200*100, B, 256]</span><span class="token comment"># add can bus signals</span>    can_bus <span class="token operator">=</span> bev_queries<span class="token punctuation">.</span>new_tensor<span class="token punctuation">(</span><span class="token punctuation">[</span>each<span class="token punctuation">[</span><span class="token string">'can_bus'</span><span class="token punctuation">]</span> <span class="token keyword">for</span> each <span class="token keyword">in</span> kwargs<span class="token punctuation">[</span><span class="token string">'img_metas'</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># [B, 18]</span>    can_bus <span class="token operator">=</span> self<span class="token punctuation">.</span>can_bus_mlp<span class="token punctuation">(</span>can_bus<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>  <span class="token comment"># [1, B, 256]</span>    bev_queries <span class="token operator">=</span> bev_queries <span class="token operator">+</span> can_bus <span class="token operator">*</span> self<span class="token punctuation">.</span>use_can_bus    <span class="token comment"># [200*100, 2, 256]</span>    feat_flatten <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    spatial_shapes <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> lvl<span class="token punctuation">,</span> feat <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>mlvl_feats<span class="token punctuation">)</span><span class="token punctuation">:</span>        bs<span class="token punctuation">,</span> num_cam<span class="token punctuation">,</span> c<span class="token punctuation">,</span> h<span class="token punctuation">,</span> w <span class="token operator">=</span> feat<span class="token punctuation">.</span>shape        spatial_shape <span class="token operator">=</span> <span class="token punctuation">(</span>h<span class="token punctuation">,</span> w<span class="token punctuation">)</span>        feat <span class="token operator">=</span> feat<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>  <span class="token comment"># [N, B, 15*25, 256]</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>use_cams_embeds<span class="token punctuation">:</span>  <span class="token comment"># camera的所有层级特征</span>            feat <span class="token operator">=</span> feat <span class="token operator">+</span> self<span class="token punctuation">.</span>cams_embeds<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>feat<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span>        feat <span class="token operator">=</span> feat <span class="token operator">+</span> self<span class="token punctuation">.</span>level_embeds<span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> lvl<span class="token punctuation">:</span>lvl <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>feat<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span>        spatial_shapes<span class="token punctuation">.</span>append<span class="token punctuation">(</span>spatial_shape<span class="token punctuation">)</span>  <span class="token comment"># (15, 25)</span>        feat_flatten<span class="token punctuation">.</span>append<span class="token punctuation">(</span>feat<span class="token punctuation">)</span>  <span class="token comment"># [N, B, 15*25, 256]</span>    feat_flatten <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>feat_flatten<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>    feat_flatten <span class="token operator">=</span> feat_flatten<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>  <span class="token comment"># (num_cam, H*W, bs, embed_dims)</span>    spatial_shapes <span class="token operator">=</span> torch<span class="token punctuation">.</span>as_tensor<span class="token punctuation">(</span>spatial_shapes<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">,</span> device<span class="token operator">=</span>bev_pos<span class="token punctuation">.</span>device<span class="token punctuation">)</span>    level_start_index <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>spatial_shapes<span class="token punctuation">.</span>new_zeros<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> spatial_shapes<span class="token punctuation">.</span>prod<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cumsum<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    bev_embed <span class="token operator">=</span> self<span class="token punctuation">.</span>encoder<span class="token punctuation">(</span>            bev_queries<span class="token punctuation">,</span>  <span class="token comment"># Q [200*100, 2, 256]</span>            feat_flatten<span class="token punctuation">,</span> <span class="token comment"># K [N, 15*25, B, 256]</span>            feat_flatten<span class="token punctuation">,</span> <span class="token comment"># V</span>            bev_h<span class="token operator">=</span>bev_h<span class="token punctuation">,</span>            bev_w<span class="token operator">=</span>bev_w<span class="token punctuation">,</span>            bev_pos<span class="token operator">=</span>bev_pos<span class="token punctuation">,</span>  <span class="token comment"># [200*100, B, 256]</span>            spatial_shapes<span class="token operator">=</span>spatial_shapes<span class="token punctuation">,</span>            level_start_index<span class="token operator">=</span>level_start_index<span class="token punctuation">,</span>            prev_bev<span class="token operator">=</span>prev_bev<span class="token punctuation">,</span>            shift<span class="token operator">=</span>shift<span class="token punctuation">,</span>            <span class="token operator">**</span>kwargs        <span class="token punctuation">)</span><span class="token keyword">return</span> bev_embed  <span class="token comment"># [B, 20000, 256]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>            bev_query<span class="token punctuation">,</span>            key<span class="token punctuation">,</span>            value<span class="token punctuation">,</span>            <span class="token operator">*</span>args<span class="token punctuation">,</span>            bev_h<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>            bev_w<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>            bev_pos<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>            spatial_shapes<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>            level_start_index<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>            valid_ratios<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>            prev_bev<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>            shift<span class="token operator">=</span><span class="token number">0.</span><span class="token punctuation">,</span>            <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>    output <span class="token operator">=</span> bev_query  <span class="token comment"># [200*100, 2, 256]</span>    intermediate <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token comment"># reference point in 3D space, used in spatial cross-attention</span>    <span class="token comment"># BEVFormer的设计，bev query查询reference point周围的RoI，每个query对应4个不同高度的point，每个point采样4个特征点。</span>    <span class="token comment"># 通过只对参考点附近的位置进行采样，减少计算量，提高模型的收敛速度</span>    ref_3d <span class="token operator">=</span> self<span class="token punctuation">.</span>get_reference_points<span class="token punctuation">(</span>          bev_h<span class="token punctuation">,</span> bev_w<span class="token punctuation">,</span> self<span class="token punctuation">.</span>pc_range<span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token operator">-</span>self<span class="token punctuation">.</span>pc_range<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_points_in_pillar<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token string">'3d'</span><span class="token punctuation">,</span> bs<span class="token operator">=</span>bev_query<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>  device<span class="token operator">=</span>bev_query<span class="token punctuation">.</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>bev_query<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span>    <span class="token comment"># reference point in 2D BEV space, used in temporal cross-attention</span>    ref_2d <span class="token operator">=</span> self<span class="token punctuation">.</span>get_reference_points<span class="token punctuation">(</span>        bev_h<span class="token punctuation">,</span> bev_w<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token string">'2d'</span><span class="token punctuation">,</span> bs<span class="token operator">=</span>bev_query<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> device<span class="token operator">=</span>bev_query<span class="token punctuation">.</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>bev_query<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span>    reference_points_cam<span class="token punctuation">,</span> bev_mask <span class="token operator">=</span> self<span class="token punctuation">.</span>point_sampling<span class="token punctuation">(</span>ref_3d<span class="token punctuation">,</span> self<span class="token punctuation">.</span>pc_range<span class="token punctuation">,</span> kwargs<span class="token punctuation">[</span><span class="token string">'img_metas'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token comment"># reference_points_cam[N, B, 20000, 4, 2] 参考点的图像坐标，空间范围是20000*4</span>    <span class="token comment"># bev_mask[N, B, 20000, 4] 3D点在图像上是否可见</span>    bev_query <span class="token operator">=</span> bev_query<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>    bev_pos <span class="token operator">=</span> bev_pos<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>    bs<span class="token punctuation">,</span> len_bev<span class="token punctuation">,</span> num_bev_level<span class="token punctuation">,</span> _ <span class="token operator">=</span> ref_2d<span class="token punctuation">.</span>shape        hybird_ref_2d <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span><span class="token punctuation">[</span>ref_2d<span class="token punctuation">,</span> ref_2d<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>bs<span class="token operator">*</span><span class="token number">2</span><span class="token punctuation">,</span> len_bev<span class="token punctuation">,</span> num_bev_level<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>    <span class="token keyword">for</span> lid<span class="token punctuation">,</span> layer <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>layers<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment"># DETR Decoder layer</span>        output <span class="token operator">=</span> layer<span class="token punctuation">(</span>            bev_query<span class="token punctuation">,</span>            key<span class="token punctuation">,</span>            value<span class="token punctuation">,</span>            <span class="token operator">*</span>args<span class="token punctuation">,</span>            bev_pos<span class="token operator">=</span>bev_pos<span class="token punctuation">,</span>            ref_2d<span class="token operator">=</span>hybird_ref_2d<span class="token punctuation">,</span>            ref_3d<span class="token operator">=</span>ref_3d<span class="token punctuation">,</span>            bev_h<span class="token operator">=</span>bev_h<span class="token punctuation">,</span>            bev_w<span class="token operator">=</span>bev_w<span class="token punctuation">,</span>            spatial_shapes<span class="token operator">=</span>spatial_shapes<span class="token punctuation">,</span>            level_start_index<span class="token operator">=</span>level_start_index<span class="token punctuation">,</span>            reference_points_cam<span class="token operator">=</span>reference_points_cam<span class="token punctuation">,</span>            bev_mask<span class="token operator">=</span>bev_mask<span class="token punctuation">,</span>            prev_bev<span class="token operator">=</span>prev_bev<span class="token punctuation">,</span>            <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>        bev_query <span class="token operator">=</span> output    <span class="token keyword">return</span> output  <span class="token comment"># [B, 20000, 256]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="MapTRDecoder"><a href="#MapTRDecoder" class="headerlink" title="MapTRDecoder"></a>MapTRDecoder</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>            mlvl_feats<span class="token punctuation">,</span>            bev_queries<span class="token punctuation">,</span>            object_query_embed<span class="token punctuation">,</span>            bev_h<span class="token punctuation">,</span>            bev_w<span class="token punctuation">,</span>            grid_length<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">0.512</span><span class="token punctuation">,</span> <span class="token number">0.512</span><span class="token punctuation">]</span><span class="token punctuation">,</span>            bev_pos<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>            reg_branches<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>            cls_branches<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>            prev_bev<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>            <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>    bev_embed <span class="token operator">=</span> self<span class="token punctuation">.</span>get_bev_features<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>  <span class="token comment"># Encoder的输出  [B, 20000, 256]</span>    bs <span class="token operator">=</span> mlvl_feats<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>    query_pos<span class="token punctuation">,</span> query <span class="token operator">=</span> torch<span class="token punctuation">.</span>split<span class="token punctuation">(</span>object_query_embed<span class="token punctuation">,</span> self<span class="token punctuation">.</span>embed_dims<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># query_pos [1000, 256]</span>    query_pos <span class="token operator">=</span> query_pos<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>expand<span class="token punctuation">(</span>bs<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>    query <span class="token operator">=</span> query<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>expand<span class="token punctuation">(</span>bs<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>    reference_points <span class="token operator">=</span> self<span class="token punctuation">.</span>reference_points<span class="token punctuation">(</span>query_pos<span class="token punctuation">)</span>    reference_points <span class="token operator">=</span> reference_points<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span>    init_reference_out <span class="token operator">=</span> reference_points  <span class="token comment"># [B, 1000, 2]</span>    query <span class="token operator">=</span> query<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>    query_pos <span class="token operator">=</span> query_pos<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>    bev_embed <span class="token operator">=</span> bev_embed<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>    inter_states<span class="token punctuation">,</span> inter_references <span class="token operator">=</span> self<span class="token punctuation">.</span>decoder<span class="token punctuation">(</span>        query<span class="token operator">=</span>query<span class="token punctuation">,</span>  <span class="token comment"># [1000, B, 256]</span>        key<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>        value<span class="token operator">=</span>bev_embed<span class="token punctuation">,</span>  <span class="token comment"># [20000, B, 256]</span>        query_pos<span class="token operator">=</span>query_pos<span class="token punctuation">,</span>  <span class="token comment"># [1000, B, 256]</span>        reference_points<span class="token operator">=</span>reference_points<span class="token punctuation">,</span>  <span class="token comment"># reference points of offset</span>        reg_branches<span class="token operator">=</span>reg_branches<span class="token punctuation">,</span>        cls_branches<span class="token operator">=</span>cls_branches<span class="token punctuation">,</span>        spatial_shapes<span class="token operator">=</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span>bev_h<span class="token punctuation">,</span> bev_w<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span>query<span class="token punctuation">.</span>device<span class="token punctuation">)</span><span class="token punctuation">,</span>        level_start_index<span class="token operator">=</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span>query<span class="token punctuation">.</span>device<span class="token punctuation">)</span><span class="token punctuation">,</span>        <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>    inter_references_out <span class="token operator">=</span> inter_references    <span class="token keyword">return</span> bev_embed<span class="token punctuation">,</span> inter_states<span class="token punctuation">,</span> init_reference_out<span class="token punctuation">,</span> inter_references_out    <span class="token comment"># [20000, B, 256] [N, 1000, B, 256] [B, 1000, 2]    [N, B, 1000, 2]  这里的N是层数</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>            query<span class="token punctuation">,</span>            <span class="token operator">*</span>args<span class="token punctuation">,</span>            reference_points<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>            reg_branches<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>            key_padding_mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>            <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>    output <span class="token operator">=</span> query  <span class="token comment"># [1000, B, 256]</span>    intermediate <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    intermediate_reference_points <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> lid<span class="token punctuation">,</span> layer <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>layers<span class="token punctuation">)</span><span class="token punctuation">:</span>        reference_points_input <span class="token operator">=</span> reference_points<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span>  <span class="token comment"># BS NUM_QUERY NUM_LEVEL 2</span>        output <span class="token operator">=</span> layer<span class="token punctuation">(</span>            output<span class="token punctuation">,</span>            <span class="token operator">*</span>args<span class="token punctuation">,</span>            reference_points<span class="token operator">=</span>reference_points_input<span class="token punctuation">,</span>            key_padding_mask<span class="token operator">=</span>key_padding_mask<span class="token punctuation">,</span>            <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>        output <span class="token operator">=</span> output<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>  <span class="token comment"># [B, 1000, 256]</span>        <span class="token comment"># 根据预测的偏移量更新参考点的位置，给级联的下一个decoder layer提供参考点位置</span>        <span class="token keyword">if</span> reg_branches <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>            tmp <span class="token operator">=</span> reg_branches<span class="token punctuation">[</span>lid<span class="token punctuation">]</span><span class="token punctuation">(</span>output<span class="token punctuation">)</span>  <span class="token comment"># [B, 1000, 2]</span>            <span class="token keyword">assert</span> reference_points<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token number">2</span>            new_reference_points <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros_like<span class="token punctuation">(</span>reference_points<span class="token punctuation">)</span>            new_reference_points<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">=</span> tmp<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">+</span> inverse_sigmoid<span class="token punctuation">(</span>reference_points<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>            new_reference_points <span class="token operator">=</span> new_reference_points<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span>            reference_points <span class="token operator">=</span> new_reference_points<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># [B, 1000, 2]</span>        output <span class="token operator">=</span> output<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        intermediate<span class="token punctuation">.</span>append<span class="token punctuation">(</span>output<span class="token punctuation">)</span>        intermediate_reference_points<span class="token punctuation">.</span>append<span class="token punctuation">(</span>reference_points<span class="token punctuation">)</span>    <span class="token keyword">return</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span>intermediate<span class="token punctuation">)</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span>intermediate_reference_points<span class="token punctuation">)</span>    <span class="token comment">#               [N, 1000, B, 256]               [N, B, 1000, 2] </span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="推理结果处理"><a href="#推理结果处理" class="headerlink" title="推理结果处理"></a>推理结果处理</h3><p>使用线性层将输出结果处理成points set的形式<br><pre class="line-numbers language-python" data-language="python"><code class="language-python">bev_embed<span class="token punctuation">,</span> hs<span class="token punctuation">,</span> init_reference<span class="token punctuation">,</span> inter_references <span class="token operator">=</span> outputs  <span class="token comment"># decoder的输出</span>hs <span class="token operator">=</span> hs<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>outputs_classes <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>outputs_coords <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>outputs_pts_coords <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token keyword">for</span> lvl <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>hs<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment"># 6</span>    <span class="token keyword">if</span> lvl <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>        reference <span class="token operator">=</span> init_reference    <span class="token keyword">else</span><span class="token punctuation">:</span>        reference <span class="token operator">=</span> inter_references<span class="token punctuation">[</span>lvl <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">]</span>    reference <span class="token operator">=</span> inverse_sigmoid<span class="token punctuation">(</span>reference<span class="token punctuation">)</span>    outputs_class <span class="token operator">=</span> self<span class="token punctuation">.</span>cls_branches<span class="token punctuation">[</span>lvl<span class="token punctuation">]</span><span class="token punctuation">(</span>hs<span class="token punctuation">[</span>lvl<span class="token punctuation">]</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>bs<span class="token punctuation">,</span>self<span class="token punctuation">.</span>num_vec<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_pts_per_vec<span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 线性层</span>    tmp <span class="token operator">=</span> self<span class="token punctuation">.</span>reg_branches<span class="token punctuation">[</span>lvl<span class="token punctuation">]</span><span class="token punctuation">(</span>hs<span class="token punctuation">[</span>lvl<span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># 线性层</span>    <span class="token keyword">assert</span> reference<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token number">2</span>    tmp<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">+=</span> reference<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span>    tmp <span class="token operator">=</span> tmp<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span>    outputs_coord<span class="token punctuation">,</span> outputs_pts_coord <span class="token operator">=</span> self<span class="token punctuation">.</span>transform_box<span class="token punctuation">(</span>tmp<span class="token punctuation">)</span>    outputs_classes<span class="token punctuation">.</span>append<span class="token punctuation">(</span>outputs_class<span class="token punctuation">)</span>    outputs_coords<span class="token punctuation">.</span>append<span class="token punctuation">(</span>outputs_coord<span class="token punctuation">)</span>    outputs_pts_coords<span class="token punctuation">.</span>append<span class="token punctuation">(</span>outputs_pts_coord<span class="token punctuation">)</span>outputs_classes <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span>outputs_classes<span class="token punctuation">)</span>outputs_coords <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span>outputs_coords<span class="token punctuation">)</span>outputs_pts_coords <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span>outputs_pts_coords<span class="token punctuation">)</span>outs <span class="token operator">=</span> <span class="token punctuation">&#123;</span>    <span class="token string">'bev_embed'</span><span class="token punctuation">:</span> bev_embed<span class="token punctuation">,</span>  <span class="token comment"># [20000, B, 256]</span>    <span class="token string">'all_cls_scores'</span><span class="token punctuation">:</span> outputs_classes<span class="token punctuation">,</span>  <span class="token comment"># [N, B, 50, 3]   最多50个instance</span>    <span class="token string">'all_bbox_preds'</span><span class="token punctuation">:</span> outputs_coords<span class="token punctuation">,</span>  <span class="token comment"># [N, B, 50, 4]  bounding box形式</span>    <span class="token string">'all_pts_preds'</span><span class="token punctuation">:</span> outputs_pts_coords<span class="token punctuation">,</span>  <span class="token comment"># [N, B, 50, 20, 2] 所有instance的points set，points set的长度固定</span>    <span class="token string">'enc_cls_scores'</span><span class="token punctuation">:</span> <span class="token boolean">None</span><span class="token punctuation">,</span>    <span class="token string">'enc_bbox_preds'</span><span class="token punctuation">:</span> <span class="token boolean">None</span><span class="token punctuation">,</span>    <span class="token string">'enc_pts_preds'</span><span class="token punctuation">:</span> <span class="token boolean">None</span><span class="token punctuation">&#125;</span><span class="token keyword">return</span> outs<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></p><h3 id="Loss计算"><a href="#Loss计算" class="headerlink" title="Loss计算"></a>Loss计算</h3><p>MapTR的损失函数由分类损失、point2point损失和方向损失三个部分组成</p><script type="math/tex; mode=display">\mathcal{L}=\lambda \mathcal{L}_{cls}+\alpha \mathcal{L}_{p2p}+\beta \mathcal{L}_{dir}</script><p>分类损失使用Focal loss</p><script type="math/tex; mode=display">\mathcal{L}_{cls} = \mathcal{L}_{Focal}(\hat{p}_{\hat{\pi}(i)},c_i)</script><p>point2point损失使用曼哈顿距离</p><script type="math/tex; mode=display">\mathcal{L}_{p2p}=\sum_{i=0}^{N-1}\mathbb{I}_{c_i\ne\varnothing} \sum_{j=0}^{N_v-1}D_{Manhattan}(\hat{v}_{\hat{\pi}(i)},v_{i,\gamma(j)})</script><p>方向损失使用配对的边之间的余弦相似度</p><script type="math/tex; mode=display">\mathcal{L}_{dir}=-\sum_{i=0}^{N-1}\mathbb{I}_{c_i\ne\varnothing}\sum_{j=0}^{N_v-1}cosine\_similarity(\hat{e}_{\hat{\pi}(i),j},e_{i,\hat{\gamma}_i}(j))</script><p>代码中实际由五个部分组成（decoder输出了六层的结果，实际只取了最后一层的loss）</p><div class="table-container"><table><thead><tr><th>loss</th><th>type</th></tr></thead><tbody><tr><td>loss_cls</td><td>Focal loss</td></tr><tr><td>loss_bbox</td><td>L1 loss</td></tr><tr><td>loss_iou</td><td>GIoU loss</td></tr><tr><td>loss_pts</td><td>pts L1 loss</td></tr><tr><td>loss_dir</td><td>pts dir cos loss</td></tr></tbody></table></div><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://arxiv.org/pdf/2208.14437.pdf">MapTR Paper</a><br><a href="https://github.com/hustvl/MapTR">MapTR Code</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Multi-view 3D </tag>
            
            <tag> HD Map </tag>
            
            <tag> ICLR </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>鱼眼相机模型-MEI模型</title>
      <link href="/2022/11/22/principle-mei-mo-xing-yu-yan-xiang-ji-biao-ding/"/>
      <url>/2022/11/22/principle-mei-mo-xing-yu-yan-xiang-ji-biao-ding/</url>
      
        <content type="html"><![CDATA[<p>本文对MEI模型进行鱼眼相机标定的原理进行了介绍<br><span id="more"></span></p><h2 id="MEI模型"><a href="#MEI模型" class="headerlink" title="MEI模型"></a>MEI模型</h2><p>MEI模型实际上是Omnidirectional相机模型+Radtan畸变模型（Radial-tangential）。由于Omnidirectional模型涵盖了Pinhole模型,（Pinhole模型是Omni模型的⼀种特殊情况），MEI模型实际上可以表示鱼眼和⾮鱼眼相机。MEI模型只用棋盘格即可完成标定。</p><p>MEI模型提出Scaramuzza模型在边界处映射不准，映射模型的参数初始值也难以获得。在畸变模型的参数选择上，MEI模型认为过多参数会导致过多的局部最优，从而使得最小化比较困难。因此，MEI模型只建模下图中Fig2所示的畸变<br><img src="/images/MEI_model.png" width="70%" height="70%"/></p><h3 id="Unified-Projection-Model"><a href="#Unified-Projection-Model" class="headerlink" title="Unified Projection Model"></a>Unified Projection Model</h3><p><img src="/images/unified_image_formation.png" width="40%" height="40%"/><br><img src="/images/MEI_process.png"/><br>通过引入${\xi}$，将原点拉到$C_p$，入射光线与Z轴的夹角变小，从而可以映射更大的fov。</p><h3 id="像素重映射"><a href="#像素重映射" class="headerlink" title="像素重映射"></a>像素重映射</h3><p>像素重映射是建立虚拟的针孔相机上的每个像素与鱼眼相机的每个像素的映射关系，即可直接用OpenCV的remap函数双线性插值得到针孔相机的图像。其具体步骤为：</p><ol><li>先确定虚拟针孔相机的内参$(fx, fy, cx, cy)$与相对于鱼眼相机的外参</li><li>对于虚拟相机的每个像素点${p_{pinhole}}$，将像素点反投影到归一化平面，得到三维点${p_{3d} (z=1)}$</li><li>将三维点根据鱼眼相机模型，投影到鱼眼相机上，得到鱼眼相机上的像素点$p_{fisheye}$。</li></ol><p>如此，便建立了针孔相机的像素点与鱼眼相机的像素点的映射关系，根据映射关系取鱼眼相机的像素即可。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.robots.ox.ac.uk/~cmei/articles/single_viewpoint_calib_mei_07.pdf">Single View Point Omnidirectional Camera Calibration from Planar Grids</a></p>]]></content>
      
      
      <categories>
          
          <category> 原理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Calibration </tag>
            
            <tag> Fisheye </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>OpenCV鱼眼相机标定模型</title>
      <link href="/2022/11/10/principle-opencv-yu-yan-biao-ding-mo-xing/"/>
      <url>/2022/11/10/principle-opencv-yu-yan-biao-ding-mo-xing/</url>
      
        <content type="html"><![CDATA[<p>本文对OpenCV鱼眼相机标定模型进行了整理<br><span id="more"></span><br>前情提要：关于相机成像畸变模型，可戳<a href="https://massive11.github.io/2022/09/12/principle-xiang-ji-cheng-xiang-ji-bian-mo-xing/">相机成像畸变模型与鱼眼相机模型</a></p><h2 id="鱼眼相机模型"><a href="#鱼眼相机模型" class="headerlink" title="鱼眼相机模型"></a>鱼眼相机模型</h2><p>$X$是世界坐标系中一点$P$的坐标，$R$是旋转矩阵，$T$是平移向量，点$P$在相机坐标系下的坐标可表示为</p><script type="math/tex; mode=display">X_c=RX+T</script><p>$x，y，z$是$X_c$的三个坐标，可表示为</p><script type="math/tex; mode=display">x=Xc_1</script><script type="math/tex; mode=display">y=Xc_2</script><script type="math/tex; mode=display">z=Xc_3</script><p>P点小孔投影的坐标为$[a;b]$，其中</p><script type="math/tex; mode=display">a = \frac{x}{z}</script><script type="math/tex; mode=display">b = \frac{y}{z}</script><script type="math/tex; mode=display">r^2=a^2+b^2</script><script type="math/tex; mode=display">\theta=atan(r)</script><p>鱼眼畸变模型</p><script type="math/tex; mode=display">\theta_d=\theta(1+k_1\theta^2+k_2\theta^4+k_3\theta^6+k_4\theta^8)</script><p>畸变点的坐标$[x’;y’]$</p><script type="math/tex; mode=display">x'=(\frac{\theta_d}{r})a</script><script type="math/tex; mode=display">y'=(\frac{\theta_d}{r})b</script><p>最终的像素坐标系坐标$[u;v]$</p><script type="math/tex; mode=display">u=f_x(x'+\alpha y')+c_x</script><script type="math/tex; mode=display">v=f_xy'+c_y</script><h2 id="鱼眼图像标定、校正代码"><a href="#鱼眼图像标定、校正代码" class="headerlink" title="鱼眼图像标定、校正代码"></a>鱼眼图像标定、校正代码</h2><p><img src="/images/opencv_distortion.png" width="70%" height="70%"/></p><h2 id="1-cv-fisheye-calibrate"><a href="#1-cv-fisheye-calibrate" class="headerlink" title="1.cv::fisheye::calibrate()"></a>1.cv::fisheye::calibrate()</h2><p>首先使用标定板标定相机。该函数输入角点，返回鱼眼相机内参K、D和外参R、T。<br>其中，K是33的相机内参矩阵，D是畸变系数，R是旋转向量，T是平移向量<br><img src="/images/opencv_calibrate.png"/></p><h2 id="2-cv-getOptimalNewCameraMatrix"><a href="#2-cv-getOptimalNewCameraMatrix" class="headerlink" title="2.cv::getOptimalNewCameraMatrix"></a>2.cv::getOptimalNewCameraMatrix</h2><blockquote><p>Returns the new camera intrinsic matrix based on the free scaling parameter.</p></blockquote><p>根据比例因子返回新的相机内参矩阵<br><img src="/images/opencv_get_new.png"/></p><h2 id="3-cv-fishheye-initUndistortRectifyMap"><a href="#3-cv-fishheye-initUndistortRectifyMap" class="headerlink" title="3.cv::fishheye::initUndistortRectifyMap"></a>3.cv::fishheye::initUndistortRectifyMap</h2><blockquote><p>Computes undistortion and rectification maps for image transform by cv::remap(). If D is empty zero distortion is used, if R or P is empty identity matrixes are used.</p></blockquote><p>该函数计算原始图像和矫正图像之间的转换关系，输入是鱼眼相机内参，输出是两者间的映射关系map1和map2。<br><img src="/images/opencv_init_undis.png"/></p><h2 id="4-cv-remap"><a href="#4-cv-remap" class="headerlink" title="4.cv::remap"></a>4.cv::remap</h2><blockquote><p>The function remap transforms the source image using the specified map:</p><script type="math/tex; mode=display">dst(x,y)=src(map_x(x,y),map_y(x,y))</script><p>where values of pixels with non-integer coordinates are computed using one of available interpolation methods. mapx and mapy can be encoded as separate floating-point maps in map1 and map2 respectively, or interleaved floating-point maps of (x,y) in map1, or fixed-point maps created by using convertMaps. The reason you might want to convert from floating to fixed-point representations of a map is that they can yield much faster (2x) remapping operations. In the converted case, map1 contains pairs (cvFloor(x), cvFloor(y)) and map2 contains indices in a table of interpolation coefficients.</p></blockquote><p>使用map1和map2进行矫正<br><img src="/images/opencv_remap.png"/></p>]]></content>
      
      
      <categories>
          
          <category> 原理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Calibration </tag>
            
            <tag> Fisheye </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MATLAB鱼眼相机标定模型Scaramuzza与相关工具使用</title>
      <link href="/2022/11/08/principle-matlab-yu-yan-xiang-ji-biao-ding-xiang-guan-gong-ju-shi-yong/"/>
      <url>/2022/11/08/principle-matlab-yu-yan-xiang-ji-biao-ding-xiang-guan-gong-ju-shi-yong/</url>
      
        <content type="html"><![CDATA[<p>本文对MATLAB鱼眼相机标定模型-Scaramuzza以及相关工具使用进行了介绍<br><span id="more"></span></p><blockquote><p>The Computer Vision Toolbox contains calibration algorithms for the pinhole camera model and the fisheye camera model. You can use the fisheye model with cameras up to a field of view (FOV) of 195 degrees.</p></blockquote><h2 id="1-Matlab鱼眼相机模型-Scaramuzza"><a href="#1-Matlab鱼眼相机模型-Scaramuzza" class="headerlink" title="1.Matlab鱼眼相机模型-Scaramuzza"></a>1.Matlab鱼眼相机模型-Scaramuzza</h2><p>官方文档：<a href="https://ww2.mathworks.cn/help/vision/ug/fisheye-calibration-basics.html">Mathworks Fisheye Calibration Basics Document</a><br><img src="/images/pinhole_fisheye.png"/></p><center>小孔成像模型与鱼眼相机模型对比</center><h3 id="1-1-世界坐标3D—-gt-—相机坐标3D"><a href="#1-1-世界坐标3D—-gt-—相机坐标3D" class="headerlink" title="1.1 世界坐标3D—&gt;—相机坐标3D"></a>1.1 世界坐标3D—&gt;—相机坐标3D</h3><p>$\begin{pmatrix} X_c\\ Y_c \\ Z_c \end{pmatrix}$为相机坐标系坐标，$R$为外参旋转矩阵，$\begin{pmatrix} X_w\\ Y_w \\ Z_w \end{pmatrix}$为世界坐标系坐标，$T$为外参平移向量</p><script type="math/tex; mode=display">\begin{equation}\begin{pmatrix} X_c\\ Y_c \\ Z_c \end{pmatrix}= R\begin{pmatrix} X_w\\ Y_w \\ Z_w \end{pmatrix}+T \end{equation}</script><h3 id="1-2-相机坐标3D—-gt-—校正图像坐标2D"><a href="#1-2-相机坐标3D—-gt-—校正图像坐标2D" class="headerlink" title="1.2 相机坐标3D—&gt;—校正图像坐标2D"></a>1.2 相机坐标3D—&gt;—校正图像坐标2D</h3><p>鱼眼相机模型的内参包括投影函数的多项式映射系数。对齐系数与传感器对齐以及从传感器平面到相机图像平面中的像素位置的转换有关。$u, v$是矫正后的图像坐标2D点，$a_0,a_2,a_3,a_4$是Scaramuzza模型[1]中描述的多项式系数（$a_1$为0），$\lambda$是scalar factor。$\rho=\sqrt{u^2+v^2}$，是$u, v$相关的函数，仅与像素点与图像中心的距离有关</p><script type="math/tex; mode=display">\begin{equation}\begin{pmatrix} X_c\\ Y_c \\ Z_c \end{pmatrix}=\lambda\begin{pmatrix} u\\ v \\ a_0 + a_2\rho^2+a_3\rho^3+a_4\rho^4 \end{pmatrix}\end{equation}</script><h3 id="1-3-校正图像坐标2D—-gt-—原始图像的像素坐标2D"><a href="#1-3-校正图像坐标2D—-gt-—原始图像的像素坐标2D" class="headerlink" title="1.3 校正图像坐标2D—&gt;—原始图像的像素坐标2D"></a>1.3 校正图像坐标2D—&gt;—原始图像的像素坐标2D</h3><p>内参也包括了拉伸和变形（仿射变换）。 拉伸矩阵补偿传感器到镜头的失准，失真向量调整图像平面的 (0,0) 位置。$\begin{pmatrix} c &amp;d\\ e&amp;1  \end{pmatrix}$为Stretch Matrix，$\begin{pmatrix} u’\\ v’  \end{pmatrix}$为原始图像的像素坐标，$\begin{pmatrix} u\\ v  \end{pmatrix}$是理想的失真坐标，$\begin{pmatrix} c_x\\ c_y  \end{pmatrix}$为Distortion Center</p><script type="math/tex; mode=display">\begin{equation}\begin{pmatrix} u'\\ v'  \end{pmatrix}=\begin{pmatrix} c &d\\ e&1  \end{pmatrix}\begin{pmatrix} u\\ v  \end{pmatrix}+ \begin{pmatrix} c_x\\ c_y  \end{pmatrix}\end{equation}</script><h2 id="2-Matlab鱼眼相机标定参数使用Pipeline"><a href="#2-Matlab鱼眼相机标定参数使用Pipeline" class="headerlink" title="2 Matlab鱼眼相机标定参数使用Pipeline"></a>2 Matlab鱼眼相机标定参数使用Pipeline</h2><p><img src="/images/scaramuzza_pipeline.png" width="70%" height="70%"/></p><h3 id="2-1-Fisheye-Model，根据鱼眼相机内参校正图像（数据预处理）"><a href="#2-1-Fisheye-Model，根据鱼眼相机内参校正图像（数据预处理）" class="headerlink" title="2.1 Fisheye Model，根据鱼眼相机内参校正图像（数据预处理）"></a>2.1 Fisheye Model，根据鱼眼相机内参校正图像（数据预处理）</h3><pre class="line-numbers language-matlab" data-language="matlab"><code class="language-matlab">params <span class="token operator">=</span> <span class="token function">struct</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>params<span class="token punctuation">.</span>pattern_origin_height <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> <span class="token comment">% Pattern is on ground</span><span class="token comment">% 1.加载预先标定好的鱼眼相机内参</span>camera_intrinsics <span class="token operator">=</span> <span class="token punctuation">&#123;</span>    <span class="token string">'/home/kk/map_data/hongqi1_intrinsics/front_center_intrinsics.mat'</span><span class="token punctuation">,</span><span class="token punctuation">...</span><span class="token punctuation">&#125;</span><span class="token punctuation">;</span>intrinsics_mat <span class="token operator">=</span> <span class="token function">load</span><span class="token punctuation">(</span>camera_intrinsics<span class="token punctuation">&#123;</span><span class="token number">1</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>front_intrinsics <span class="token operator">=</span> intrinsics_mat<span class="token punctuation">.</span>front_center_cameraParams<span class="token punctuation">.</span>Intrinsics<span class="token punctuation">;</span><span class="token comment">% 2.得到校正后的虚拟内参以及校正图像</span>img <span class="token operator">=</span> <span class="token function">imread</span><span class="token punctuation">(</span><span class="token string">'/home/kk/Desktop/024010.jpg'</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">[</span>J<span class="token punctuation">,</span> cam_intrinsics<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token function">undistortFisheyeImage</span><span class="token punctuation">(</span>img<span class="token punctuation">,</span> front_intrinsics<span class="token punctuation">,</span> <span class="token string">'Output'</span><span class="token punctuation">,</span> <span class="token string">'full'</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">subplot</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">imshow</span><span class="token punctuation">(</span>img<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">title</span><span class="token punctuation">(</span><span class="token string">'原始图像'</span><span class="token punctuation">)</span><span class="token function">subplot</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">imshow</span><span class="token punctuation">(</span>J<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">title</span><span class="token punctuation">(</span><span class="token string">'校正图像'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/images/scaramuzza_comparison.png"/></p><center>Matlab工具箱 校正效果展示</center><p>校正图像放大后的效果对比<br><img src="/images/scaramuzza_compare_v2.png"/></p><center>Matlab工具箱 校正效果展示（放大版）</center><h3 id="1-2-2-Pinhole-Model，校正图像的坐标转换（模型中）"><a href="#1-2-2-Pinhole-Model，校正图像的坐标转换（模型中）" class="headerlink" title="1.2.2 Pinhole Model，校正图像的坐标转换（模型中）"></a>1.2.2 Pinhole Model，校正图像的坐标转换（模型中）</h3><p>在数据预处理过程中，原始图像已经经过鱼眼模型校正，校正后的图像可以近似为小孔成像模型。因此，可以利用校正图像的虚拟相机的内参去标定外参，由此得到转换过程中需要的全部坐标转换矩阵。<br><pre class="line-numbers language-matlab" data-language="matlab"><code class="language-matlab"><span class="token comment">% 3.加载预先标定的2D-3D匹配点 img_points和world_points</span><span class="token function">load</span><span class="token punctuation">(</span><span class="token string">'./img_points.mat'</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">load</span><span class="token punctuation">(</span><span class="token string">'./world_points.mat'</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment">% 4.求解鱼眼相机外参</span><span class="token punctuation">[</span>params<span class="token punctuation">.</span>front_extri<span class="token punctuation">.</span>pitch<span class="token punctuation">,</span> params<span class="token punctuation">.</span>front_extri<span class="token punctuation">.</span>yaw<span class="token punctuation">,</span> params<span class="token punctuation">.</span>front_extri<span class="token punctuation">.</span>roll<span class="token punctuation">,</span> params<span class="token punctuation">.</span>front_extri<span class="token punctuation">.</span>x<span class="token punctuation">,</span> params<span class="token punctuation">.</span>front_extri<span class="token punctuation">.</span>y<span class="token punctuation">,</span> params<span class="token punctuation">.</span>front_extri<span class="token punctuation">.</span>height<span class="token punctuation">,</span> params<span class="token punctuation">.</span>front_extri<span class="token punctuation">.</span>R<span class="token punctuation">,</span> params<span class="token punctuation">.</span>front_extri<span class="token punctuation">.</span>T<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token function">estimateMonoCameraParametersModify</span><span class="token punctuation">(</span>front_intrinsics<span class="token punctuation">,</span> <span class="token punctuation">...</span>                            img_points<span class="token punctuation">,</span>world_points<span class="token punctuation">,</span> params<span class="token punctuation">.</span>pattern_origin_height<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment">% 5.得到虚拟的小孔成像模型的内参</span>sensor <span class="token operator">=</span> <span class="token function">monoCamera</span><span class="token punctuation">(</span>cam_intrinsics<span class="token punctuation">,</span> params<span class="token punctuation">.</span>front_extri<span class="token punctuation">.</span>height<span class="token punctuation">,</span> <span class="token string">'pitch'</span><span class="token punctuation">,</span> params<span class="token punctuation">.</span>front_extri<span class="token punctuation">.</span>pitch<span class="token punctuation">,</span> <span class="token string">'yaw'</span><span class="token punctuation">,</span> params<span class="token punctuation">.</span>front_extri<span class="token punctuation">.</span>yaw<span class="token punctuation">,</span> <span class="token string">'roll'</span><span class="token punctuation">,</span> params<span class="token punctuation">.</span>front_extri<span class="token punctuation">.</span>roll<span class="token punctuation">)</span><span class="token punctuation">;</span>new_intrin <span class="token operator">=</span> sensor<span class="token punctuation">.</span>Intrinsics<span class="token punctuation">;</span><span class="token comment">% 6.img_points从畸变图像转到校正图像</span><span class="token punctuation">[</span>size_1<span class="token punctuation">,</span>size_2<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token function">size</span><span class="token punctuation">(</span>img_points<span class="token punctuation">)</span><span class="token punctuation">;</span>new_img_points<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">;</span><span class="token keyword">for</span> n <span class="token operator">=</span> <span class="token number">1</span><span class="token operator">:</span>size_1    u_v <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token function">img_points</span><span class="token punctuation">(</span>n<span class="token operator">*</span><span class="token number">2</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">img_points</span><span class="token punctuation">(</span>n<span class="token operator">*</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">;</span>    u_v <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token function">inv</span><span class="token punctuation">(</span>front_intrinsics<span class="token punctuation">.</span>StretchMatrix<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">*</span><span class="token punctuation">(</span>u_v <span class="token operator">-</span> front_intrinsics<span class="token punctuation">.</span>DistortionCenter<span class="token operator">.'</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    new_img_points <span class="token operator">=</span> <span class="token punctuation">[</span>new_img_points<span class="token punctuation">;</span>u_v<span class="token operator">.'</span><span class="token punctuation">]</span><span class="token punctuation">;</span><span class="token keyword">end</span><span class="token comment">% 7.得到虚拟的小孔成像模型的外参</span><span class="token punctuation">[</span>pitch<span class="token punctuation">,</span> yaw<span class="token punctuation">,</span> roll<span class="token punctuation">,</span> x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> height<span class="token punctuation">,</span> R<span class="token punctuation">,</span> T<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token function">estimateMonoCameraParametersModify</span><span class="token punctuation">(</span>new_intrin<span class="token punctuation">,</span> <span class="token punctuation">...</span>                             new_img_points<span class="token punctuation">,</span>world_points<span class="token punctuation">,</span> params<span class="token punctuation">.</span>pattern_origin_height<span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></p><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><p>[1] Scaramuzza, D., A. Martinelli, and R. Siegwart. “A Toolbox for Easy Calibrating Omnidirectional Cameras.” _Proceedings to IEEE International Conference on Intelligent Robots and Systems, (IROS)_. Beijing, China, October 7–15, 2006.</p>]]></content>
      
      
      <categories>
          
          <category> 原理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Calibration </tag>
            
            <tag> Fisheye </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基于BEVDET剖析MMDet3D框架</title>
      <link href="/2022/10/20/code-ji-yu-bevdet-pou-xi-mmdet3d-jia-gou/"/>
      <url>/2022/10/20/code-ji-yu-bevdet-pou-xi-mmdet3d-jia-gou/</url>
      
        <content type="html"><![CDATA[<p>本文基于BEVDET对MMDet3D框架进行了介绍<br><span id="more"></span></p><h1 id="0-MMDetection3D简介"><a href="#0-MMDetection3D简介" class="headerlink" title="0.MMDetection3D简介"></a>0.MMDetection3D简介</h1><p><a href="https://mmdetection3d.readthedocs.io/zh_CN/latest/">MMDet3D 官方文档</a><br><a href="https://github.com/open-mmlab/mmdetection3d">MMDet3D官方仓库</a></p><h2 id="选择MMDet3D的原因"><a href="#选择MMDet3D的原因" class="headerlink" title="选择MMDet3D的原因"></a>选择MMDet3D的原因</h2><ol><li>MMDetection3D 支持<strong>_VoteNet_</strong>,<strong>_ MVXNe__t_</strong>，<strong>_PointPillars_</strong>等多种算法，覆盖了单模态和多模态检测，室内和室外场景SOTA; 还可以直接使用训练MMDetection里面的所有300+模型和40+算法，支持算法的数量和覆盖方向为3D检测代码库之最。</li><li>MMDetection3D 支持<strong>_SUN RGB-D_</strong>, <strong>_ScanNet_</strong>, <strong>_nuScenes_</strong>, <strong>_Lyft_</strong>和<strong>_KITTI_</strong>共5个主流数据集，支持的数据集数量为3D检测代码库之最。</li><li>MMDetection3D 拥有最快的训练速度，支持pip install一键安装，简单易用。</li></ol><h1 id="1-BEVDET网络框架"><a href="#1-BEVDET网络框架" class="headerlink" title="1.BEVDET网络框架"></a>1.BEVDET网络框架</h1><p><img src="/images/bevdet.png"/></p><center>图1 BEVDET网络结构</center><h1 id="2-基于MMDet3D的BEVDET代码框架"><a href="#2-基于MMDet3D的BEVDET代码框架" class="headerlink" title="2.基于MMDet3D的BEVDET代码框架"></a>2.基于MMDet3D的BEVDET代码框架</h1><h2 id="2-1数据预处理"><a href="#2-1数据预处理" class="headerlink" title="2.1数据预处理"></a>2.1数据预处理</h2><h3 id="2-1-1-数据集定义"><a href="#2-1-1-数据集定义" class="headerlink" title="2.1.1 数据集定义"></a>2.1.1 数据集定义</h3><p>MMDet3D支持<strong>_SUN RGB-D_</strong>, <strong>_ScanNet_</strong>, <strong>_nuScenes_</strong>, <strong>_Lyft_</strong>和<strong>_KITTI_</strong>共5个主流数据集。对于上述数据集之外的公开数据集或者自定义数据集，可以通过继承 Custom3DDataset 来实现新的数据集类，并重载相关的方法，如 BEVDETNuScenesDataset数据集所示，该文件位于/mmdet3d/datasets/bevdet_nuscenes_dataset.py。<br>数据集类中主要提供加载标注数据、转换数据格式、验证模型结果、定义数据集处理流程等相关功能。<br><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> mmcv<span class="token keyword">import</span> torch<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">import</span> tempfile<span class="token keyword">from</span> nuscenes<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data_classes <span class="token keyword">import</span> Box <span class="token keyword">as</span> NuScenesBox<span class="token keyword">from</span> os <span class="token keyword">import</span> path <span class="token keyword">as</span> osp<span class="token keyword">from</span> mmdet<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> DATASETS<span class="token keyword">from</span> <span class="token punctuation">.</span><span class="token punctuation">.</span>core <span class="token keyword">import</span> show_result<span class="token keyword">from</span> <span class="token punctuation">.</span><span class="token punctuation">.</span>core<span class="token punctuation">.</span>bbox <span class="token keyword">import</span> Box3DMode<span class="token punctuation">,</span> Coord3DMode<span class="token punctuation">,</span> LiDARInstance3DBoxes<span class="token keyword">from</span> <span class="token punctuation">.</span>custom_3d <span class="token keyword">import</span> Custom3DDataset<span class="token keyword">from</span> <span class="token punctuation">.</span>pipelines <span class="token keyword">import</span> Compose<span class="token decorator annotation punctuation">@DATASETS<span class="token punctuation">.</span>register_module</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">class</span> <span class="token class-name">BEVDETNuScenesDataset</span><span class="token punctuation">(</span>Custom3DDataset<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">pass</span>    <span class="token keyword">def</span> <span class="token function">load_annotations</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> ann_file<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""Load annotations from ann_file."""</span>        <span class="token keyword">pass</span>    <span class="token keyword">def</span> <span class="token function">_format_bbox</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> results<span class="token punctuation">,</span> jsonfile_prefix<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""Convert the results to the standard format."""</span>        <span class="token keyword">pass</span>            <span class="token keyword">def</span> <span class="token function">evaluate</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""Evaluation in BEVDETNuScenesDataset protocol."""</span>        <span class="token keyword">pass</span>    <span class="token keyword">def</span> <span class="token function">format_results</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> results<span class="token punctuation">,</span> jsonfile_prefix<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""Format the results to json"""</span>        <span class="token keyword">pass</span>    <span class="token keyword">def</span> <span class="token function">_build_default_pipeline</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""Build the default pipeline for this dataset."""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></p><h4 id=""><a href="#" class="headerlink" title=" "></a> </h4><h4 id="新增自定义数据集"><a href="#新增自定义数据集" class="headerlink" title="新增自定义数据集"></a>新增自定义数据集</h4><p>在 /mmdet3d/datasets/my_dataset.py 中创建一个新的数据集类来进行数据的加载，如下所示。<br><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">from</span> os <span class="token keyword">import</span> path <span class="token keyword">as</span> osp<span class="token keyword">from</span> mmdet3d<span class="token punctuation">.</span>core <span class="token keyword">import</span> show_result<span class="token keyword">from</span> mmdet3d<span class="token punctuation">.</span>core<span class="token punctuation">.</span>bbox <span class="token keyword">import</span> DepthInstance3DBoxes<span class="token keyword">from</span> mmdet<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> DATASETS<span class="token keyword">from</span> <span class="token punctuation">.</span>custom_3d <span class="token keyword">import</span> Custom3DDataset<span class="token decorator annotation punctuation">@DATASETS<span class="token punctuation">.</span>register_module</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">class</span> <span class="token class-name">MyDataset</span><span class="token punctuation">(</span>Custom3DDataset<span class="token punctuation">)</span><span class="token punctuation">:</span>    CLASSES <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token string">'cabinet'</span><span class="token punctuation">,</span> <span class="token string">'bed'</span><span class="token punctuation">,</span> <span class="token string">'chair'</span><span class="token punctuation">,</span> <span class="token string">'sofa'</span><span class="token punctuation">,</span> <span class="token string">'table'</span><span class="token punctuation">,</span> <span class="token string">'door'</span><span class="token punctuation">,</span> <span class="token string">'window'</span><span class="token punctuation">,</span>               <span class="token string">'bookshelf'</span><span class="token punctuation">,</span> <span class="token string">'picture'</span><span class="token punctuation">,</span> <span class="token string">'counter'</span><span class="token punctuation">,</span> <span class="token string">'desk'</span><span class="token punctuation">,</span> <span class="token string">'curtain'</span><span class="token punctuation">,</span>               <span class="token string">'refrigerator'</span><span class="token punctuation">,</span> <span class="token string">'showercurtrain'</span><span class="token punctuation">,</span> <span class="token string">'toilet'</span><span class="token punctuation">,</span> <span class="token string">'sink'</span><span class="token punctuation">,</span> <span class="token string">'bathtub'</span><span class="token punctuation">,</span>               <span class="token string">'garbagebin'</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>                 data_root<span class="token punctuation">,</span>                 ann_file<span class="token punctuation">,</span>                 pipeline<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>                 classes<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>                 modality<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>                 box_type_3d<span class="token operator">=</span><span class="token string">'Depth'</span><span class="token punctuation">,</span>                 filter_empty_gt<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>                 test_mode<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span>            data_root<span class="token operator">=</span>data_root<span class="token punctuation">,</span>            ann_file<span class="token operator">=</span>ann_file<span class="token punctuation">,</span>            pipeline<span class="token operator">=</span>pipeline<span class="token punctuation">,</span>            classes<span class="token operator">=</span>classes<span class="token punctuation">,</span>            modality<span class="token operator">=</span>modality<span class="token punctuation">,</span>            box_type_3d<span class="token operator">=</span>box_type_3d<span class="token punctuation">,</span>            filter_empty_gt<span class="token operator">=</span>filter_empty_gt<span class="token punctuation">,</span>            test_mode<span class="token operator">=</span>test_mode<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">get_ann_info</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> index<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment"># 通过下标来获取标注信息，evalhook 也能够通过此接口来获取标注信息</span>        info <span class="token operator">=</span> self<span class="token punctuation">.</span>data_infos<span class="token punctuation">[</span>index<span class="token punctuation">]</span>        <span class="token keyword">if</span> info<span class="token punctuation">[</span><span class="token string">'annos'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'gt_num'</span><span class="token punctuation">]</span> <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">:</span>            gt_bboxes_3d <span class="token operator">=</span> info<span class="token punctuation">[</span><span class="token string">'annos'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'gt_boxes_upright_depth'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span>                np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>  <span class="token comment"># k, 6</span>            gt_labels_3d <span class="token operator">=</span> info<span class="token punctuation">[</span><span class="token string">'annos'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'class'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span>np<span class="token punctuation">.</span>int64<span class="token punctuation">)</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            gt_bboxes_3d <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>            gt_labels_3d <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>int64<span class="token punctuation">)</span>        <span class="token comment"># 转换为目标标注框的结构</span>        gt_bboxes_3d <span class="token operator">=</span> DepthInstance3DBoxes<span class="token punctuation">(</span>            gt_bboxes_3d<span class="token punctuation">,</span>            box_dim<span class="token operator">=</span>gt_bboxes_3d<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>            with_yaw<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>            origin<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>convert_to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>box_mode_3d<span class="token punctuation">)</span>        pts_instance_mask_path <span class="token operator">=</span> osp<span class="token punctuation">.</span>join<span class="token punctuation">(</span>self<span class="token punctuation">.</span>data_root<span class="token punctuation">,</span>                                          info<span class="token punctuation">[</span><span class="token string">'pts_instance_mask_path'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        pts_semantic_mask_path <span class="token operator">=</span> osp<span class="token punctuation">.</span>join<span class="token punctuation">(</span>self<span class="token punctuation">.</span>data_root<span class="token punctuation">,</span>                                          info<span class="token punctuation">[</span><span class="token string">'pts_semantic_mask_path'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        anns_results <span class="token operator">=</span> <span class="token builtin">dict</span><span class="token punctuation">(</span>            gt_bboxes_3d<span class="token operator">=</span>gt_bboxes_3d<span class="token punctuation">,</span>            gt_labels_3d<span class="token operator">=</span>gt_labels_3d<span class="token punctuation">,</span>            pts_instance_mask_path<span class="token operator">=</span>pts_instance_mask_path<span class="token punctuation">,</span>            pts_semantic_mask_path<span class="token operator">=</span>pts_semantic_mask_path<span class="token punctuation">)</span>        <span class="token keyword">return</span> anns_results<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></p><p>修改配置文件来调用 MyDataset 数据集类，如下所示。<br><pre class="line-numbers language-python" data-language="python"><code class="language-python">dataset_A_train <span class="token operator">=</span> <span class="token builtin">dict</span><span class="token punctuation">(</span>    <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'MyDataset'</span><span class="token punctuation">,</span>    ann_file <span class="token operator">=</span> <span class="token string">'annotation.pkl'</span><span class="token punctuation">,</span>    pipeline<span class="token operator">=</span>train_pipeline<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre></p><h3 id="2-1-2-数据预处理流程"><a href="#2-1-2-数据预处理流程" class="headerlink" title="2.1.2 数据预处理流程"></a>2.1.2 数据预处理流程</h3><p>数据预处理流程和数据集之间是互相分离的两个部分，通常数据集定义了如何处理标注信息，而数据预处理流程定义了准备数据项字典的所有步骤。数据集预处理流程包含一系列的操作，每个操作将一个字典作为输入，并输出应用于下一个转换的一个新的字典。<br>图2是一个最经典的数据集预处理流程，其中蓝色框表示预处理流程中的各项操作。随着预处理的进行，每一个操作都会添加新的键值（图中标记为绿色）到输出字典中，或者更新当前存在的键值（图中标记为橙色）。<br><img src="/images/data_process_pipeline.png"/></p><center>图2 数据集预处理流程</center><p>预处理流程中的各项操作主要分为数据加载、预处理、格式化、测试时的数据增强。以BEVDET为例，我们对预处理流程中各项操作进行具体的分析。<br><pre class="line-numbers language-python" data-language="python"><code class="language-python">train_pipeline <span class="token operator">=</span> <span class="token punctuation">[</span>    <span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'LoadMultiViewImageFromFiles_BEVDet'</span><span class="token punctuation">,</span> is_train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> data_config<span class="token operator">=</span>data_config<span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'LoadPointsFromFile'</span><span class="token punctuation">,</span>        coord_type<span class="token operator">=</span><span class="token string">'LIDAR'</span><span class="token punctuation">,</span>        load_dim<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span>        use_dim<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span>        file_client_args<span class="token operator">=</span>file_client_args<span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'LoadAnnotations3D'</span><span class="token punctuation">,</span> with_bbox_3d<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> with_label_3d<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'GlobalRotScaleTrans'</span><span class="token punctuation">,</span>        rot_range<span class="token operator">=</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.3925</span><span class="token punctuation">,</span> <span class="token number">0.3925</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        scale_ratio_range<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">0.95</span><span class="token punctuation">,</span> <span class="token number">1.05</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        translation_std<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        update_img2lidar<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'RandomFlip3D'</span><span class="token punctuation">,</span>        sync_2d<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>        flip_ratio_bev_horizontal<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">,</span>        flip_ratio_bev_vertical<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">,</span>        update_img2lidar<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'ObjectRangeFilter'</span><span class="token punctuation">,</span> point_cloud_range<span class="token operator">=</span>point_cloud_range<span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'ObjectNameFilter'</span><span class="token punctuation">,</span> classes<span class="token operator">=</span>class_names<span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'DefaultFormatBundle3D'</span><span class="token punctuation">,</span> class_names<span class="token operator">=</span>class_names<span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'Collect3D'</span><span class="token punctuation">,</span> keys<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'img_inputs'</span><span class="token punctuation">,</span> <span class="token string">'gt_bboxes_3d'</span><span class="token punctuation">,</span> <span class="token string">'gt_labels_3d'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         meta_keys<span class="token operator">=</span><span class="token punctuation">(</span><span class="token string">'filename'</span><span class="token punctuation">,</span> <span class="token string">'ori_shape'</span><span class="token punctuation">,</span> <span class="token string">'img_shape'</span><span class="token punctuation">,</span> <span class="token string">'lidar2img'</span><span class="token punctuation">,</span>                            <span class="token string">'depth2img'</span><span class="token punctuation">,</span> <span class="token string">'cam2img'</span><span class="token punctuation">,</span> <span class="token string">'pad_shape'</span><span class="token punctuation">,</span>                            <span class="token string">'scale_factor'</span><span class="token punctuation">,</span> <span class="token string">'flip'</span><span class="token punctuation">,</span> <span class="token string">'pcd_horizontal_flip'</span><span class="token punctuation">,</span>                            <span class="token string">'pcd_vertical_flip'</span><span class="token punctuation">,</span> <span class="token string">'box_mode_3d'</span><span class="token punctuation">,</span> <span class="token string">'box_type_3d'</span><span class="token punctuation">,</span>                            <span class="token string">'img_norm_cfg'</span><span class="token punctuation">,</span> <span class="token string">'pcd_trans'</span><span class="token punctuation">,</span> <span class="token string">'sample_idx'</span><span class="token punctuation">,</span>                            <span class="token string">'pcd_scale_factor'</span><span class="token punctuation">,</span> <span class="token string">'pcd_rotation'</span><span class="token punctuation">,</span> <span class="token string">'pts_filename'</span><span class="token punctuation">,</span>                            <span class="token string">'transformation_3d_flow'</span><span class="token punctuation">,</span> <span class="token string">'img_info'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></p><p>上述流程中涉及到的各项操作及其在MMDet3D框架下的实际位置如表1所示。Collect3D最后返回的img_meta包含模型输入的全部数据。<br>表1 BEVDET相关数据处理操作</p><div class="table-container"><table><thead><tr><th>操作项</th><th>数据处理操作</th><th>功能</th><th>代码位置</th></tr></thead><tbody><tr><td>数据加载</td><td>LoadMultiViewImageFromFiles_BEVDet</td><td>加载六个相机的单帧图像，进行图像的缩放和裁剪操作等</td><td>/mmdet3d/datasets/pipelines/loading.py</td></tr><tr><td></td><td>LoadPointsFromFile</td><td>加载LiDAR点云（如果不需要点云数据可以不加载）</td><td></td></tr><tr><td></td><td>LoadAnnotations3D</td><td>加载标注数据</td><td></td></tr><tr><td>数据预处理</td><td>GlobalRotScaleTrans</td><td>对于点云数据的旋转、平移、缩放</td><td>/mmdet3d/datasets/pipelines/transforms_3d.py</td></tr><tr><td></td><td>RandomFlip3D</td><td>翻转点云和目标框</td><td></td></tr><tr><td></td><td>ObjectRangeFilter</td><td>根据范围过滤目标框</td><td></td></tr><tr><td></td><td>ObjectNameFilter</td><td>根据类别过滤目标框</td><td></td></tr><tr><td>格式化</td><td>DefaultFormatBundle3D</td><td>格式化真值数据</td><td>/mmdet3d/datasets/pipelines/formating.py</td></tr><tr><td></td><td>Collect3D</td><td>添加img_meta （由 meta_keys 指定的键值构成的 img_meta），移除所有除 keys 指定的键值以外的其他键值</td></tr></tbody></table></div><h4 id="新增自定义数据处理方法"><a href="#新增自定义数据处理方法" class="headerlink" title="新增自定义数据处理方法"></a>新增自定义数据处理方法</h4><p>在 /mmdet3d/datasets/pipelines/my_pipeline.py中写入新的数据集预处理方法，该预处理方法的输入和输出均为字典<br><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> mmdet<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> PIPELINES<span class="token decorator annotation punctuation">@PIPELINES<span class="token punctuation">.</span>register_module</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">class</span> <span class="token class-name">MyTransform</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__call__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> results<span class="token punctuation">)</span><span class="token punctuation">:</span>        results<span class="token punctuation">[</span><span class="token string">'dummy'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">True</span>        <span class="token keyword">return</span> results<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></p><p>在/mmdet3d/datasets/pipelines/<strong>init</strong>.py 中导入新的数据处理方法<br><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> <span class="token punctuation">.</span>my_pipeline <span class="token keyword">import</span> MyTransform<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></p><p>在配置文件中使用该数据预处理方法<br><pre class="line-numbers language-python" data-language="python"><code class="language-python">train_pipeline <span class="token operator">=</span> <span class="token punctuation">[</span>    <span class="token builtin">dict</span><span class="token punctuation">(</span>        <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'LoadPointsFromFile'</span><span class="token punctuation">,</span>        load_dim<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span>        use_dim<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span>        file_client_args<span class="token operator">=</span>file_client_args<span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token triple-quoted-string string">"""..."""</span>    <span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'MyTransform'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token triple-quoted-string string">"""..."""</span>    <span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'Collect3D'</span><span class="token punctuation">,</span> keys<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'points'</span><span class="token punctuation">,</span> <span class="token string">'gt_bboxes_3d'</span><span class="token punctuation">,</span> <span class="token string">'gt_labels_3d'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></p><h2 id="2-2模型"><a href="#2-2模型" class="headerlink" title="2.2模型"></a>2.2模型</h2><h3 id="2-2-1-配置模型结构"><a href="#2-2-1-配置模型结构" class="headerlink" title="2.2.1 配置模型结构"></a>2.2.1 配置模型结构</h3><p>MMDet3D使用config文件配置模型结构，BEVDET-sttiny版本的模型配置部分如下</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">model <span class="token operator">=</span> <span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'BEVDet'</span><span class="token punctuation">,</span>    img_backbone<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'SwinTransformer'</span><span class="token punctuation">,</span>        pretrained<span class="token operator">=</span><span class="token string">'https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_tiny_patch4_window7_224.pth'</span><span class="token punctuation">,</span>        pretrain_img_size<span class="token operator">=</span><span class="token number">224</span><span class="token punctuation">,</span>        embed_dims<span class="token operator">=</span><span class="token number">96</span><span class="token punctuation">,</span>        patch_size<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span>        window_size<span class="token operator">=</span><span class="token number">7</span><span class="token punctuation">,</span>        mlp_ratio<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span>        depths<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        num_heads<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">12</span><span class="token punctuation">,</span> <span class="token number">24</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        strides<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        out_indices<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        qkv_bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>        qk_scale<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>        patch_norm<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>        drop_rate<span class="token operator">=</span><span class="token number">0.</span><span class="token punctuation">,</span>        attn_drop_rate<span class="token operator">=</span><span class="token number">0.</span><span class="token punctuation">,</span>        drop_path_rate<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span>        use_abs_pos_embed<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>        act_cfg<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'GELU'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        norm_cfg<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'LN'</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        pretrain_style<span class="token operator">=</span><span class="token string">'official'</span><span class="token punctuation">,</span>        output_missing_index_as_none<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    img_neck<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'FPN_LSS'</span><span class="token punctuation">,</span>        in_channels<span class="token operator">=</span><span class="token number">384</span><span class="token operator">+</span><span class="token number">768</span><span class="token punctuation">,</span>        out_channels<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">,</span>        extra_upsample<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>        input_feature_index<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        scale_factor<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    img_view_transformer<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'ViewTransformerLiftSplatShoot'</span><span class="token punctuation">,</span> grid_config<span class="token operator">=</span>grid_config<span class="token punctuation">,</span> data_config<span class="token operator">=</span>data_config<span class="token punctuation">,</span> numC_Trans<span class="token operator">=</span>numC_Trans<span class="token punctuation">)</span><span class="token punctuation">,</span>    img_bev_encoder_backbone <span class="token operator">=</span> <span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'ResNetForBEVDet'</span><span class="token punctuation">,</span> numC_input<span class="token operator">=</span>numC_Trans<span class="token punctuation">)</span><span class="token punctuation">,</span>    img_bev_encoder_neck <span class="token operator">=</span> <span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'FPN_LSS'</span><span class="token punctuation">,</span> in_channels<span class="token operator">=</span>numC_Trans<span class="token operator">*</span><span class="token number">8</span><span class="token operator">+</span>numC_Trans<span class="token operator">*</span><span class="token number">2</span><span class="token punctuation">,</span> out_channels<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    pts_bbox_head<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'CenterHeadBEVDet'</span><span class="token punctuation">,</span>        in_channels<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span>        tasks<span class="token operator">=</span><span class="token punctuation">[</span><span class="token builtin">dict</span><span class="token punctuation">(</span>num_class<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> class_names<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'car'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            <span class="token builtin">dict</span><span class="token punctuation">(</span>num_class<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> class_names<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'truck'</span><span class="token punctuation">,</span> <span class="token string">'construction_vehicle'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            <span class="token builtin">dict</span><span class="token punctuation">(</span>num_class<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> class_names<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'bus'</span><span class="token punctuation">,</span> <span class="token string">'trailer'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            <span class="token builtin">dict</span><span class="token punctuation">(</span>num_class<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> class_names<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'barrier'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            <span class="token builtin">dict</span><span class="token punctuation">(</span>num_class<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> class_names<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'motorcycle'</span><span class="token punctuation">,</span> <span class="token string">'bicycle'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            <span class="token builtin">dict</span><span class="token punctuation">(</span>num_class<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> class_names<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'pedestrian'</span><span class="token punctuation">,</span> <span class="token string">'traffic_cone'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        <span class="token punctuation">]</span><span class="token punctuation">,</span>        common_heads<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>reg<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> height<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> rot<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> vel<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        share_conv_channel<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span>        bbox_coder<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'CenterPointBBoxCoder'</span><span class="token punctuation">,</span>            pc_range<span class="token operator">=</span>point_cloud_range<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span>            post_center_range<span class="token operator">=</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">61.2</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">61.2</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">10.0</span><span class="token punctuation">,</span> <span class="token number">61.2</span><span class="token punctuation">,</span> <span class="token number">61.2</span><span class="token punctuation">,</span> <span class="token number">10.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>            max_num<span class="token operator">=</span><span class="token number">500</span><span class="token punctuation">,</span>            score_threshold<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span>            out_size_factor<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span>            voxel_size<span class="token operator">=</span>voxel_size<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span>            code_size<span class="token operator">=</span><span class="token number">9</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        separate_head<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'SeparateHead'</span><span class="token punctuation">,</span> init_bias<span class="token operator">=</span><span class="token operator">-</span><span class="token number">2.19</span><span class="token punctuation">,</span> final_kernel<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        loss_cls<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'GaussianFocalLoss'</span><span class="token punctuation">,</span> reduction<span class="token operator">=</span><span class="token string">'mean'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        loss_bbox<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'L1Loss'</span><span class="token punctuation">,</span> reduction<span class="token operator">=</span><span class="token string">'mean'</span><span class="token punctuation">,</span> loss_weight<span class="token operator">=</span><span class="token number">0.25</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        norm_bbox<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token punctuation">)</span>  <span class="token comment"># 省略了模型的训练和配置信息</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>上述模块划分与论文中的模块划分基本一致，其对应关系以及在MMDet3D框架下的实际位置如表2所示<br>表2  BEVDET的模型配置</p><div class="table-container"><table><thead><tr><th>论文模块</th><th>代码模块</th><th>Tensor Size</th><th>类型</th><th>代码位置</th><th>原论文</th></tr></thead><tbody><tr><td>Image-view Encoder</td><td>img_backbone</td><td>[B, N, 3, 256, 704] $\downarrow$ [[B, N, 384, 16, 44], [B, N, 768, 8, 22]]</td><td>SwinTransformer</td><td>mmdet3d/models/backbones/swin.py</td><td>Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</td></tr><tr><td></td><td>img_neck</td><td>[[B, N, 384, 16, 44], [B, N, 768, 8, 22]]$\downarrow$[B, N, 512, 16, 44]</td><td>FPN_LSS</td><td>mmdet3d/models/necks/lss_fpn.py</td><td>Feature Pyramid Networks for Object Detection</td></tr><tr><td>View Transformer</td><td>img_view_transformer</td><td>[B, N, 512, 16, 44]$\downarrow$[B, 64, 128, 128]</td><td>ViewTransformerLSS</td><td>mmdet3d/models/necks/view_transformer_bevdet_bevdepth.py</td><td>Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D</td></tr><tr><td>BEVEncoder</td><td>img_bev_encoder_backbone</td><td>[B, 64, 128, 128]$\downarrow$[[B, 128, 64, 64], [B, 256, 32, 32], [B, 512, 16, 16]]</td><td>ResNetForBevDet</td><td>mmdet3d/models/backbones/resnet.py</td><td>Deep Residual Learning for Image Recognition</td></tr><tr><td></td><td>img_bev_encoder_neck</td><td>[[B, 128, 64, 64], [B, 256, 32, 32], [B, 512, 16, 16]]$\downarrow$[B, 256, 128, 128]</td><td>FPN_LSS</td><td>mmdet3d/models/necks/lss_fpn.py</td><td>Feature Pyramid Networks for Object Detection</td></tr><tr><td>Head</td><td>pts_bbox_head</td><td>[B, 256, 128, 128]$\downarrow$[B, …, 128, 128]</td><td>CenterHeadBEVDet</td><td>mmdet3d/models/dense_heads/centerpoint_head_bevdet.py</td><td>Center-based 3D Object Detection and Tracking</td></tr></tbody></table></div><h3 id="2-2-2-模型的各个组件"><a href="#2-2-2-模型的各个组件" class="headerlink" title="2.2.2 模型的各个组件"></a>2.2.2 模型的各个组件</h3><p>MMDet3D通常把模型的各个组成成分分成6种类型：</p><ul><li>骨干网络（backbone）：通常采用 FCN 网络来提取特征图，如 <strong>_ResNet _</strong>和 <strong>_SECOND_</strong>。</li><li>颈部网络（neck）：位于 backbones 和 heads 之间的组成模块，如<strong>_ FPN_</strong> 和 <strong>_SECONDFPN_</strong>。</li><li>RoI 提取器（RoI extractor）：用于从特征图中提取 RoI 特征的组成模块，如<strong>_ H3DRoIHead_</strong> 和 <strong>_PartAggregationROIHead_</strong>。</li><li>编码器（encoder）：包括 voxel layer、voxel encoder 和 middle encoder 等进入 backbone 前所使用的基于 voxel 的方法，如 <strong>_HardVFE_</strong> 和<strong>_ PointPillarsScatter_</strong>。</li><li>检测头（head）：用于特定任务的组成模块，如检测框的预测和掩码的预测。</li><li>损失函数（loss）：heads 中用于计算损失函数的组成模块，如<strong>_ FocalLoss_</strong>、<strong>_L1Loss_</strong> 和<strong>_ GHMLoss_</strong>。</li></ul><h4 id="Detector"><a href="#Detector" class="headerlink" title="Detector"></a>Detector</h4><p>对于3D检测模型，模型的总体框架由Dectectors定义，也即BEVDET-sttiny的配置文件中，model的type。在Detector中定义从输入的六张图像到模型输出结果的模型训练、测试的整体流程，包括对模型各子组件的调用。对于BEVDET-sttiny，其model的type为“BEVDet”，具体定义如下<br><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">from</span> mmcv<span class="token punctuation">.</span>runner <span class="token keyword">import</span> force_fp32<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F<span class="token keyword">from</span> mmdet<span class="token punctuation">.</span>models <span class="token keyword">import</span> DETECTORS<span class="token keyword">from</span> <span class="token punctuation">.</span>centerpoint <span class="token keyword">import</span> CenterPoint<span class="token keyword">from</span> <span class="token punctuation">.</span><span class="token punctuation">.</span> <span class="token keyword">import</span> builder<span class="token decorator annotation punctuation">@DETECTORS<span class="token punctuation">.</span>register_module</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">class</span> <span class="token class-name">BEVDet</span><span class="token punctuation">(</span>CenterPoint<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> img_view_transformer<span class="token punctuation">,</span> img_bev_encoder_backbone<span class="token punctuation">,</span> img_bev_encoder_neck<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>BEVDet<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>img_view_transformer <span class="token operator">=</span> builder<span class="token punctuation">.</span>build_neck<span class="token punctuation">(</span>img_view_transformer<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>img_bev_encoder_backbone <span class="token operator">=</span> builder<span class="token punctuation">.</span>build_backbone<span class="token punctuation">(</span>img_bev_encoder_backbone<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>img_bev_encoder_neck <span class="token operator">=</span> builder<span class="token punctuation">.</span>build_neck<span class="token punctuation">(</span>img_bev_encoder_neck<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">image_encoder</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> img<span class="token punctuation">)</span><span class="token punctuation">:</span>        imgs <span class="token operator">=</span> img        B<span class="token punctuation">,</span> N<span class="token punctuation">,</span> C<span class="token punctuation">,</span> imH<span class="token punctuation">,</span> imW <span class="token operator">=</span> imgs<span class="token punctuation">.</span>shape        imgs <span class="token operator">=</span> imgs<span class="token punctuation">.</span>view<span class="token punctuation">(</span>B <span class="token operator">*</span> N<span class="token punctuation">,</span> C<span class="token punctuation">,</span> imH<span class="token punctuation">,</span> imW<span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>img_backbone<span class="token punctuation">(</span>imgs<span class="token punctuation">)</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>with_img_neck<span class="token punctuation">:</span>            x <span class="token operator">=</span> self<span class="token punctuation">.</span>img_neck<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        _<span class="token punctuation">,</span> output_dim<span class="token punctuation">,</span> ouput_H<span class="token punctuation">,</span> output_W <span class="token operator">=</span> x<span class="token punctuation">.</span>shape        x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span>B<span class="token punctuation">,</span> N<span class="token punctuation">,</span> output_dim<span class="token punctuation">,</span> ouput_H<span class="token punctuation">,</span> output_W<span class="token punctuation">)</span>        <span class="token keyword">return</span> x    <span class="token keyword">def</span> <span class="token function">bev_encoder</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>img_bev_encoder_backbone<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>img_bev_encoder_neck<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token keyword">return</span> x    <span class="token keyword">def</span> <span class="token function">extract_img_feat</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> img<span class="token punctuation">,</span> img_metas<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""Extract features of images."""</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>image_encoder<span class="token punctuation">(</span>img<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>img_view_transformer<span class="token punctuation">(</span><span class="token punctuation">[</span>x<span class="token punctuation">]</span> <span class="token operator">+</span> img<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>bev_encoder<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token keyword">return</span> <span class="token punctuation">[</span>x<span class="token punctuation">]</span>    <span class="token keyword">def</span> <span class="token function">extract_feat</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> points<span class="token punctuation">,</span> img<span class="token punctuation">,</span> img_metas<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""Extract features from images and points."""</span>        img_feats <span class="token operator">=</span> self<span class="token punctuation">.</span>extract_img_feat<span class="token punctuation">(</span>img<span class="token punctuation">,</span> img_metas<span class="token punctuation">)</span>        pts_feats <span class="token operator">=</span> <span class="token boolean">None</span>        <span class="token keyword">return</span> <span class="token punctuation">(</span>img_feats<span class="token punctuation">,</span> pts_feats<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward_train</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>                      points<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>                      img_metas<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>                      gt_bboxes_3d<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>                      gt_labels_3d<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>                      gt_labels<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>                      gt_bboxes<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>                      img_inputs<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>                      proposals<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>                      gt_bboxes_ignore<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""Forward training function."""</span>        img_feats<span class="token punctuation">,</span> pts_feats <span class="token operator">=</span> self<span class="token punctuation">.</span>extract_feat<span class="token punctuation">(</span>            points<span class="token punctuation">,</span> img<span class="token operator">=</span>img_inputs<span class="token punctuation">,</span> img_metas<span class="token operator">=</span>img_metas<span class="token punctuation">)</span>        <span class="token keyword">assert</span> self<span class="token punctuation">.</span>with_pts_bbox        losses <span class="token operator">=</span> <span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token punctuation">)</span>        losses_pts <span class="token operator">=</span> self<span class="token punctuation">.</span>forward_pts_train<span class="token punctuation">(</span>img_feats<span class="token punctuation">,</span> gt_bboxes_3d<span class="token punctuation">,</span>                                            gt_labels_3d<span class="token punctuation">,</span> img_metas<span class="token punctuation">,</span>                                            gt_bboxes_ignore<span class="token punctuation">)</span>        losses<span class="token punctuation">.</span>update<span class="token punctuation">(</span>losses_pts<span class="token punctuation">)</span>        <span class="token keyword">return</span> losses    <span class="token keyword">def</span> <span class="token function">forward_test</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> points<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> img_metas<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> img_inputs<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">for</span> var<span class="token punctuation">,</span> name <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token punctuation">(</span>img_inputs<span class="token punctuation">,</span> <span class="token string">'img_inputs'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>img_metas<span class="token punctuation">,</span> <span class="token string">'img_metas'</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">:</span>            <span class="token keyword">if</span> <span class="token keyword">not</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>var<span class="token punctuation">,</span> <span class="token builtin">list</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                <span class="token keyword">raise</span> TypeError<span class="token punctuation">(</span><span class="token string">'&#123;&#125; must be a list, but got &#123;&#125;'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>                    name<span class="token punctuation">,</span> <span class="token builtin">type</span><span class="token punctuation">(</span>var<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        num_augs <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>img_inputs<span class="token punctuation">)</span>        <span class="token keyword">if</span> num_augs <span class="token operator">!=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>img_metas<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span>                <span class="token string">'num of augmentations (&#123;&#125;) != num of image meta (&#123;&#125;)'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>                    <span class="token builtin">len</span><span class="token punctuation">(</span>img_inputs<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>img_metas<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> <span class="token keyword">not</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>img_inputs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token builtin">list</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            img_inputs <span class="token operator">=</span> <span class="token punctuation">[</span>img_inputs<span class="token punctuation">]</span> <span class="token keyword">if</span> img_inputs <span class="token keyword">is</span> <span class="token boolean">None</span> <span class="token keyword">else</span> img_inputs            points <span class="token operator">=</span> <span class="token punctuation">[</span>points<span class="token punctuation">]</span> <span class="token keyword">if</span> points <span class="token keyword">is</span> <span class="token boolean">None</span> <span class="token keyword">else</span> points            <span class="token keyword">return</span> self<span class="token punctuation">.</span>simple_test<span class="token punctuation">(</span>points<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> img_metas<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> img_inputs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            <span class="token keyword">return</span> self<span class="token punctuation">.</span>aug_test<span class="token punctuation">(</span><span class="token boolean">None</span><span class="token punctuation">,</span> img_metas<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> img_inputs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">simple_test</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> points<span class="token punctuation">,</span> img_metas<span class="token punctuation">,</span> img<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> rescale<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""Test function without augmentaiton."""</span>        img_feats<span class="token punctuation">,</span> _ <span class="token operator">=</span> self<span class="token punctuation">.</span>extract_feat<span class="token punctuation">(</span>points<span class="token punctuation">,</span> img<span class="token operator">=</span>img<span class="token punctuation">,</span> img_metas<span class="token operator">=</span>img_metas<span class="token punctuation">)</span>        bbox_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>img_metas<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span>        bbox_pts <span class="token operator">=</span> self<span class="token punctuation">.</span>simple_test_pts<span class="token punctuation">(</span>img_feats<span class="token punctuation">,</span> img_metas<span class="token punctuation">,</span> rescale<span class="token operator">=</span>rescale<span class="token punctuation">)</span>        <span class="token keyword">for</span> result_dict<span class="token punctuation">,</span> pts_bbox <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>bbox_list<span class="token punctuation">,</span> bbox_pts<span class="token punctuation">)</span><span class="token punctuation">:</span>            result_dict<span class="token punctuation">[</span><span class="token string">'pts_bbox'</span><span class="token punctuation">]</span> <span class="token operator">=</span> pts_bbox        <span class="token keyword">return</span> bbox_list<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></p><h4 id="BackBone"><a href="#BackBone" class="headerlink" title="BackBone"></a>BackBone</h4><p>BackBone通常是用于提取图像特征的骨干网络，一般是指定已内置的网络直接调用，包括_<strong>ResNET</strong>_、<strong>_SECOND_</strong>、<strong>_DLANet_</strong>等。对于BEVDET-sttiny，其img_backbone的type为“SwinTransformer”、img_bev_encoder_backbone的type为“ResNetForBEVDet”，后者的具体定义如下<br><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> torch <span class="token keyword">import</span> nn<span class="token keyword">from</span> mmdet<span class="token punctuation">.</span>models<span class="token punctuation">.</span>backbones<span class="token punctuation">.</span>resnet <span class="token keyword">import</span> Bottleneck<span class="token punctuation">,</span> BasicBlock<span class="token keyword">import</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>checkpoint <span class="token keyword">as</span> checkpoint<span class="token keyword">from</span> mmdet<span class="token punctuation">.</span>models <span class="token keyword">import</span> BACKBONES<span class="token decorator annotation punctuation">@BACKBONES<span class="token punctuation">.</span>register_module</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">class</span> <span class="token class-name">ResNetForBEVDet</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> numC_input<span class="token punctuation">,</span> num_layer<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> num_channels<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                 backbone_output_ids<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> norm_cfg<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'BN'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                 with_cp<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> block_type<span class="token operator">=</span><span class="token string">'Basic'</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>ResNetForBEVDet<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">assert</span> <span class="token builtin">len</span><span class="token punctuation">(</span>num_layer<span class="token punctuation">)</span><span class="token operator">==</span><span class="token builtin">len</span><span class="token punctuation">(</span>stride<span class="token punctuation">)</span>        num_channels <span class="token operator">=</span> <span class="token punctuation">[</span>numC_input<span class="token operator">*</span><span class="token number">2</span><span class="token operator">**</span><span class="token punctuation">(</span>i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>num_layer<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span> \            <span class="token keyword">if</span> num_channels <span class="token keyword">is</span> <span class="token boolean">None</span> <span class="token keyword">else</span> num_channels        self<span class="token punctuation">.</span>backbone_output_ids <span class="token operator">=</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>num_layer<span class="token punctuation">)</span><span class="token punctuation">)</span> \            <span class="token keyword">if</span> backbone_output_ids <span class="token keyword">is</span> <span class="token boolean">None</span> <span class="token keyword">else</span> backbone_output_ids        layers <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        <span class="token keyword">if</span> block_type <span class="token operator">==</span> <span class="token string">'BottleNeck'</span><span class="token punctuation">:</span>            curr_numC <span class="token operator">=</span> numC_input            <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>num_layer<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                layer<span class="token operator">=</span><span class="token punctuation">[</span>Bottleneck<span class="token punctuation">(</span>curr_numC<span class="token punctuation">,</span> num_channels<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">//</span><span class="token number">4</span><span class="token punctuation">,</span> stride<span class="token operator">=</span>stride<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span>                                  downsample<span class="token operator">=</span>nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>curr_numC<span class="token punctuation">,</span>num_channels<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span>stride<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                                  norm_cfg<span class="token operator">=</span>norm_cfg<span class="token punctuation">)</span><span class="token punctuation">]</span>                curr_numC<span class="token operator">=</span> num_channels<span class="token punctuation">[</span>i<span class="token punctuation">]</span>                layer<span class="token punctuation">.</span>extend<span class="token punctuation">(</span><span class="token punctuation">[</span>Bottleneck<span class="token punctuation">(</span>curr_numC<span class="token punctuation">,</span> curr_numC<span class="token operator">//</span><span class="token number">4</span><span class="token punctuation">,</span>                                         norm_cfg<span class="token operator">=</span>norm_cfg<span class="token punctuation">)</span> <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_layer<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>                layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token operator">*</span>layer<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">elif</span> block_type <span class="token operator">==</span> <span class="token string">'Basic'</span><span class="token punctuation">:</span>            curr_numC <span class="token operator">=</span> numC_input            <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>num_layer<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                layer<span class="token operator">=</span><span class="token punctuation">[</span>BasicBlock<span class="token punctuation">(</span>curr_numC<span class="token punctuation">,</span> num_channels<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> stride<span class="token operator">=</span>stride<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span>                                  downsample<span class="token operator">=</span>nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>curr_numC<span class="token punctuation">,</span>num_channels<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span>stride<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                                  norm_cfg<span class="token operator">=</span>norm_cfg<span class="token punctuation">)</span><span class="token punctuation">]</span>                curr_numC<span class="token operator">=</span> num_channels<span class="token punctuation">[</span>i<span class="token punctuation">]</span>                layer<span class="token punctuation">.</span>extend<span class="token punctuation">(</span><span class="token punctuation">[</span>BasicBlock<span class="token punctuation">(</span>curr_numC<span class="token punctuation">,</span> curr_numC<span class="token punctuation">,</span> norm_cfg<span class="token operator">=</span>norm_cfg<span class="token punctuation">)</span> <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_layer<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>                layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token operator">*</span>layer<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            <span class="token keyword">assert</span> <span class="token boolean">False</span>        self<span class="token punctuation">.</span>layers <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token operator">*</span>layers<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>with_cp <span class="token operator">=</span> with_cp    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        feats <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        x_tmp <span class="token operator">=</span> x        <span class="token keyword">for</span> lid<span class="token punctuation">,</span> layer <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>layers<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">if</span> self<span class="token punctuation">.</span>with_cp<span class="token punctuation">:</span>                x_tmp <span class="token operator">=</span> checkpoint<span class="token punctuation">.</span>checkpoint<span class="token punctuation">(</span>layer<span class="token punctuation">,</span> x_tmp<span class="token punctuation">)</span>            <span class="token keyword">else</span><span class="token punctuation">:</span>                x_tmp <span class="token operator">=</span> layer<span class="token punctuation">(</span>x_tmp<span class="token punctuation">)</span>            <span class="token keyword">if</span> lid <span class="token keyword">in</span> self<span class="token punctuation">.</span>backbone_output_ids<span class="token punctuation">:</span>                feats<span class="token punctuation">.</span>append<span class="token punctuation">(</span>x_tmp<span class="token punctuation">)</span>        <span class="token keyword">return</span> feats<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></p><h4 id="Neck"><a href="#Neck" class="headerlink" title="Neck"></a>Neck</h4><p>Neck一般是FPN，用于增强模型对不同scale的目标的处理能力，一般是指定已内置的网络直接调用，包括<strong>_ FPN_</strong> 、<strong>_SECONDFPN_</strong>等。对于BEVDET-sttiny，其img_neck和img_bev_encoder_neck的type均为“FPN_LSS”，具体定义如下<br><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">from</span> mmcv<span class="token punctuation">.</span>cnn <span class="token keyword">import</span> build_norm_layer<span class="token keyword">from</span> mmdet<span class="token punctuation">.</span>models <span class="token keyword">import</span> NECKS<span class="token decorator annotation punctuation">@NECKS<span class="token punctuation">.</span>register_module</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">class</span> <span class="token class-name">FPN_LSS</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> scale_factor<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span>                 input_feature_index<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                 norm_cfg<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'BN'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                 extra_upsample<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>                 lateral<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>input_feature_index <span class="token operator">=</span> input_feature_index        self<span class="token punctuation">.</span>extra_upsample <span class="token operator">=</span> extra_upsample <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span>        self<span class="token punctuation">.</span>up <span class="token operator">=</span> nn<span class="token punctuation">.</span>Upsample<span class="token punctuation">(</span>scale_factor<span class="token operator">=</span>scale_factor<span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'bilinear'</span><span class="token punctuation">,</span> align_corners<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>        channels_factor <span class="token operator">=</span> <span class="token number">2</span> <span class="token keyword">if</span> self<span class="token punctuation">.</span>extra_upsample <span class="token keyword">else</span> <span class="token number">1</span>        self<span class="token punctuation">.</span>conv <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> out_channels <span class="token operator">*</span> channels_factor<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            build_norm_layer<span class="token punctuation">(</span>norm_cfg<span class="token punctuation">,</span> out_channels <span class="token operator">*</span> channels_factor<span class="token punctuation">,</span> postfix<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>out_channels <span class="token operator">*</span> channels_factor<span class="token punctuation">,</span> out_channels <span class="token operator">*</span> channels_factor<span class="token punctuation">,</span>                      kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            build_norm_layer<span class="token punctuation">(</span>norm_cfg<span class="token punctuation">,</span> out_channels <span class="token operator">*</span> channels_factor<span class="token punctuation">,</span> postfix<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        <span class="token punctuation">)</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>extra_upsample<span class="token punctuation">:</span>            self<span class="token punctuation">.</span>up2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>                nn<span class="token punctuation">.</span>Upsample<span class="token punctuation">(</span>scale_factor<span class="token operator">=</span>extra_upsample <span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'bilinear'</span><span class="token punctuation">,</span> align_corners<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>out_channels <span class="token operator">*</span> channels_factor<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                build_norm_layer<span class="token punctuation">(</span>norm_cfg<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> postfix<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>out_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            <span class="token punctuation">)</span>        self<span class="token punctuation">.</span>lateral<span class="token operator">=</span>  lateral <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>lateral<span class="token punctuation">:</span>            self<span class="token punctuation">.</span>lateral_conv <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>                nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>lateral<span class="token punctuation">,</span> lateral<span class="token punctuation">,</span>                          kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                build_norm_layer<span class="token punctuation">(</span>norm_cfg<span class="token punctuation">,</span> lateral<span class="token punctuation">,</span> postfix<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        <span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> feats<span class="token punctuation">)</span><span class="token punctuation">:</span>        x2<span class="token punctuation">,</span> x1 <span class="token operator">=</span> feats<span class="token punctuation">[</span>self<span class="token punctuation">.</span>input_feature_index<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> feats<span class="token punctuation">[</span>self<span class="token punctuation">.</span>input_feature_index<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>lateral<span class="token punctuation">:</span>            x2 <span class="token operator">=</span> self<span class="token punctuation">.</span>lateral_conv<span class="token punctuation">(</span>x2<span class="token punctuation">)</span>        x1 <span class="token operator">=</span> self<span class="token punctuation">.</span>up<span class="token punctuation">(</span>x1<span class="token punctuation">)</span>        x1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>x2<span class="token punctuation">,</span> x1<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>conv<span class="token punctuation">(</span>x1<span class="token punctuation">)</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>extra_upsample<span class="token punctuation">:</span>            x <span class="token operator">=</span> self<span class="token punctuation">.</span>up2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token keyword">return</span> x<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></p><h4 id="Head"><a href="#Head" class="headerlink" title="Head"></a>Head</h4><p>Head通常用于完成特定任务，loss函数和真值数据的处理也通常在这里完成，是稍微复杂一些的模块。MMDet3D已经提供的模块包括<strong>_FCOSMono3DHead_</strong> 、<strong>_CenterHead_</strong>等，对于任务相近的模型，可以选择直接继承这些任务增加一些自定义的函数，也可以直接继承Head的基类实现。对于BEVDET-sttiny，其pts_bbox_head的type为“CenterHeadBEVDet”，具体定义如下<br><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> copy<span class="token keyword">import</span> torch<span class="token keyword">from</span> mmcv<span class="token punctuation">.</span>cnn <span class="token keyword">import</span> ConvModule<span class="token punctuation">,</span> build_conv_layer<span class="token keyword">from</span> mmcv<span class="token punctuation">.</span>runner <span class="token keyword">import</span> BaseModule<span class="token punctuation">,</span> force_fp32<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn<span class="token keyword">from</span> mmdet3d<span class="token punctuation">.</span>core <span class="token keyword">import</span> <span class="token punctuation">(</span>circle_nms<span class="token punctuation">,</span> draw_heatmap_gaussian<span class="token punctuation">,</span> gaussian_radius<span class="token punctuation">,</span> xywhr2xyxyr<span class="token punctuation">)</span><span class="token keyword">from</span> mmdet3d<span class="token punctuation">.</span>core<span class="token punctuation">.</span>post_processing <span class="token keyword">import</span> nms_bev<span class="token keyword">from</span> mmdet3d<span class="token punctuation">.</span>models <span class="token keyword">import</span> builder<span class="token keyword">from</span> mmdet3d<span class="token punctuation">.</span>models<span class="token punctuation">.</span>utils <span class="token keyword">import</span> clip_sigmoid<span class="token keyword">from</span> mmdet<span class="token punctuation">.</span>core <span class="token keyword">import</span> build_bbox_coder<span class="token punctuation">,</span> multi_apply<span class="token keyword">from</span> <span class="token punctuation">.</span><span class="token punctuation">.</span>builder <span class="token keyword">import</span> HEADS<span class="token punctuation">,</span> build_loss<span class="token decorator annotation punctuation">@HEADS<span class="token punctuation">.</span>register_module</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">class</span> <span class="token class-name">CenterHeadBEVDet</span><span class="token punctuation">(</span>BaseModule<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>                 in_channels<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">128</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                 tasks<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>                 train_cfg<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>                 test_cfg<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>                 bbox_coder<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>                 loss_cls<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'GaussianFocalLoss'</span><span class="token punctuation">,</span> reduction<span class="token operator">=</span><span class="token string">'mean'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                 loss_bbox<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>                     <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'L1Loss'</span><span class="token punctuation">,</span> reduction<span class="token operator">=</span><span class="token string">'none'</span><span class="token punctuation">,</span> loss_weight<span class="token operator">=</span><span class="token number">0.25</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                 separate_head<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>                     <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'SeparateHead'</span><span class="token punctuation">,</span> init_bias<span class="token operator">=</span><span class="token operator">-</span><span class="token number">2.19</span><span class="token punctuation">,</span> final_kernel<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                 <span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">assert</span> init_cfg <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token string">'To prevent abnormal initialization '</span> \            <span class="token string">'behavior, init_cfg is not allowed to be set'</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>CenterHeadBEVDet<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span>init_cfg<span class="token operator">=</span>init_cfg<span class="token punctuation">)</span>        <span class="token keyword">pass</span>    <span class="token keyword">def</span> <span class="token function">forward_single</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""Forward function for CenterPoint."""</span>        <span class="token keyword">pass</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> feats<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""Forward pass."""</span>        <span class="token keyword">return</span> multi_apply<span class="token punctuation">(</span>self<span class="token punctuation">.</span>forward_single<span class="token punctuation">,</span> feats<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">_gather_feat</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> feat<span class="token punctuation">,</span> ind<span class="token punctuation">,</span> mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""Gather feature map"""</span>        <span class="token keyword">pass</span>    <span class="token keyword">def</span> <span class="token function">get_targets</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> gt_bboxes_3d<span class="token punctuation">,</span> gt_labels_3d<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""Generate targets"""</span>        heatmaps<span class="token punctuation">,</span> anno_boxes<span class="token punctuation">,</span> inds<span class="token punctuation">,</span> masks <span class="token operator">=</span> multi_apply<span class="token punctuation">(</span>            self<span class="token punctuation">.</span>get_targets_single<span class="token punctuation">,</span> gt_bboxes_3d<span class="token punctuation">,</span> gt_labels_3d<span class="token punctuation">)</span>        <span class="token keyword">pass</span>    <span class="token keyword">def</span> <span class="token function">get_targets_single</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> gt_bboxes_3d<span class="token punctuation">,</span> gt_labels_3d<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""Generate training targets for a single sample"""</span>        <span class="token keyword">pass</span>    <span class="token decorator annotation punctuation">@force_fp32</span><span class="token punctuation">(</span>apply_to<span class="token operator">=</span><span class="token punctuation">(</span><span class="token string">'preds_dicts'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">loss</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> gt_bboxes_3d<span class="token punctuation">,</span> gt_labels_3d<span class="token punctuation">,</span> preds_dicts<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""Loss function for CenterHead"""</span>        heatmaps<span class="token punctuation">,</span> anno_boxes<span class="token punctuation">,</span> inds<span class="token punctuation">,</span> masks <span class="token operator">=</span> self<span class="token punctuation">.</span>get_targets<span class="token punctuation">(</span>            gt_bboxes_3d<span class="token punctuation">,</span> gt_labels_3d<span class="token punctuation">)</span>        loss_dict <span class="token operator">=</span> <span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">for</span> task_id<span class="token punctuation">,</span> preds_dict <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>preds_dicts<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token comment"># heatmap focal loss</span>            preds_dict<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'heatmap'</span><span class="token punctuation">]</span> <span class="token operator">=</span> clip_sigmoid<span class="token punctuation">(</span>preds_dict<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'heatmap'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>            num_pos <span class="token operator">=</span> heatmaps<span class="token punctuation">[</span>task_id<span class="token punctuation">]</span><span class="token punctuation">.</span>eq<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>            loss_heatmap <span class="token operator">=</span> self<span class="token punctuation">.</span>loss_cls<span class="token punctuation">(</span>                preds_dict<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'heatmap'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                heatmaps<span class="token punctuation">[</span>task_id<span class="token punctuation">]</span><span class="token punctuation">,</span>                avg_factor<span class="token operator">=</span><span class="token builtin">max</span><span class="token punctuation">(</span>num_pos<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>            target_box <span class="token operator">=</span> anno_boxes<span class="token punctuation">[</span>task_id<span class="token punctuation">]</span>            preds_dict<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'anno_box'</span><span class="token punctuation">]</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>                <span class="token punctuation">(</span>preds_dict<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'reg'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> preds_dict<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'height'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                 preds_dict<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'dim'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> preds_dict<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'rot'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                 preds_dict<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'vel'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>            ind <span class="token operator">=</span> inds<span class="token punctuation">[</span>task_id<span class="token punctuation">]</span>            num <span class="token operator">=</span> masks<span class="token punctuation">[</span>task_id<span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span>            pred <span class="token operator">=</span> preds_dict<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'anno_box'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span>            pred <span class="token operator">=</span> pred<span class="token punctuation">.</span>view<span class="token punctuation">(</span>pred<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> pred<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>            pred <span class="token operator">=</span> self<span class="token punctuation">.</span>_gather_feat<span class="token punctuation">(</span>pred<span class="token punctuation">,</span> ind<span class="token punctuation">)</span>            mask <span class="token operator">=</span> masks<span class="token punctuation">[</span>task_id<span class="token punctuation">]</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>expand_as<span class="token punctuation">(</span>target_box<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span>            isnotnan <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token operator">~</span>torch<span class="token punctuation">.</span>isnan<span class="token punctuation">(</span>target_box<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span>            mask <span class="token operator">*=</span> isnotnan            code_weights <span class="token operator">=</span> self<span class="token punctuation">.</span>train_cfg<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">'code_weights'</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span>            bbox_weights <span class="token operator">=</span> mask <span class="token operator">*</span> mask<span class="token punctuation">.</span>new_tensor<span class="token punctuation">(</span>code_weights<span class="token punctuation">)</span>            <span class="token keyword">if</span> self<span class="token punctuation">.</span>task_specific<span class="token punctuation">:</span>                name_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'xy'</span><span class="token punctuation">,</span> <span class="token string">'z'</span><span class="token punctuation">,</span> <span class="token string">'whl'</span><span class="token punctuation">,</span> <span class="token string">'yaw'</span><span class="token punctuation">,</span> <span class="token string">'vel'</span><span class="token punctuation">]</span>                clip_index <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">]</span>                <span class="token keyword">for</span> reg_task_id <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>name_list<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                    pred_tmp <span class="token operator">=</span> pred<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> clip_index<span class="token punctuation">[</span>reg_task_id<span class="token punctuation">]</span><span class="token punctuation">:</span>clip_index<span class="token punctuation">[</span>reg_task_id <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span>                    target_box_tmp <span class="token operator">=</span> target_box<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> clip_index<span class="token punctuation">[</span>reg_task_id<span class="token punctuation">]</span><span class="token punctuation">:</span>clip_index<span class="token punctuation">[</span>reg_task_id <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span>                    bbox_weights_tmp <span class="token operator">=</span> bbox_weights<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> clip_index<span class="token punctuation">[</span>reg_task_id<span class="token punctuation">]</span><span class="token punctuation">:</span>clip_index<span class="token punctuation">[</span>reg_task_id <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span>                    loss_bbox_tmp <span class="token operator">=</span> self<span class="token punctuation">.</span>loss_bbox<span class="token punctuation">(</span>                        pred_tmp<span class="token punctuation">,</span> target_box_tmp<span class="token punctuation">,</span> bbox_weights_tmp<span class="token punctuation">,</span> avg_factor<span class="token operator">=</span><span class="token punctuation">(</span>num <span class="token operator">+</span> <span class="token number">1e-4</span><span class="token punctuation">)</span><span class="token punctuation">)</span>                    loss_dict<span class="token punctuation">[</span><span class="token string-interpolation"><span class="token string">f'%stask</span><span class="token interpolation"><span class="token punctuation">&#123;</span>task_id<span class="token punctuation">&#125;</span></span><span class="token string">.loss_%s'</span></span> <span class="token operator">%</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>loss_prefix<span class="token punctuation">,</span> name_list<span class="token punctuation">[</span>reg_task_id<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> loss_bbox_tmp            <span class="token keyword">else</span><span class="token punctuation">:</span>                loss_bbox <span class="token operator">=</span> self<span class="token punctuation">.</span>loss_bbox<span class="token punctuation">(</span>                    pred<span class="token punctuation">,</span> target_box<span class="token punctuation">,</span> bbox_weights<span class="token punctuation">,</span> avg_factor<span class="token operator">=</span><span class="token punctuation">(</span>num <span class="token operator">+</span> <span class="token number">1e-4</span><span class="token punctuation">)</span><span class="token punctuation">)</span>                loss_dict<span class="token punctuation">[</span><span class="token string-interpolation"><span class="token string">f'task</span><span class="token interpolation"><span class="token punctuation">&#123;</span>task_id<span class="token punctuation">&#125;</span></span><span class="token string">.loss_bbox'</span></span><span class="token punctuation">]</span> <span class="token operator">=</span> loss_bbox            loss_dict<span class="token punctuation">[</span><span class="token string-interpolation"><span class="token string">f'%stask</span><span class="token interpolation"><span class="token punctuation">&#123;</span>task_id<span class="token punctuation">&#125;</span></span><span class="token string">.loss_heatmap'</span></span> <span class="token operator">%</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>loss_prefix<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> loss_heatmap        <span class="token keyword">return</span> loss_dict    <span class="token keyword">def</span> <span class="token function">get_bboxes</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> preds_dicts<span class="token punctuation">,</span> img_metas<span class="token punctuation">,</span> img<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> rescale<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""Generate bboxes from bbox head predictions"""</span>        <span class="token keyword">pass</span>    <span class="token keyword">def</span> <span class="token function">get_task_detections</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> num_class_with_bg<span class="token punctuation">,</span> batch_cls_preds<span class="token punctuation">,</span>                            batch_reg_preds<span class="token punctuation">,</span> batch_cls_labels<span class="token punctuation">,</span> img_metas<span class="token punctuation">,</span> task_id<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""Rotate nms for each task"""</span>        <span class="token keyword">pass</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></p><h4 id="新增自定义模型组件"><a href="#新增自定义模型组件" class="headerlink" title="新增自定义模型组件"></a>新增自定义模型组件</h4><p>此处以新增BackBone为例介绍如何新增自定义模型组件，其他组件的方法类似。<br>创建一个新文件mmdet3d/models/backbones/second.py<br><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">from</span> <span class="token punctuation">.</span><span class="token punctuation">.</span>builder <span class="token keyword">import</span> BACKBONES<span class="token decorator annotation punctuation">@BACKBONES<span class="token punctuation">.</span>register_module</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">class</span> <span class="token class-name">SECOND</span><span class="token punctuation">(</span>BaseModule<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> arg1<span class="token punctuation">,</span> arg2<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">pass</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment"># should return a tuple</span>        <span class="token keyword">pass</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></p><p>在/mmdet3d/models/backbones/<strong>init</strong>.py中导入该新增模块<br><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> <span class="token punctuation">.</span>second <span class="token keyword">import</span> SECOND<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></p><p>在配置文件中使用新增的BackBone<br><pre class="line-numbers language-python" data-language="python"><code class="language-python">model <span class="token operator">=</span> <span class="token builtin">dict</span><span class="token punctuation">(</span>    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>    backbone<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>        <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'SECOND'</span><span class="token punctuation">,</span>        arg1<span class="token operator">=</span>xxx<span class="token punctuation">,</span>        arg2<span class="token operator">=</span>xxx<span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></p>]]></content>
      
      
      <categories>
          
          <category> 实战 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Multi-view 3D </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>相机成像畸变模型与鱼眼相机模型</title>
      <link href="/2022/09/12/principle-xiang-ji-cheng-xiang-ji-bian-mo-xing/"/>
      <url>/2022/09/12/principle-xiang-ji-cheng-xiang-ji-bian-mo-xing/</url>
      
        <content type="html"><![CDATA[<p>相机成像畸变模型与鱼眼相机模型介绍<br><span id="more"></span></p><h2 id="相机成像畸变"><a href="#相机成像畸变" class="headerlink" title="相机成像畸变"></a>相机成像畸变</h2><h3 id="畸变的产生和公式化描述"><a href="#畸变的产生和公式化描述" class="headerlink" title="畸变的产生和公式化描述"></a>畸变的产生和公式化描述</h3><p>为了好的成像效果，通常会在相机的前方增加透镜。透镜的加入会对成像过程中光线的传播产生新的影响：一是透镜自身的形状对光线传播的影响；二是在机械组装过程中，透镜不可能和成像平面完全平行，会导致光线穿过透镜投影到成像面时的位置发生变化。</p><h4 id="径向畸变"><a href="#径向畸变" class="headerlink" title="径向畸变"></a>径向畸变</h4><p>由透镜形状引起的畸变称为径向畸变。在针孔模型中，一条直线投影到像素平面上还是一条直线。实际环境中，相机的透镜往往使得一条直线在图片中变成了曲线。越靠近图像的边缘效果越明显。</p><p>径向畸变主要分为桶形畸变和枕形畸变，如图所示。桶形畸变图像放大率随着与光轴之间的距离增加而减小，枕形畸变则相反。这两种畸变中，穿过图像中心和光轴有交点的直线还能保持形状不变。</p><p><img src="/images/distortion.jpeg" width="70%" height="70%"/></p><p>使用数学公式来描述径向畸变。考虑归一化平面上的任意一点$p$，其坐标为$[x,y]^T$，也可写成极坐标的形式$[r, \theta]^T$，其中$r$表示点$p$与坐标系原点之间的距离，$p$表示与水平轴的夹角。径向畸变可以看成坐标点沿着长度方向发生了变化，也即其距离远点的长度发生了变化。通常假设畸变呈多项式关系，假设$[x_{distorted}, y_{distorted}]^T$是畸变后点的归一化坐标，即</p><script type="math/tex; mode=display">x_{distorted} = x(1 + k_1 r^2 + k_2 r^4 + k_3 r^6)</script><script type="math/tex; mode=display">y_{distorted} = y(1 + k_1 r^2 + k_2 r^4 + k_3 r^6)</script><h4 id="切向畸变"><a href="#切向畸变" class="headerlink" title="切向畸变"></a>切向畸变</h4><p>透镜和成像平面不能完全平行引起的畸变称为切向畸变。<br>参考上述的公式描述，切向畸变可以看成坐标点沿着切线方向发生了变化，也即水平夹角发生了变化。使用参数$p_1, p_2$进行纠正，畸变描述如下</p><script type="math/tex; mode=display">x_{distorted} = x + 2p_1 xy + p_2 (r^2 + 2x^2)</script><script type="math/tex; mode=display">y_{distorted} = y + p_1 (r^2 + 2y^2) + 2p_2xy</script><h4 id="联合畸变模型"><a href="#联合畸变模型" class="headerlink" title="联合畸变模型"></a>联合畸变模型</h4><p>联合上述两种畸变，对于相机坐标系中的一点$P$，可以通过5个畸变系数找到这个点在像素平面上的正确位置：</p><ol><li>将三维空间点投影到归一化图像平面，使其归一化坐标为$[x,y]^T$</li><li><p>对归一化平面上的点计算径向畸变和切向畸变</p><script type="math/tex; mode=display">x_{distorted} = x(1 + k_1 r^2 + k_2 r^4 + k_3 r^6) + 2p_1 xy + p_2 (r^2 + 2x^2)</script><script type="math/tex; mode=display">y_{distorted} = y(1 + k_1 r^2 + k_2 r^4 + k_3 r^6) + p_1 (r^2 + 2y^2) + 2p_2xy</script></li><li><p>将畸变后的点通过内参数矩阵投影到像素平面，得到该点在图像上的正确位置</p><script type="math/tex; mode=display">u = f_x x_{distorted} + c_x</script><script type="math/tex; mode=display">v = f_y y_{distorted} + c_y</script></li></ol><h3 id="去畸变处理"><a href="#去畸变处理" class="headerlink" title="去畸变处理"></a>去畸变处理</h3><p>去畸变处理有两种做法。可以选择先对整张图像进行去畸变，得到去畸变后的图像，然后讨论此图像上的点的空间位置。也可以从畸变图像上的某个点出发，按照畸变方程，讨论其畸变前的空间位置。去畸变之后，就可以直接用针孔模型建立投影关系。（通常前者更为常用）</p><h2 id="鱼眼相机"><a href="#鱼眼相机" class="headerlink" title="鱼眼相机"></a>鱼眼相机</h2><p>鱼眼镜头设计的目的是要拍摄大的视野，其视场角可达到180-270度。为了将尽可能大的场景投影到有限的图像平面内，鱼眼相机将相机畸变列为其成像特征用以在有限的图像平面内表达本应落在无穷远处的空间点投影。鱼眼镜头采用非相似成像，在成像过程中引入畸变，通过对直径空间的压缩，突破成像视角的局限，从而达到广角成像。因此经入射光线经过镜头之后射向成像面，其出射角一定是要小于入射角的。鱼眼镜头是由十几个不同的透镜组合而成的，在成像过程中，入射光线经过不同程度的折射，投影到尺寸有限的成像平面上，使得鱼眼镜头与普通镜头相比拥有更大的视野。</p><p><img src="/images/fisheye_model.png" width="70%" height="70%"/></p><h3 id="鱼眼相机成像模型"><a href="#鱼眼相机成像模型" class="headerlink" title="鱼眼相机成像模型"></a>鱼眼相机成像模型</h3><p>鱼眼相机成像模型近似为单位球面投影模型。一般将鱼眼相机成像过程分解成两步：先将三维空间点线性的投影到虚拟单位球面上；随后将单位球面上的点投影到图像平面上，这个过程是非线性的。由于鱼眼相机所成影像存在畸变，其中径向畸变非常严重，因此其畸变模型主要考虑径向畸变。</p><p><img src="/images/fisheye_camera_model.png" width="40%" height="40%"/></p><p>鱼眼相机的投影函数是为了尽可能将庞大的场景投影到有限的图像平面所设计的。根据投影函数的不同，鱼眼相机的设计模型大致分为等距投影模型、等立体角投影模型、正交投影模型和体视投影模型四种。相机的成像模型实际上表征的是成像的像高与入射角之间的映射关系。</p><p>以针孔模型为例，其理想成像公式为</p><script type="math/tex; mode=display">r_d = f tan \theta</script><p>f为鱼眼相机的焦距，即成像平面的半径，θ是入射光线与鱼眼相机光轴的夹角，即入射角。$r_d$为鱼眼图像的点到畸变中心的距离，即成像高度。$r_d$随着设计模型的变化而发生变化，四种模型投射距离的大小关系如下（反映空间中的一点P投影到球面再到图像平面的成像过程）</p><h4 id="等距投影"><a href="#等距投影" class="headerlink" title="等距投影"></a>等距投影</h4><p>等距投影模型是使用最广泛的投影模型，其投影关系在入射光线之间的角度相同时，保持其对应各投影点之间的间距相同。其投影模型为：</p><script type="math/tex; mode=display">r_d = f \theta</script><h4 id="正交投影"><a href="#正交投影" class="headerlink" title="正交投影"></a>正交投影</h4><p>正交投影模型的畸变很大，近180度处的图像信息几乎全部丢失，且180度之外的场景区域将无法进行描述。采用这种模型的镜头拍出的图片即使在视角较小的区域也会比其他模型镜头的畸变更明显。因此，针对艺术摄影类的广角镜头，该模型的校正效果更好。其模型如下：</p><script type="math/tex; mode=display">r_d = f sin\theta</script><h4 id="等立体角投影（等积投影）"><a href="#等立体角投影（等积投影）" class="headerlink" title="等立体角投影（等积投影）"></a>等立体角投影（等积投影）</h4><p>等立体角的特点是相等立体角的入射面会产生相等面积的像，其畸变程度介于等距模型与正交模型之间。这种投影方式的特征在于，能保持变换前后，物体所占的立体角大小不变。或者说，在半球形空间中，半球面上两个「面积」相同的图案，成像后，在成像平面上的两个图案的面积仍然相同（虽然两者形状不一定相似）。其模型如下：</p><script type="math/tex; mode=display">r_d = 2f sin \frac{\theta}{2}</script><h4 id="体视投影"><a href="#体视投影" class="headerlink" title="体视投影"></a>体视投影</h4><p>体视模型相比之下畸变最小，球形物面上的微小面元经过体视投影后，其像仍然是一个小圆。所以体视投影对微小物体成像有相似性。但正是这种成像的相似性，使得该投影方式不能提供足够的桶形畸变，影响成像视场。</p><script type="math/tex; mode=display">r_d = 2f tan \frac{\theta}{2}</script><p>以上四种畸变模型畸变量从大到小依次为：正交投影、等立体角投影、等距投影、体视投影。</p><p>一些常用的鱼眼相机模型，可以参考<a href="https://massive11.github.io/2022/11/08/principle-matlab-yu-yan-xiang-ji-biao-ding-xiang-guan-gong-ju-shi-yong/">Scaramuzza畸变模型</a>、<a href="https://massive11.github.io/2022/11/10/principle-opencv-yu-yan-biao-ding-mo-xing/">OpenCV畸变模型</a>和<a href="https://massive11.github.io/2022/11/22/principle-mei-mo-xing-yu-yan-xiang-ji-biao-ding/">MEI模型</a></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>《视觉SLAM十四讲》—高翔<br><a href="https://zhuanlan.zhihu.com/p/511284263">https://zhuanlan.zhihu.com/p/511284263</a><br><a href="https://zhuanlan.zhihu.com/p/340751380">https://zhuanlan.zhihu.com/p/340751380</a><br><a href="https://zhuanlan.zhihu.com/p/29273352">https://zhuanlan.zhihu.com/p/29273352</a><br><a href="https://www.researchgate.net/publication/347632237_Universal_Semantic_Segmentation_for_Fisheye_Urban_Driving_Images">https://www.researchgate.net/publication/347632237_Universal_Semantic_Segmentation_for_Fisheye_Urban_Driving_Images</a></p>]]></content>
      
      
      <categories>
          
          <category> 原理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Coordinate system </tag>
            
            <tag> Camera imaging </tag>
            
            <tag> Distortion </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PGD-Probabilistic and Geometric Depth_ Detecting Objects in Perspective</title>
      <link href="/2022/08/22/paper-pgd-probabilistic-and-geometric-depth-detecting-objects-in-perspective/"/>
      <url>/2022/08/22/paper-pgd-probabilistic-and-geometric-depth-detecting-objects-in-perspective/</url>
      
        <content type="html"><![CDATA[<p>本文整理了Probabilistic and Geometric Depth_ Detecting Objects in Perspective论文主要内容<br><span id="more"></span></p><p><strong>本文核心思想</strong>：利用目标之间的相对位置关系构造深度传播图以获取更准确的深度估计值。</p><h1 id="0-Overview"><a href="#0-Overview" class="headerlink" title="0.Overview"></a>0.Overview</h1><ul><li>作者提出的主要观点<ul><li>深度估计误差是限制单目3D检测性能的关键因素。通过目标间的相对位置关系，可以进一步优化深度估计结果以提高3D检测性能。</li></ul></li><li>本文具体实现<ul><li>本文主要是在FCOS3D的基础上替换了深度估计模块，该模块的设计方面主要有以下两点考量<ul><li><ol><li>作者认为准确地估计物体深度其实是一件非常难的事情，因此把原本的确定性深度转化为概率性深度，有点类似于hard label → soft label</li></ol></li><li><ol><li>作者认为仅依靠单一物体进行深度估计是不准确的，因此引入了Graph-Based Depth Propagation，利用多个物体间的关系获得更准确的深度估计结果。</li></ol></li></ul></li></ul></li></ul><h1 id="1-Method"><a href="#1-Method" class="headerlink" title="1.Method"></a>1.Method</h1><h2 id="1-1-Uncertainty-Modeling-with-Probabilistic-Representation"><a href="#1-1-Uncertainty-Modeling-with-Probabilistic-Representation" class="headerlink" title="1.1 Uncertainty Modeling with Probabilistic Representation"></a>1.1 Uncertainty Modeling with Probabilistic Representation</h2><blockquote><p>To establish an effective depth propagation mechanism, modeling the uncertainty of depth estimation for each instance is an important preliminary, which can provide useful guidance for weighing the propagation among instances.</p></blockquote><p>为了建立有效的深度传播机制，对每个实例的深度估计不确定度进行建模是一个重要的前期工作，可以为实例间的传播赋权提供有益的指导。</p><p>是_probabilistic depth_，</p><p>是每个孤立实例的局部深度估计，它与得到的深度评分一起作为构造深度传播图的基础。</p><h2 id="1-2-Depth-Propagation-from-Perspective-Geometry"><a href="#1-2-Depth-Propagation-from-Perspective-Geometry" class="headerlink" title="1.2 Depth Propagation from Perspective Geometry"></a>1.2 Depth Propagation from Perspective Geometry</h2><p>针对深度估计问题，我们提出了一种几何深度传播机制，考虑了实例之间的相互依赖性。我们将首先推导两个实例之间的透视关系，然后给出带有边缘修剪和门控的基于图的深度传播方案的细节。</p><h3 id="Perspective-Relationship"><a href="#Perspective-Relationship" class="headerlink" title="Perspective Relationship"></a>Perspective Relationship</h3><p>注：本文的方法也是基于平面假设的</p><h3 id="Graph-Based-Depth-Propagation"><a href="#Graph-Based-Depth-Propagation" class="headerlink" title="Graph-Based Depth Propagation"></a>Graph-Based Depth Propagation</h3><blockquote><p>From our observation, the same category of nearby objects can well satisfy the “same ground” condition, so we select the following 3 most important factors to decide which edges are influential and reliable, including the depth confidence sd j , 2D distance score s2D ij , and classification similarity scls ij .</p></blockquote><p>此处针对地面假设的部分做了补充。</p><h2 id="1-3-Probabilistic-and-Geometric-Depth-Estimation"><a href="#1-3-Probabilistic-and-Geometric-Depth-Estimation" class="headerlink" title="1.3 Probabilistic and Geometric Depth Estimation"></a>1.3 Probabilistic and Geometric Depth Estimation</h2><h1 id="2-Coding"><a href="#2-Coding" class="headerlink" title="2.Coding"></a>2.Coding</h1><p>PGD在代码实现上，实质上就是FCOS3D的基础上扩充了深度估计模块。模型的Backbone为DCN v2，Neck为FPN，Head部分的结构图如图1<br><img src="https://cdn.nlark.com/yuque/0/2022/png/478741/1666493133769-8a4cac8a-61b0-4bcb-a64a-73e5092b5ad7.png#clientId=ub5dee652-7dc8-4&amp;errorMessage=unknown%20error&amp;from=drop&amp;id=udd02e737&amp;originHeight=1300&amp;originWidth=2464&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;size=195957&amp;status=error&amp;style=none&amp;taskId=ue3b64591-a44c-43c8-b989-133cdbdfadb&amp;title=" alt="截屏2022-10-23 10.45.30.png"></p><h1 id="3-Conclusion"><a href="#3-Conclusion" class="headerlink" title="3.Conclusion"></a>3.Conclusion</h1>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Multi-view 3D </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Mono3D Detection 整理</title>
      <link href="/2022/08/12/paper-mono3d-paper-lu-xian-shu-li/"/>
      <url>/2022/08/12/paper-mono3d-paper-lu-xian-shu-li/</url>
      
        <content type="html"><![CDATA[<p>本文整理了单目3D目标检测的相关工作<br><span id="more"></span></p><h1 id="0-Introduction-of-3D-Object-Detection"><a href="#0-Introduction-of-3D-Object-Detection" class="headerlink" title="0.Introduction of 3D Object Detection"></a>0.Introduction of 3D Object Detection</h1><p>在缺乏深度信息的情况下，根据RGB图像恢复3D结构是一个不适定问题。但是在提供先验信息的情况下，这个任务也能取得不错的效果。<br>在具体的方案实现上，总体可以分为四条路线：</p><ol><li>Proposal-based的方案，先生成3D proposal ，再根据设定的方案对proposal进行排序，从而得到最后的结果</li><li>RGB-only的方案，将3D框的各组成项解耦，直接从RGB中回归各组成项再组合成3D框</li><li>Perspective transformation的方案，将原始的输入图像换一种表达形式（此处不包含借助depth进行3D结构恢复的方案）</li><li>Pseudo-LiDAR的方案 ，本质上都是在RGB图像上进行深度估计，恢复3D结构，然后在视觉点云上进行3D检测</li></ol><p>以下将一些代表性的论文粗略分为上述四个类别，按照时间线进行简单的回顾。</p><h1 id="1-Proposal-based"><a href="#1-Proposal-based" class="headerlink" title="1.Proposal-based"></a>1.Proposal-based</h1><h2 id="2016—CVPR—Mono3D"><a href="#2016—CVPR—Mono3D" class="headerlink" title="2016—CVPR—Mono3D"></a>2016—CVPR—Mono3D</h2><p>[<a href="https://www.cs.toronto.edu/~urtasun/publications/chen_etal_cvpr16.pdf">Paper</a>]     [Code]</p><blockquote><p>Mono3D first samples candidates based on the ground prior and scores them with semantic/instance segmentation, contextual information, object shape, and location prior.<br>MonoFlex</p></blockquote><p>在3D空间中生成proposal，投影回2D后结合分割、形状、位置先验进行评分，最后保留的作为结果。</p><h2 id="2019—ICIP-Shift-RCNN"><a href="#2019—ICIP-Shift-RCNN" class="headerlink" title="2019—ICIP-Shift RCNN"></a>2019—ICIP-Shift RCNN</h2><p>[<a href="https://arxiv.org/pdf/1905.09970.pdf">Paper</a>]     [Code]</p><blockquote><p>avoids dense proposal sampling by “actively” regressing the offset from the Deep3DBox proposal. They feed all the known 2D and 3D bbox info into a fast and simple fully connected network called ShiftNet and refine the 3D location</p></blockquote><h2 id="2019—ICCV—MonoDIS"><a href="#2019—ICCV—MonoDIS" class="headerlink" title="2019—ICCV—MonoDIS"></a>2019—ICCV—MonoDIS</h2><p>[<a href="https://arxiv.org/pdf/1905.09970.pdf">Paper</a>]     [Code]</p><blockquote><p>Instead of directly supervising each of the components of 2D and 3D bbox output, it takes a holistic view of bbox regression and uses 2D (signed) IoU loss and 3D corner loss. These losses are hard to train in general, so it proposes a disentanglement technique by fixing all elements but one group (including one or more elements) to ground truth and calculate the loss, essentially training only parameters in that group.</p></blockquote><h2 id="2019—CVPR—MonoPSR"><a href="#2019—CVPR—MonoPSR" class="headerlink" title="2019—CVPR—MonoPSR"></a>2019—CVPR—MonoPSR</h2><blockquote><p>It generates 3D proposal first and then reconstructing local point cloud of dynamic objects.<br>The reconstruction branch regresses a local point cloud of the object and compares it with the GT in point cloud and camera (after projection).</p></blockquote><h2 id="2020—CVPR—D4LCN"><a href="#2020—CVPR—D4LCN" class="headerlink" title="2020—CVPR—D4LCN"></a>2020—CVPR—D4LCN</h2><blockquote><p>took the idea of depth-aware convolution from M3D-RPN even further by introducing a dynamic filter prediction branch. This additional branch which takes in the depth prediction as input and generates a filter feature volume, which generates different filters for each specific location in terms of both weights and dilation rates.</p></blockquote><h1 id="RGB-Image-only"><a href="#RGB-Image-only" class="headerlink" title="RGB Image only"></a>RGB Image only</h1><p>基于RGB图像直接得到3D框通常是通过借助关键点、目标形状、2D-3D的几何一致性约束等来进行3D框的解耦从而得到最后的结果。<br>关注的目标大尺寸大多有相似的尺寸，这个信息对于估计到目标的距离有很大的帮助。<br>很多方法延续2D的方法来预测关键点。</p><h2 id="2016—CVPR—Deep3DBox"><a href="#2016—CVPR—Deep3DBox" class="headerlink" title="2016—CVPR—Deep3DBox"></a>2016—CVPR—Deep3DBox</h2><p>对于3D框的回归，分成了三个分支。一个分支预测目标框的size相当于一类目标的均值的偏差，另外两个分支用于预测角度。角度的回归使用的是MultiBin的方法，预测每个bin的置信度以及角度的正弦和余弦。得到2D的结果和3D的结果后，使用最小二乘法通过最小化重投影误差计算目标框的位置。<br>缺点在于非常依赖于2D框的精度。<br><img src="https://cdn.nlark.com/yuque/0/2022/png/478741/1668950168695-bbc010be-f9ea-4d70-b406-27cbf1950c11.png#averageHue=%23f7f6e6&amp;clientId=u9c117383-1941-4&amp;from=paste&amp;height=329&amp;id=u1fd5cfaa&amp;originHeight=657&amp;originWidth=816&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;size=167856&amp;status=done&amp;style=none&amp;taskId=ueb74e161-5352-4983-86d0-e65a9687d6e&amp;title=&amp;width=408" alt="image.png"></p><h2 id="2017—CVPR—Deep-MANTA"><a href="#2017—CVPR—Deep-MANTA" class="headerlink" title="2017—CVPR—Deep MANTA"></a>2017—CVPR—Deep MANTA</h2><p>本文是这一方案的先驱。首先使用级联的faster RCNN的架构来回归2D框、分类和模板的相似度。然后使用预先选择好的3D CAD模型，使用EpnP算法进行2D/3D的匹配。</p><h2 id="2018—IROS—The-Earth-ain’t-Flat"><a href="#2018—IROS—The-Earth-ain’t-Flat" class="headerlink" title="2018—IROS—The Earth ain’t Flat"></a>2018—IROS—The Earth ain’t Flat</h2><p>本文目标在陡峭和分级的道路上对车辆进行单目重建。一个关键设计在于从单目图像中估计3D shape和6自由度。本文参考Deep MANTA进行3D model的匹配，但不是从所有可能的 3D 形状模板中挑选出最好的一个，而是使用基本向量和变形系数来捕捉车辆的形状。</p><h2 id="2019—CVPR—RoI-10D"><a href="#2019—CVPR—RoI-10D" class="headerlink" title="2019—CVPR—RoI-10D"></a>2019—CVPR—RoI-10D</h2><p>10D=6DoF pose + 3DoF size + 1D shape space</p><h2 id="2019-AAAI-MonoGRNet"><a href="#2019-AAAI-MonoGRNet" class="headerlink" title="2019-AAAI-MonoGRNet"></a>2019-AAAI-MonoGRNet</h2><p>[<a href="https://arxiv.org/pdf/1811.10247.pdf">Paper</a>]     [<a href="https://github.com/Zengyi-Qin/MonoGRNet">Code</a>]<br>本文是由多个子网构成的统一网络，包括2D目标检测、实例深度估计、3D定位和局部角回归。<br>pixel-wise的深度估计最小化的是整张图像的所有像素得到一个平均最优估计，一些只占据了很小面积的目标就会被忽略了。<br>本文的核心思想是将3D定位问题解耦成几个循序渐进的子任务。<br>本文回归的是3D中心点的投影和粗糙的实例深度，然后使用两者估计粗糙的3D位置。它强调了2D框的中心点和3D框中心点在2D图像上的投影的不同。<br>本文没有直接回归更好观察的角度，而是回归了8个顶点相对于3D中心点的偏移。</p><h2 id="2019-ICCV-MVRA"><a href="#2019-ICCV-MVRA" class="headerlink" title="2019-ICCV-MVRA"></a>2019-ICCV-MVRA</h2><blockquote><p>It introduces a 3D reconstruction layer to lift 2D to 3D instead of solving an over-constrained equation, with two losses in two different spaces: 1) IoU loss in perspective view, between the reprojected 3D bbox and the 2d bbox in IoU, and 2) L2 loss in BEV loss between estimated distance and gt distance.<br>It recognizes that deep3DBox does not handle truncated box well, as not four sides of the bounding box now correspond to the real physical extent of the vehicle.</p></blockquote><h2 id="2019—CVPR—CenterNet—Objects-as-Points"><a href="#2019—CVPR—CenterNet—Objects-as-Points" class="headerlink" title="2019—CVPR—CenterNet—Objects as Points"></a>2019—CVPR—CenterNet—Objects as Points</h2><p>[<a href="https://arxiv.org/pdf/1904.07850.pdf">Paper</a>]     [<a href="https://github.com/xingyizhou/CenterNet">Code</a>]<br>本文认为许多方法列出全部潜在的目标框是一个计算冗余且不够高效的方案，本文将目标视作single point，利用关键点估计来回归每个目标框的中心点及其其他属性，如size、3D位置、朝向、甚至姿态。<br>在head部分的设计，是通过heatmap得到目标的位置，因此不需要NMS之类的后处理操作，也无需进行目标框的组合。对比anchor-based的方案来说，CenterNet要简单方便许多。</p><h2 id="GPP—Ground-Plane-Polling"><a href="#GPP—Ground-Plane-Polling" class="headerlink" title="GPP—Ground Plane Polling"></a>GPP—Ground Plane Polling</h2><p>本文使用3D框标注生成虚拟的2D关键点。</p><h2 id="2020—RTM3D—Real-time-Monocular-3D-Detection-from-Object-Keypoints-for-Autonomous-Driving"><a href="#2020—RTM3D—Real-time-Monocular-3D-Detection-from-Object-Keypoints-for-Autonomous-Driving" class="headerlink" title="2020—RTM3D—Real-time Monocular 3D Detection from Object Keypoints for Autonomous Driving"></a>2020—RTM3D—Real-time Monocular 3D Detection from Object Keypoints for Autonomous Driving</h2><blockquote><p>This article uses virtual keypoints and use CenterNet-like structure to directly detect the 2d projection of all 8 cuboid vertices + cuboid center. The paper also directly regresses distance, orientation, size. Instead of using these values to form cuboids directly, these values are used as the initial values (priors) to initialize the offline optimizer to generate 3D bboxes.</p></blockquote><h2 id="2020—CVPR—MonoPair—MonoPair-Monocular-3D-Object-Detection-Using-Pairwise-Spatial-Relationships"><a href="#2020—CVPR—MonoPair—MonoPair-Monocular-3D-Object-Detection-Using-Pairwise-Spatial-Relationships" class="headerlink" title="2020—CVPR—MonoPair—MonoPair: Monocular 3D Object Detection Using Pairwise Spatial Relationships"></a>2020—CVPR—MonoPair—MonoPair: Monocular 3D Object Detection Using Pairwise Spatial Relationships</h2><blockquote><p>MonoPair considers the pair-wise relationships between neighboring objects, which are utilized as spatial constraints to optimize the results of detection.</p></blockquote><p>有针对occluded目标进行处理<br>本文考虑了成对的目标之间的空间关系，加入了uncertainty的考虑。<br>Homography Loss for Monocular 3D Object Detection是本文的衍生方案，都使用了graph的方案，考虑了多目标之间的空间约束</p><h2 id="2020—CVPRW—SMOKE"><a href="#2020—CVPRW—SMOKE" class="headerlink" title="2020—CVPRW—SMOKE"></a>2020—CVPRW—SMOKE</h2><p>本文完全消除了 2D bbox 的回归并直接预测 3D bbox<br>显著降低了60米以内的distance error</p><h2 id="2020—AAAI—Monocular-3D-Object-Detection-with-Decoupled-Structured-Polygon-Estimation-and-Height-Guided-Depth-Estimation"><a href="#2020—AAAI—Monocular-3D-Object-Detection-with-Decoupled-Structured-Polygon-Estimation-and-Height-Guided-Depth-Estimation" class="headerlink" title="2020—AAAI—Monocular 3D Object Detection with Decoupled Structured Polygon Estimation and Height-Guided Depth Estimation"></a>2020—AAAI—Monocular 3D Object Detection with Decoupled Structured Polygon Estimation and Height-Guided Depth Estimation</h2><blockquote><p>The first work to clearly state that the estimation of the 2D projection of the 3D vertices is totally decoupled from the depth estimation.<br>It uses a similar method as RTM3D to regress the eight projected points of the cuboid, then uses vertical edge height as a strong prior to guide distance estimation. This generates a coarse 3D cuboid.</p></blockquote><h2 id="2021—CVPR—FCOS3D"><a href="#2021—CVPR—FCOS3D" class="headerlink" title="2021—CVPR—FCOS3D"></a>2021—CVPR—FCOS3D</h2><h1 id="Pesudo-LiDAR-and-BEV"><a href="#Pesudo-LiDAR-and-BEV" class="headerlink" title="Pesudo LiDAR and BEV"></a>Pesudo LiDAR and BEV</h1><p>在透视视角下，对于纯视觉的方案存在着遮挡和尺度变化的挑战。一些方法通过转换输入图像的表示形式来应对这个问题。<br>基于Pesudo-LiDAR和BEV的方案实际上都是将输入图像换了一种表示方法，近年来的主流BEV方法也多是借助Pseudo LiDAR的思想再过渡到BEV的表示方式，因此此处合并在一个类别下。</p><h2 id="BEV-without-Pseudo-LiDAR"><a href="#BEV-without-Pseudo-LiDAR" class="headerlink" title="BEV without Pseudo LiDAR"></a>BEV without Pseudo LiDAR</h2><p>BEV的表示下，不同的车辆之间不会出现重叠的现象。过去常使用IPM的方式得到BEV的图像，但是这种方案假设所有的像素都在地面上，并且需要获得相机精准的内外参。对于在路上行进的车辆而已，外参是实时变化的，其精度可能无法达到IPM的要求。</p><h3 id="2019—IV—Deep-Learning-based-Vehicle-Position-and-Orientation-Estimation-via-Inverse-Perspective-Mapping-Image"><a href="#2019—IV—Deep-Learning-based-Vehicle-Position-and-Orientation-Estimation-via-Inverse-Perspective-Mapping-Image" class="headerlink" title="2019—IV—Deep Learning based Vehicle Position and Orientation Estimation via Inverse Perspective Mapping Image"></a>2019—IV—Deep Learning based Vehicle Position and Orientation Estimation via Inverse Perspective Mapping Image</h3><p>这篇文章使用IMU数据来进行外参的在线标定从而得到更精确的IPM图像，然后在此之上进行目标检测。</p><h3 id="2019—BMVC—OFT—Orthographic-Feature-Transform-for-Monocular-3D-Object-Detection"><a href="#2019—BMVC—OFT—Orthographic-Feature-Transform-for-Monocular-3D-Object-Detection" class="headerlink" title="2019—BMVC—OFT—Orthographic Feature Transform for Monocular 3D Object Detection"></a>2019—BMVC—OFT—Orthographic Feature Transform for Monocular 3D Object Detection</h3><p>本文是将透视图转到BEV视角的另一种方式。其思想是利用正交特征变换来将透视视角的图像特征转到正交BEV下。然后通过在投影的voxel area上累加图像特征从而得到voxel-based的特征，再沿着垂直维度折叠voxel feature以产生正交地平面的特征。<br><img src="https://cdn.nlark.com/yuque/0/2022/png/478741/1668776253342-b70b7601-3e96-4dac-8a64-cf251da049cd.png#averageHue=%23f0f0ef&amp;clientId=ud79661c2-ad60-4&amp;from=paste&amp;height=252&amp;id=u1b5fb897&amp;originHeight=252&amp;originWidth=845&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;size=121370&amp;status=done&amp;style=none&amp;taskId=ue8ddc226-f280-4f9d-b12f-27682a739f7&amp;title=&amp;width=845" alt="image.png"><br>OFT的思路非常简单，效果也很好。</p><blockquote><p>Although the back-projection step could have been improved by using some heuristics for better initialization of the voxel-based features, rather than naively do a back-projection.<br>review的作者的观点，还需要熟悉一下文章体会一下。</p></blockquote><h2 id="Pseudo-LiDAR"><a href="#Pseudo-LiDAR" class="headerlink" title="Pseudo LiDAR"></a>Pseudo LiDAR</h2><p>基于Pseudo-LiDAR的方案的实质是根据2D图像估计深度，借助单目深度估计的方案生成点云。</p><h3 id="2017—CVPR—MonoDepth—Unsupervised-Monocular-Depth-Estimation-with-Left-Right-Consistency"><a href="#2017—CVPR—MonoDepth—Unsupervised-Monocular-Depth-Estimation-with-Left-Right-Consistency" class="headerlink" title="2017—CVPR—MonoDepth—Unsupervised Monocular Depth Estimation with Left-Right Consistency"></a>2017—CVPR—MonoDepth—Unsupervised Monocular Depth Estimation with Left-Right Consistency</h3><h3 id="2018—CVPR—MLF—Multi-Level-Fusion-based-3D-Object-Detection-from-Monocular-Images"><a href="#2018—CVPR—MLF—Multi-Level-Fusion-based-3D-Object-Detection-from-Monocular-Images" class="headerlink" title="2018—CVPR—MLF—Multi-Level Fusion based 3D Object Detection from Monocular Images"></a>2018—CVPR—MLF—Multi-Level Fusion based 3D Object Detection from Monocular Images</h3><p>本文是严格意义上第一篇提出将估计的深度信息lift到3D的，使用估计的深度信息将每个像素从RGB图像上投影到3D空间，然后再将得到的点云特征和图像特征融合起来得到3D目标框。<br><img src="https://cdn.nlark.com/yuque/0/2022/png/478741/1668777488770-0e5506ce-bde1-4af6-9959-c91a488564c3.png#averageHue=%23e2d2b7&amp;clientId=ud79661c2-ad60-4&amp;from=paste&amp;height=347&amp;id=u3d33257d&amp;originHeight=347&amp;originWidth=777&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;size=167932&amp;status=done&amp;style=none&amp;taskId=u30e094d5-b500-41b0-ad6e-b52b5c07eec&amp;title=&amp;width=777" alt="image.png"></p><h3 id="2018—CVPR—Pseudo-LiDAR-from-Visual-Depth-Estimation-Bridging-the-Gap-in-3D-Object-Detection-for-Autonomous-Driving"><a href="#2018—CVPR—Pseudo-LiDAR-from-Visual-Depth-Estimation-Bridging-the-Gap-in-3D-Object-Detection-for-Autonomous-Driving" class="headerlink" title="2018—CVPR—Pseudo-LiDAR from Visual Depth Estimation: Bridging the Gap in 3D Object Detection for Autonomous Driving"></a>2018—CVPR—Pseudo-LiDAR from Visual Depth Estimation: Bridging the Gap in 3D Object Detection for Autonomous Driving</h3><p>本文在得到Pseudo-LiDAR的特征后，直接使用最新的lidar-based的3D检测器。</p><blockquote><p> The authors argue that representation matters, and convolution on depth map does not make sense, as neighboring pixels on depth images may be physically far away in 3D space.<br>review</p></blockquote><h3 id="2018—CVPR—Frustum-PointNets-for-3D-Object-Detection-from-RGB-D-Data"><a href="#2018—CVPR—Frustum-PointNets-for-3D-Object-Detection-from-RGB-D-Data" class="headerlink" title="2018—CVPR—Frustum PointNets for 3D Object Detection from RGB-D Data"></a>2018—CVPR—Frustum PointNets for 3D Object Detection from RGB-D Data</h3><p><img src="https://cdn.nlark.com/yuque/0/2022/png/478741/1668848902425-3d8dbb09-3d3b-4de1-81d4-d22cf6ddf968.png#averageHue=%23d4d0cb&amp;clientId=uf53aa924-d691-4&amp;from=paste&amp;height=263&amp;id=u51387129&amp;originHeight=263&amp;originWidth=661&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;size=176617&amp;status=done&amp;style=none&amp;taskId=u884520b7-fa04-4ac1-9341-8dc357a6e72&amp;title=&amp;width=661" alt="image.png"></p><h3 id="2019—ICCV—Monocular-3D-Object-Detection-with-Pseudo-LiDAR-Point-Cloud"><a href="#2019—ICCV—Monocular-3D-Object-Detection-with-Pseudo-LiDAR-Point-Cloud" class="headerlink" title="2019—ICCV—Monocular 3D Object Detection with Pseudo-LiDAR Point Cloud"></a>2019—ICCV—Monocular 3D Object Detection with Pseudo-LiDAR Point Cloud</h3><p>本文提出了pseudo-lidar的方案的两个缺点：不准确的深度估计导致的local misalignment 和目标外围深度伪影导致的long tail。提出了实例分割mask，并引入了2D-3D框的consistency loss。<br><img src="https://cdn.nlark.com/yuque/0/2022/png/478741/1668848848450-f7427826-075c-413b-8f55-1ad7bd4487f1.png#averageHue=%23faf2cd&amp;clientId=uf53aa924-d691-4&amp;from=paste&amp;height=414&amp;id=u3315ef35&amp;originHeight=827&amp;originWidth=508&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;size=418515&amp;status=done&amp;style=none&amp;taskId=uf6b88a11-2464-4136-a612-36d07177c01&amp;title=&amp;width=254" alt="image.png"></p><h3 id="ForeSeE"><a href="#ForeSeE" class="headerlink" title="ForeSeE"></a>ForeSeE</h3><p>ForeSeE 也注意到了这些缺点，它们提出不是所有的pixel在深度估计中都有相同的重要性。因此它们训练了两个不同的深度估计器，一个用于前景、一个用于背景。在推理的时候自适应的融合深度图。</p><h3 id="2019—ICCV—AM3D—Accurate-Monocular-3D-Object-Detection-via-Color-Embedded-3D-Reconstruction-for-Autonomous-Driving"><a href="#2019—ICCV—AM3D—Accurate-Monocular-3D-Object-Detection-via-Color-Embedded-3D-Reconstruction-for-Autonomous-Driving" class="headerlink" title="2019—ICCV—AM3D—Accurate Monocular 3D Object Detection via Color-Embedded 3D Reconstruction for Autonomous Driving"></a>2019—ICCV—AM3D—Accurate Monocular 3D Object Detection via Color-Embedded 3D Reconstruction for Autonomous Driving</h3><blockquote><p>AM3D proposes a multi-modal fusion module to enhance the pseudo-LiDAR with color information</p></blockquote><h3 id=""><a href="#" class="headerlink" title=" "></a> </h3><h3 id="202008—ECCV2020—PatchNet—Rethinking-Pseudo-LiDAR-Representation"><a href="#202008—ECCV2020—PatchNet—Rethinking-Pseudo-LiDAR-Representation" class="headerlink" title="202008—ECCV2020—PatchNet—Rethinking Pseudo-LiDAR Representation"></a>202008—ECCV2020—PatchNet—Rethinking Pseudo-LiDAR Representation</h3><blockquote><p>PatchNet organizes pseudo-LiDAR into the image representation and utilizes powerful 2D CNN to boost the detection performance.<br>MonoFlex</p></blockquote><h3 id="202103—ICCV—Are-we-Missing-Confidence-in-Pseudo-LiDAR-Methods-for-Monocular-3D-Object-Detection"><a href="#202103—ICCV—Are-we-Missing-Confidence-in-Pseudo-LiDAR-Methods-for-Monocular-3D-Object-Detection" class="headerlink" title="202103—ICCV—Are we Missing Confidence in Pseudo-LiDAR Methods for Monocular 3D Object Detection?"></a>202103—ICCV—Are we Missing Confidence in Pseudo-LiDAR Methods for Monocular 3D Object Detection?</h3><p>本文认为虽然PL-based的方法表现出比RGB image only的方法更好的指标，但是并不意味着PL-based的方法是完全优于后者的，其中还有KITTI数据集中存在的问题。<br><img src="https://cdn.nlark.com/yuque/0/2022/png/478741/1668346439612-8e3d2178-e242-48a5-9737-8bc6d41d4143.png#averageHue=%2375e692&amp;clientId=u137eae7a-36b8-4&amp;from=paste&amp;height=574&amp;id=uc9dccc1a&amp;originHeight=574&amp;originWidth=678&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;size=141185&amp;status=done&amp;style=none&amp;taskId=u5d0519b3-35da-44f3-9086-715356b9b33&amp;title=&amp;width=678" alt="image.png"></p><h3 id="2021—CVPR—Objects-are-Different-Flexible-Monocular-3D-Object-Detection"><a href="#2021—CVPR—Objects-are-Different-Flexible-Monocular-3D-Object-Detection" class="headerlink" title="2021—CVPR—Objects are Different: Flexible Monocular 3D Object Detection"></a>2021—CVPR—Objects are Different: Flexible Monocular 3D Object Detection</h3><p>MonoFlex针对截断目标设计了不同的3D目标框解耦方式</p><p>DD3D</p><h3 id="2022—CVPR—Homography-Loss-for-Monocular-3D-Object-Detection"><a href="#2022—CVPR—Homography-Loss-for-Monocular-3D-Object-Detection" class="headerlink" title="2022—CVPR—Homography Loss for Monocular 3D Object Detection"></a>2022—CVPR—Homography Loss for Monocular 3D Object Detection</h3><p><img src="https://cdn.nlark.com/yuque/0/2022/png/478741/1668655897144-c7dab5f8-4ebe-4bad-b72b-c24c85c71505.png#averageHue=%23f6f6f5&amp;clientId=udc62ad13-1138-4&amp;from=drop&amp;id=ub50b52a1&amp;originHeight=312&amp;originWidth=1174&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;size=149118&amp;status=done&amp;style=none&amp;taskId=uc423b1ad-7ac0-4635-b41d-40a973f2216&amp;title=" alt="截屏2022-11-17 11.31.32.png"></p><h1 id="Related-Review"><a href="#Related-Review" class="headerlink" title="Related Review"></a>Related Review</h1><p>XPeng—-Patrick Langechuan Liu—-2020<br><a href="https://towardsdatascience.com/monocular-3d-object-detection-in-autonomous-driving-2476a3c7f57e">Monocular 3D Object Detection in Autonomous Driving — A Review</a></p>]]></content>
      
      
      <categories>
          
          <category> 系统梳理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Mono3D </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>BEVFormer论文解读</title>
      <link href="/2022/07/27/paper-bevformer-lun-wen-yu-dai-ma-jie-du/"/>
      <url>/2022/07/27/paper-bevformer-lun-wen-yu-dai-ma-jie-du/</url>
      
        <content type="html"><![CDATA[<p>BEVFormer论文解读<br><span id="more"></span></p><h2 id="BEVFormer"><a href="#BEVFormer" class="headerlink" title="BEVFormer"></a>BEVFormer</h2><p>BEVFormer是自顶向下的稠密BEV特征建模。文章主要是把DETR3D思想中的3D特征反投影到2D图像上以得到密集、准确的BEV特征，从而能够在此基础上更好的进行检测、分割。如果说DETR3D是一种新的3D目标框的生成、特征交互方式，那么BEVFormer应该是一种更精准、更有效的BEV特征的生成方式。</p><h3 id="BEVFormer整体结构"><a href="#BEVFormer整体结构" class="headerlink" title="BEVFormer整体结构"></a>BEVFormer整体结构</h3><p>如下图所示，BEVformer框架也是从DETR3D发展而来，也是从bev空间的3D query出发，得到参考点和采样点，再通过多相机的内外参投影到多视角2D图片上，和相应特征进行交互。但DETR3D的object queries是稀疏的，每个query代表一个可能的目标框，而Bevformer的bev queries是稠密的，大小为$H<em>W</em>C$，$H,W$为设定的bev特征尺度，每个query代表一个网格（grid）的查询，这样可以得到稠密的bev特征。<br><img src="/images/bevformer.png"/></p><center>图1 BEVFormer网络结构</center><p>具体来说，BEV Queries是鸟瞰视角网格状的可学习参数。每个query都有对应的固定的空间位置，也即唯一对应的BEV坐标系的坐标，结合各相机的内外参，可以很容易的得到每个BEV Query对应的空间位置在各个视角的图像上的对应位置。图像特征的编码和常规的2D图像的处理没有什么区别，仍然是通过ResNet+FPN等类似的方案实现。整个BEVFormer对BEV特征的编码方式，主要由Spatial Cross-Attention和Temporal Self-Attention组成，共同作用得到时空信息都得到增强的BEV特征。</p><h3 id="Spatial-Cross-Attention"><a href="#Spatial-Cross-Attention" class="headerlink" title="Spatial Cross-Attention"></a>Spatial Cross-Attention</h3><p>Spatial Cross-Attention是为了让每个随机初始化的BEV Query能够与其对应的图像特征进行交互，从而得到相应的空间信息。spatial cross-attention是基于Deformable Attention实现的，这是一个比较节省资源的交互方案，它能够让每个Query只与其对应的图像特征交互，而无需负担multi-head self attention进行全局交互所需的计算成本。<br>要实现BEV Queryies和2D的图像特征之间的交互，就需要找到每个Query与其对应的图像特征之间的对应关系。众所周知，从2D坐标到3D坐标的转换是一个不适定的问题，过去基于LSS的方式对每一个2D坐标进行了一个深度预测，深度的范围是$[2, 50)$。而将3D坐标转为2D坐标则能够避免深度的预测，只是我们初始化的BEV Queries实际上只有BEV的2D坐标，还缺乏车载坐标系的高度（z轴坐标），但高度的范围实际上要远远小于深度的范围。BEVFormer首先将每个Query提升成pillar-like Query，每个pillar对应4个不同的高度（通常在$[-5,3]$范围内取）。将高度与BEV坐标组合起来，就可以得到这个query对应的一组参考点，分别将他们投影到2D图像上。每个query可能投影到不止1个视角上，需要对该query对应的全部2D图像特征进行加权和以得到Spatial Cross-Attention的输出。</p><h3 id="Temporal-Self-Attention"><a href="#Temporal-Self-Attention" class="headerlink" title="Temporal Self-Attention"></a>Temporal Self-Attention</h3><p>Spatial Cross-Attention是为了让每个随机初始化的BEV Query能够与上一帧已经得到的BEV信息进行交互，从而得到相应的时序信息，辅助速度之类的属性预测。首先要根据惯导信息将时刻的BEV特征与当前时刻的BEV Queries $Q$对齐，得到$BEV’_{t-1}$。这是一个比较常见的操作，在之前<a href="https://massive11.github.io/2022/03/18/principle-fiery-yuan-li/">FIERY论文与代码讲解</a>的博客中也有相关的内容。然后也是使用一个deformable attention的操作进行时序上的交互，比较特别的部分是，这里是将$\{Q, BEV’_{t-1}\}$concate起来得到offset。</p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Multi-view 3D </tag>
            
            <tag> BEV Perception </tag>
            
            <tag> Attention </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>BEV模型深度估计模块Overview</title>
      <link href="/2022/07/26/paper-bev-mo-xing-shen-du-gu-ji-mo-kuai-overview/"/>
      <url>/2022/07/26/paper-bev-mo-xing-shen-du-gu-ji-mo-kuai-overview/</url>
      
        <content type="html"><![CDATA[<p>本文对BEV模型深度估计模块进行了梳理。<br><span id="more"></span></p><h2 id="1-图像特征直接卷积"><a href="#1-图像特征直接卷积" class="headerlink" title="1.图像特征直接卷积"></a>1.图像特征直接卷积</h2><h3 id="LSS、Fiery、BEVDet"><a href="#LSS、Fiery、BEVDet" class="headerlink" title="LSS、Fiery、BEVDet"></a>LSS、Fiery、BEVDet</h3><ul><li>深度估计模块（均为原始的LSS的模块，这里以Fiery的模块为例）</li></ul><p><img src="https://cdn.nlark.com/yuque/0/2022/png/478741/1664348295899-ef580e6c-5d37-434d-9bbd-1d9ca12c09f9.png#clientId=u73016541-a3c0-4&amp;errorMessage=unknown%20error&amp;from=ui&amp;id=u979da66c&amp;originHeight=582&amp;originWidth=1750&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;size=119663&amp;status=error&amp;style=none&amp;taskId=u74358c08-0cea-45f0-855b-3745b3fb6cd&amp;title=" alt="截屏2022-09-28 14.57.39.png"></p><ul><li>loss计算</li></ul><p>没有单独计算Depth部分的loss，直接计算的3D box的loss</p><hr><h2 id="2-Camera-Aware卷积"><a href="#2-Camera-Aware卷积" class="headerlink" title="2.Camera-Aware卷积"></a>2.Camera-Aware卷积</h2><h3 id="BEVDepth"><a href="#BEVDepth" class="headerlink" title="BEVDepth"></a>BEVDepth</h3><ul><li>深度估计模块</li></ul><p><img src="https://cdn.nlark.com/yuque/0/2022/png/478741/1664333207590-285175fa-af3c-4b4e-81fd-ee06c1221fba.png#clientId=uc9f89468-6e64-4&amp;errorMessage=unknown%20error&amp;from=paste&amp;height=363&amp;id=ub85d9bca&amp;originHeight=545&amp;originWidth=786&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;size=135374&amp;status=error&amp;style=none&amp;taskId=u7405615b-15a1-41cd-a083-9467cc45656&amp;title=&amp;width=524" alt="image.png"></p><ul><li>loss计算</li></ul><p>[H/16, W/16]的特征图和相应的真值之间计算BCE loss。<br>真值是将LiDAR点云先通过内外参投影到像素坐标系，经过resize处理后得到[256, 704]的depth真值。划分16×16的patch，每个patch取有深度点的<strong>最小深度值</strong>，若不在深度范围内则赋值0。得到[16, 44]的gt_depth。</p><ul><li>LiDAR点云真值downsample代码<pre class="line-numbers language-Python" data-language="Python"><code class="language-Python">def get_downsampled_gt_depth(self, gt_depths):    B, N, H, W &#x3D; gt_depths.shape    gt_depths &#x3D; gt_depths.view(B*N, H&#x2F;&#x2F;self.downsample_factor,        self.downsample_factor,        W &#x2F;&#x2F; self.downsample_factor, self.downsample_factor, 1)    gt_depths &#x3D; gt_depths.permute(0, 1, 3, 5, 2, 4).contiguous()    gt_depths &#x3D; gt_depths.view(        -1, self.downsample_factor * self.downsample_factor)    gt_depths_tmp &#x3D; torch.where(gt_depths &#x3D;&#x3D; 0.0,                                1e5 * torch.ones_like(gt_depths),                                gt_depths)    gt_depths &#x3D; torch.min(gt_depths_tmp, dim&#x3D;-1).values    gt_depths &#x3D; gt_depths.view(B * N, H &#x2F;&#x2F; self.downsample_factor,                               W &#x2F;&#x2F; self.downsample_factor)    gt_depths &#x3D; (gt_depths -                 (self.dbound[0] - self.dbound[2])) &#x2F; self.dbound[2]    gt_depths &#x3D; torch.where(        (gt_depths &lt; self.depth_channels + 1) &amp; (gt_depths &gt;&#x3D; 0.0),        gt_depths, torch.zeros_like(gt_depths))   # 6, 16, 44    gt_depths &#x3D; F.one_hot(gt_depths.long(),                          num_classes&#x3D;self.depth_channels + 1).view(                              -1, self.depth_channels + 1)[:, 1:]    return gt_depths.float()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="单目上Camera-Aware卷积的模型"><a href="#单目上Camera-Aware卷积的模型" class="headerlink" title="单目上Camera-Aware卷积的模型"></a>单目上Camera-Aware卷积的模型</h3><h4 id="DD3D"><a href="#DD3D" class="headerlink" title="DD3D"></a>DD3D</h4></li></ul><ul><li>深度估计模块</li></ul><p>先进行深度网络预训练。<br>“Two paths in DD3D from the input image to the 3D bounding box and to the dense depth prediction differ only in the last 3×3 convolutional layer, and thus share nearly all parameters.”</p><ul><li>loss计算</li></ul><p><img src="https://cdn.nlark.com/yuque/0/2022/png/478741/1664352247582-6513d9b6-0753-4cc6-87db-0384d7ba8f3a.png#clientId=u7261c876-16a5-4&amp;errorMessage=unknown%20error&amp;from=paste&amp;height=299&amp;id=u83b85434&amp;originHeight=449&amp;originWidth=924&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;size=93733&amp;status=error&amp;style=none&amp;taskId=u9f8cda92-d471-43b9-9662-d5fec36cc53&amp;title=&amp;width=616" alt="image.png"></p><hr><h2 id="3-Gaussian参数估计"><a href="#3-Gaussian参数估计" class="headerlink" title="3.Gaussian参数估计"></a>3.Gaussian参数估计</h2><h3 id="BEVStereo"><a href="#BEVStereo" class="headerlink" title="BEVStereo"></a>BEVStereo</h3><ul><li>深度估计模块<ul><li>基本参考MaGNet实现的</li></ul></li></ul><p><img src="https://cdn.nlark.com/yuque/0/2022/png/478741/1664334366753-81bcf681-64f7-4579-a7fe-43834f263167.png#clientId=uc9f89468-6e64-4&amp;errorMessage=unknown%20error&amp;from=paste&amp;height=678&amp;id=u0d32a70e&amp;originHeight=1017&amp;originWidth=2167&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;size=615610&amp;status=error&amp;style=none&amp;taskId=u53b7a47e-d78b-4bea-ad4f-c90fbc26c7d&amp;title=&amp;width=1444.6666666666667" alt="image.png"></p><ul><li>loss计算</li></ul><p>[H/4, W/4]的特征图和相应的真值之间计算BCE loss</p><h3 id="单目上Gaussian参数估计的模型"><a href="#单目上Gaussian参数估计的模型" class="headerlink" title="单目上Gaussian参数估计的模型"></a>单目上Gaussian参数估计的模型</h3><h4 id="MaGNet"><a href="#MaGNet" class="headerlink" title="MaGNet"></a>MaGNet</h4><ul><li>深度估计模块</li></ul><p>DNet将每个像素(u, v)的分布参数化为高斯分布。预训练DNet，完整训练时固定DNet的网络参数。<br><img src="https://cdn.nlark.com/yuque/0/2022/png/478741/1664350299798-da8e63cb-70ec-48d9-be6b-6631b92ff589.png#clientId=u7261c876-16a5-4&amp;errorMessage=unknown%20error&amp;from=paste&amp;height=515&amp;id=ue19e96bb&amp;originHeight=773&amp;originWidth=1596&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;size=521747&amp;status=error&amp;style=none&amp;taskId=u81f63b6d-398c-4fde-a9d9-f078fa02c5c&amp;title=&amp;width=1064" alt="image.png"><br>本文的contribution主要是融合单视图深度概率与多视图几何，具体由以下几个模块组成</p><ul><li>Probabilistic Depth Sampling</li></ul><p>在<img src="https://cdn.nlark.com/yuque/__latex/17a8f206259a783f10b67682ecb10278.svg#card=math&amp;code=%5B%5Cmu-%5Cbeta%5Csigma%2C%20%5Cmu%2B%5Cbeta%5Csigma%5D&amp;id=XyE3S" alt="">的搜索区间里面去找depth candidates，搜索范围更接近Gaussian的参数<br><img src="https://cdn.nlark.com/yuque/0/2022/png/478741/1664350349804-d1a004d5-3ff6-4f90-b006-af313f227430.png#clientId=u7261c876-16a5-4&amp;errorMessage=unknown%20error&amp;from=paste&amp;height=285&amp;id=ubaa7a5d6&amp;originHeight=427&amp;originWidth=793&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;size=70934&amp;status=error&amp;style=none&amp;taskId=ub3ad164a-6859-4f1e-8f97-2e6c3db3396&amp;title=&amp;width=528.6666666666666" alt="image.png"></p><ul><li>Depth consistency weighting for multi-view matching</li></ul><p>“Depth consistency weighting discards the candidates with low single-view depth probability.”</p><ul><li>Iterative refinement</li></ul><p><img src="https://cdn.nlark.com/yuque/0/2022/png/478741/1664350428858-380bfa02-6410-4958-8e3f-5050fcc7eaf6.png#clientId=u7261c876-16a5-4&amp;errorMessage=unknown%20error&amp;from=paste&amp;height=454&amp;id=u9a8cdc2f&amp;originHeight=681&amp;originWidth=1695&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;size=367191&amp;status=error&amp;style=none&amp;taskId=u7ba05a05-5cf8-45ad-81e4-5c167938a4e&amp;title=&amp;width=1130" alt="image.png"></p><ul><li>loss计算</li></ul><p><img src="https://cdn.nlark.com/yuque/0/2022/png/478741/1664350492107-fd0ec7b3-f712-4ff6-9b61-f57fe1528eee.png#clientId=u7261c876-16a5-4&amp;errorMessage=unknown%20error&amp;from=paste&amp;height=75&amp;id=ubad58ffd&amp;originHeight=113&amp;originWidth=837&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;size=23693&amp;status=error&amp;style=none&amp;taskId=ubf9251eb-b325-409d-83cb-e8ce2f972a9&amp;title=&amp;width=558" alt="image.png"></p><hr><h2 id="4-单目时序深度估计模型"><a href="#4-单目时序深度估计模型" class="headerlink" title="4.单目时序深度估计模型"></a>4.单目时序深度估计模型</h2><h3 id="DFM"><a href="#DFM" class="headerlink" title="DFM"></a>DFM</h3>]]></content>
      
      
      <categories>
          
          <category> 系统梳理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Multi-view 3D </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>3D目标框解耦方案梳理</title>
      <link href="/2022/07/18/paper-3d-mu-biao-kuang-jie-ou-fang-an-shu-li/"/>
      <url>/2022/07/18/paper-3d-mu-biao-kuang-jie-ou-fang-an-shu-li/</url>
      
        <content type="html"><![CDATA[<p>本文对3D目标框解耦方案进行了梳理。<br><span id="more"></span></p><p>直接通过RGB图像预测3D目标框由于深度信息的缺失存在困难，但由于自动驾驶场景中关注的目标尺寸比较一致，可以通过借鉴2D-3D的一致性信息作为先验来提升模型效果。因此，存在不同的3D目标框解耦方案，以下将对相关论文进行简单梳理。</p><h1 id="2017—CVPR—Deep3DBox—3D-Bounding-Box-Estimation-Using-Deep-Learning-and-Geometry"><a href="#2017—CVPR—Deep3DBox—3D-Bounding-Box-Estimation-Using-Deep-Learning-and-Geometry" class="headerlink" title="2017—CVPR—Deep3DBox—3D Bounding Box Estimation Using Deep Learning and Geometry"></a>2017—CVPR—Deep3DBox—3D Bounding Box Estimation Using Deep Learning and Geometry</h1><p>[<a href="https://arxiv.org/pdf/1612.00496.pdf">Paper</a>]     [Code]<br>本文认为3D框的透视投影与其2D检测窗口联系紧密，根据目标的2D框及周围像素估计目标的3D框和姿态，因此非常依赖2D框的精度。<br>针对size回归，计算了每个类别的均值，网络估计每个目标相对其类别的平均size的偏差；针对角度回归，本文提出MultiBin的方案回归local orientation；针对目标位置，得到上述信息后，通过最小二乘法最小化重投影误差进行计算。</p><h2 id="Dimension"><a href="#Dimension" class="headerlink" title="Dimension"></a>Dimension</h2><p>深度估计的变化很小，同一类别的size比较接近，并且不随目标的朝向变化而变化。得到目标的类别后，dimension实际上就基本能确定下来。<br>对于目标的size，使用L2 loss，计算如下</p><script type="math/tex; mode=display">L_{dims}=\frac{1}{n}  \sum (D^* - \overline{D} - \delta)^2</script><p>其中，$D^*$是目标框的真值，$\overline{D}$是固定类别的目标框均值，$\delta$是网络估计的相对于均值的偏差。</p><h2 id="Orientation"><a href="#Orientation" class="headerlink" title="Orientation"></a>Orientation</h2><p>仅从检测窗口中去估计相机坐标下的目标global orientation是不现实的。如下图所示，一辆沿直线行驶的车辆，尽管其global orientation没有发生变化，但其local orientation一直在发生变化导致外观上看起来很大的不同。仅从2D上的检测框范围内来确定其角度无法得到准确的结果，受到距离和角度两方面的影响。<br><img src="https://cdn.nlark.com/yuque/0/2022/png/478741/1668927963099-c9e35577-0709-4c8a-a7c0-428d4bc1280c.png#averageHue=%2341453f&amp;clientId=uefed95a9-2cad-4&amp;from=paste&amp;height=357&amp;id=u904acb1f&amp;originHeight=714&amp;originWidth=833&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;size=635745&amp;status=done&amp;style=none&amp;taskId=u0cb2b0c9-c70c-4ea7-8eb3-66ba3856f21&amp;title=&amp;width=417" alt="image.png"><br>因此，本文回归local orientation，local orientation和global orientation示意图如下图所示<br><img src="https://cdn.nlark.com/yuque/0/2022/png/478741/1668866616524-19703d13-6e4d-4019-90e1-f5e15cd6e2af.png#averageHue=%23e7e9e3&amp;clientId=uad073999-40d0-4&amp;from=paste&amp;id=u839e9e14&amp;originHeight=375&amp;originWidth=382&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;size=67679&amp;status=done&amp;style=none&amp;taskId=u029a028f-e836-4180-8b85-6159a41fdd2&amp;title=" alt="image.png"><br>本文提出了MultiBin的分解方案。首先，将方向角离散化，并将其分成了n个重叠的bin。对于每个bin，CNN网络估计输出的角度位于第i个bin内的置信概率以及为了得到输出角度，需要应用于该bin中心射线方向的residual rotation error（剩余旋转矫正）。residual rotation使用角度的正弦和余弦两个数值来表示。因此，每个bin的输出一共三个：$(c_i,cos(\Delta \theta_i),sin(\Delta \theta_i))$。<br>有效的正弦和余弦数值是通过在二维输入上应用L2 normalization 层得到的。<br>MultiBin方案的整体loss计算如下</p><script type="math/tex; mode=display">L_\theta = L_{conf} +\omega \times L_{loc}</script><p>$L_{conf}$等价于每个bin的置信度的softmax loss。<br>$L_{loc}$在每个覆盖了角度真值的bin之中最小化估计的角度与真值的差异。由于bin之间存在重叠，每个包含真值的bin都需要估计正确的角度，其计算如下</p><script type="math/tex; mode=display">L_{loc}=-\frac{1}{n_{\theta*}}  \sum cos(\theta ^ * - c_i  - \Delta \theta_i)</script><p>其中，$n_{\theta<em>}$是覆盖了角度真值$\theta</em>$的bin的数量，$c_i$是bin $i$的中心的角度，$\Delta \theta_i$是在bin i的中心上施加的变化。</p><h2 id="Location"><a href="#Location" class="headerlink" title="Location"></a>Location</h2><p>没有直接回归位置，得到目标的3D dimension和orientation以及2D检测框后，使用最小二乘来最小化重投影误差以计算目标相对于相机的平移T。<br><img src="https://cdn.nlark.com/yuque/0/2022/png/478741/1668866601033-5fb855c0-0b3c-4764-9690-4b724d4a7390.png#averageHue=%239a9d87&amp;clientId=uad073999-40d0-4&amp;from=paste&amp;height=337&amp;id=u67445f08&amp;originHeight=366&amp;originWidth=380&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;size=141039&amp;status=done&amp;style=none&amp;taskId=uf9e71a9a-fcf8-47eb-80e9-c6242343aeb&amp;title=&amp;width=350" alt="image.png"><br>总体的loss组成如下</p><script type="math/tex; mode=display">L = \alpha \times L_{dims} + L_{\theta}</script><h1 id="2019—AAAI—MonoGRNet—MonoGRNet-A-Geometric-Reasoning-Network-for-Monocular-3D-Object-Localization"><a href="#2019—AAAI—MonoGRNet—MonoGRNet-A-Geometric-Reasoning-Network-for-Monocular-3D-Object-Localization" class="headerlink" title="2019—AAAI—MonoGRNet—MonoGRNet: A Geometric Reasoning Network for Monocular 3D Object Localization"></a>2019—AAAI—MonoGRNet—MonoGRNet: A Geometric Reasoning Network for Monocular 3D Object Localization</h1><p>[<a href="https://arxiv.org/pdf/1811.10247.pdf">Paper</a>]     [<a href="https://github.com/Zengyi-Qin/MonoGRNet">Code</a>]</p><blockquote><p>MonoGRNet predicts 3D object localizations from a monocular RGB image considering geometric reasoning in 2D projection and the unobserved depth dimension.<br>MonoPair</p></blockquote><p>本文核心是将3D定位问题解耦成几个循序渐进的子问题，这些子问题只需要RGB数据就能解决。<br><img src="https://cdn.nlark.com/yuque/0/2022/png/478741/1669429825762-5b5885d3-9077-4cdc-8ec7-6aa1c128aeef.png#averageHue=%23f6f5f3&amp;clientId=ud9dbc665-bd88-4&amp;from=drop&amp;id=u3423a3d6&amp;originHeight=502&amp;originWidth=1299&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;size=142671&amp;status=done&amp;style=none&amp;taskId=u319f6500-cad7-4fcd-8fe9-90d133c9f1b&amp;title=" alt="截屏2022-11-26 10.30.19.png"></p><h2 id="3D-loc"><a href="#3D-loc" class="headerlink" title="3D loc"></a>3D loc</h2><p>本文将3D 定位任务ABBox-3D 定位$B_{3d}$分解成四个渐进式的子任务。<br>首先，检测3D框的2D 投影框$B_{2d}$的中心$b$和尺寸$(w,h)$。<br>然后，通过预测深度$Z_c$和3D中心点的2D投影点$c$定位3D中心点$C$<br>最后，基于局部特征回归相对于3D中心点的局部角$O$<br>定位任务可描述为</p><script type="math/tex; mode=display">B_{3d}=(B_{2d},Z_c,c,O)</script><p><img src="https://cdn.nlark.com/yuque/0/2022/png/478741/1669463677646-2b8bc33c-75eb-4ddc-8dbd-8fcfa91d3766.png#averageHue=%23f7f2ee&amp;clientId=ud9dbc665-bd88-4&amp;from=drop&amp;id=ud2c9cb19&amp;originHeight=329&amp;originWidth=879&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;size=80114&amp;status=done&amp;style=none&amp;taskId=u9322771f-ce08-41f7-9eaa-778a3e56779&amp;title=" alt="截屏2022-11-26 19.54.32.png"><br>分别回归2D框的中心点$b$和3D框在2D图像上的投影$c$，由于透视变化，两个点通常不会重合。<br>首先回归$c$，然后基于估计的深度$Z_c$反投影到3D空间。</p><script type="math/tex; mode=display">X=(u-p_x)*Z_c/f_x</script><script type="math/tex; mode=display">Y=(v-p_y)*Z_c/f_y</script><p>因此，3D定位问题就能转化为一个2D关键点定位任务进行求解。</p><h2 id="IDE-Instance-level-depth-estimation"><a href="#IDE-Instance-level-depth-estimation" class="headerlink" title="IDE(Instance-level depth estimation)"></a>IDE(Instance-level depth estimation)</h2><p>每个grid预测距离其在一定距离阈值内最近的实例的深度，距离更近的目标解决遮挡的问题。<br><img src="https://cdn.nlark.com/yuque/0/2022/png/478741/1669454605948-5a209344-e4bc-4723-8e48-c1108bac7525.png#averageHue=%2399a792&amp;clientId=ud9dbc665-bd88-4&amp;from=drop&amp;id=u6c1c56a3&amp;originHeight=296&amp;originWidth=777&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;size=135690&amp;status=done&amp;style=none&amp;taskId=ua10202b2-69c0-40e5-a665-e890bfa3da1&amp;title=" alt="截屏2022-11-26 17.23.20.png"><br>IDE模块包括区域深度的粗略回归，不考虑目标的尺度和特定的2D位置；和一个细化阶段，依赖2D边界框来精确提取目标所占区域编码的深度特征。<br><img src="https://cdn.nlark.com/yuque/0/2022/png/478741/1669463337241-a88c710f-bbfa-4940-b8c7-582af6d2c56f.png#averageHue=%23f6f6f5&amp;clientId=ud9dbc665-bd88-4&amp;from=drop&amp;id=uc1c32191&amp;originHeight=265&amp;originWidth=877&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;size=57397&amp;status=done&amp;style=none&amp;taskId=u089368ac-45f2-4c60-97d6-f500c3b0159&amp;title=" alt="截屏2022-11-26 19.48.52.png"><br>粗略回归的特征图小，对于目标的实际位置没有那么敏感，负责回归粗略的深度偏移$Z_{cc}$；对齐的特征回归<script type="math/tex">\delta_{Z_c}$，用于修正instance-level的深度值。最终的深度预测为$Z_c=Z_{cc}+\delta_{Z_c}</script></p><h2 id="3D-Box-Corner-Regression"><a href="#3D-Box-Corner-Regression" class="headerlink" title="3D Box Corner Regression"></a>3D Box Corner Regression</h2><p>这个子网回归局部坐标系下的八个角点，局部坐标系是参考Deep3DBox构建的</p><h2 id="loss"><a href="#loss" class="headerlink" title="loss"></a>loss</h2><h3 id="2D-detection-loss"><a href="#2D-detection-loss" class="headerlink" title="2D detection loss"></a>2D detection loss</h3><p>目标置信度的训练使用的是softmax cross entropy，2D bounding box回归使用的是masked L1 distance loss</p><script type="math/tex; mode=display">L_{conf}=CE_{g\in \mathcal{g}} (s \cdot (Pr_{obj}^{g}), \widetilde{Pr}_{obj}^g)</script><script type="math/tex; mode=display">L_{bbox}=\sum_g \mathbb{I}_g^{obj} \cdot d(B_{2d}^g,\widetilde{B}_{2d}^g)</script><script type="math/tex; mode=display">L_{2d}=L_{conf}+\omega L_{bbox}</script><h3 id="Instance-Depth-loss"><a href="#Instance-Depth-loss" class="headerlink" title="Instance Depth loss"></a>Instance Depth loss</h3><p>L1 loss</p><script type="math/tex; mode=display">L_{zc}=\sum_g \mathbb{I}_g^{obj} \cdot d(Z_{cc}^g,\widetilde{Z}_c^g)</script><script type="math/tex; mode=display">L_{z\delta}=\sum_g \mathbb{I}_g^{obj} \cdot d(Z_{cc}^g+\delta_{Z_c}^g,\widetilde{Z}_c^g)</script><script type="math/tex; mode=display">L_{depth}=\alpha L_{zc}+L_{z\delta}</script><h3 id="3D-localization-loss"><a href="#3D-localization-loss" class="headerlink" title="3D localization loss"></a>3D localization loss</h3><p>L1 loss</p><script type="math/tex; mode=display">L_c^{2d}=\sum_g \mathbb{I}_g^{obj} \cdot d(g+\delta_C^g,\widetilde{c}^g)</script><script type="math/tex; mode=display">L_c^{3d}=\sum_g \mathbb{I}_g^{obj} \cdot d(C_s^g+\delta_C^g,\widetilde{C}^g)</script><script type="math/tex; mode=display">L_{location}=\beta L_c^{2d} + L_c^{3d}</script><h3 id="Local-Corner-Loss"><a href="#Local-Corner-Loss" class="headerlink" title="Local Corner Loss"></a>Local Corner Loss</h3><script type="math/tex; mode=display">L_{corners}=\sum_g \sum_k \mathbb{I}_g^{obj} \cdot d(O_k,\widetilde{O}_k)</script><h3 id="Joint-3D-Loss"><a href="#Joint-3D-Loss" class="headerlink" title="Joint 3D Loss"></a>Joint 3D Loss</h3><p>预测应当是一个整体，各个part之间也要建立一定的联系。我们将joint 3D loss表示为相机坐标系的角点坐标的距离之和</p><script type="math/tex; mode=display">L_{joint}=\sum_g\sum_k     \mathbb{I}_{g}^{obj}\cdot d(O_k^{cam},\widetilde{O}_k^{cam})</script><h1 id="2019—CVPR—CenterNet—Objects-as-Points"><a href="#2019—CVPR—CenterNet—Objects-as-Points" class="headerlink" title="2019—CVPR—CenterNet—Objects as Points"></a>2019—CVPR—CenterNet—Objects as Points</h1><p>[<a href="https://arxiv.org/pdf/1904.07850.pdf">Paper</a>]     [<a href="https://github.com/xingyizhou/CenterNet">Code</a>]</p><h2 id="2D-Detection"><a href="#2D-Detection" class="headerlink" title="2D Detection"></a>2D Detection</h2><p>CenterNet采用关键点回归的思路，首先先生成一个关键点的heatmap，并使用Gaussian Kernel对其splat</p><script type="math/tex; mode=display">Y_{xyc}=exp(- \frac{(x-\widetilde p_x)^2+(y-\widetilde p_y)^2}{2\sigma^2_p})</script><p>其中，$\sigma_p$是目标大小自适应的标准偏差。（如果同一个类别的Gaussian重叠了，取每元素的最大值）</p><p>对于heatmap，训练的目标是使用focal loss减少惩罚的像素级别的logistic regression</p><script type="math/tex; mode=display">L_k=\frac{-1}{N}\sum_{xyc}\left\{\begin{matrix}(1-\hat{Y}_{xyc})^\alpha log(\hat{Y}_{xyc}) &&if&Y_{xyc}=1\\(1-Y_{xyc})^\beta(\hat{Y}_{xyc})^\alphalog(1-\hat{Y}_{xyc}) && otherwise\\\end{matrix}\right.</script><p>其中，$\alpha$和$\beta$是focal loss的超参数（论文中设置的都是$\alpha=2$和$\beta=4$），N是图像的关键点数量。</p><p>为了恢复由输出步幅引起的离散化误差，对每个中心点增加了一个local offset。所有类别共享一个预测的offset，使用L1 loss进行训练（这个监督只对关键点的位置$\widetilde{p}$起作用，其他位置都忽略）</p><script type="math/tex; mode=display">L_{off}=\frac{1}{N}\sum_p|\hat{O}_p-(\frac{p}{R}-\widetilde{p})|</script><p>令$(x_1^{(k)}, y_1^{(k)}, x_2^{(k)}, y_2^{(k)})$是类别$c_k$目标$k$的bounding box，其中心点位于</p><script type="math/tex; mode=display">p_k=(\frac{x_1^{(k)}+x_2^{(k)}}{2}, \frac{y_1^{(k)}+y_2^{(k)}}{2})</script><p>对每个目标$k$回归其size </p><script type="math/tex; mode=display">s_k=(x_2^{(k)}-x_1^{(k)}, y_2^{(k)}-y_1^{(k)})</script><p>为了限制计算开销，对每个中心点使用L1 loss进行简单的size预测</p><script type="math/tex; mode=display">L_{size}=\frac{1}{N}\sum_{k=1}^N|\hat{S}_{p_k}-s_k|</script><p>总的训练目标</p><script type="math/tex; mode=display">L_{det}=L_k+\lambda_{size}L_{size}+\lambda_{off}L_{off}</script><p>论文中设置$\lambda_{size}=0.1$和$\lambda_{off}=1$。</p><p>在推理阶段，将points组合成boxes，令$\hat{P}_c$是类别$c$检测出的$n$个中心点$\hat{P}=\{(\hat{x_i}, \hat{y_i}) \}_{i=1}^n$的set 。每个关键点的位置由一个整数坐标$(x_i, y_i)$给出。用关键点数值$\hat{Y}_{x_iy_ic}$作为检测置信度的衡量，可以得到bounding box的位置</p><script type="math/tex; mode=display">(\hat{x}_i+\delta \hat{x}_i-\frac{\hat{w}_i}{2},\hat{y}_i+\delta \hat{y}_i-\frac{\hat{h}_i}{2},\hat{x}_i+\delta \hat{x}_i+\frac{\hat{w}_i}{2},\hat{y}_i+\delta \hat{y}_i+\frac{\hat{h}_i}{2})</script><p>其中，$(\delta\hat{x}_i,\delta\hat{y}_i)=\hat{O}_{\hat{x}_i,\hat{y}_i}$是偏移量的预测，$(\hat{w}_i,\hat{h}_i)=\hat{S}_{\hat{x}_i,\hat{y}_i}$是目标框size的预测。</p><p>所有的输出都直接从关键点预测给出，不需要基于IoU的NMS或是其他后处理。</p><h2 id="3D-Detection"><a href="#3D-Detection" class="headerlink" title="3D Detection"></a>3D Detection</h2><h3 id="3D-dimension"><a href="#3D-dimension" class="headerlink" title="3D dimension"></a>3D dimension</h3><p>使用绝对度量的L1 loss训练3D box的size</p><script type="math/tex; mode=display">L_{dim}=\frac{1}{N}\sum_{k=1}^N | \hat{\gamma}_k - \gamma_k|</script><h3 id="depth-d"><a href="#depth-d" class="headerlink" title="depth d"></a>depth d</h3><p>depth很难直接回归，因此我们参考的<a href="https://arxiv.org/pdf/1406.2283.pdf">Depth map prediction</a>输出变换，预测</p><script type="math/tex; mode=display">d=\frac{1}{\sigma(\hat{d})} - 1</script><p>其中，$\sigma$是sigmoid函数。<br>在原始的深度上进行sigmoid变换后，使用L1 loss计算损失</p><script type="math/tex; mode=display">L_{dep}=\frac{1}{N}\sum_{k=1}^N | \frac{1}{\sigma(\hat{d}_k)}-1-d_k |</script><h3 id="orientation"><a href="#orientation" class="headerlink" title="orientation"></a>orientation</h3><p>orientation同样很难直接回归，参考<a href="https://arxiv.org/pdf/1612.00496.pdf">Deep3DBox</a>，使用in-bin回归将方向表示为两个bin。方向回归使用8-scalar encoding，每个bin四个标量，其中两个用于softmax分类，剩下两个回归in-bin offset的角度的正弦和余弦值。<br>分类使用softmax进行训练，角度值使用L1 loss进行训练</p><script type="math/tex; mode=display">L_{ori}=\frac{1}{N}\sum_{k=1}^N \sum_{i=1}^2(softmax(\hat{b}_i,c_i)+c_i|\hat{a}_i-a_i|)</script><p>其中，$a_i=(sin(\theta-m_i), cos(\theta-m_i))$</p><p>预测的方向$\theta$从8-scalar encoding中解码</p><script type="math/tex; mode=display">\hat{\theta}=arctan2(\hat{a}_{j1},\hat{a}_{j2})+m_j</script><p>其中，$j$是分类score最大的bin的索引</p><h1 id="2020—CVPR—CenterPoint—Center-based-3D-object-detection-and-tracking"><a href="#2020—CVPR—CenterPoint—Center-based-3D-object-detection-and-tracking" class="headerlink" title="2020—CVPR—CenterPoint—Center-based 3D object detection and tracking"></a>2020—CVPR—CenterPoint—Center-based 3D object detection and tracking</h1><p>[<a href="https://arxiv.org/pdf/2006.11275.pdf">Paper</a>]     [<a href="https://github.com/tianweiy/CenterPoint">Code</a>]<br>CenterPoint是一个两阶段的架构。第一个阶段预测class-specific的heatmap、目标框的size、亚体素位置细化(sub-voxel location refinement)、旋转和速度，所有的输出都是密集预测。第二个阶段在第一阶段的预测结果之上预测类别不知（class-agnostic）的置信度得分和目标框细化。</p><h2 id="Stage-1"><a href="#Stage-1" class="headerlink" title="Stage 1"></a>Stage 1</h2><h3 id="heatmap"><a href="#heatmap" class="headerlink" title="heatmap"></a>heatmap</h3><p>heatmap使用Gaussian核对真值中心点进行展开，增大真值的比例，使用focal loss进行训练。高斯核半径设置为$\sigma=max(f(wl),\tau)$，其中$\tau=2$是允许的最小的高斯半径，$f$是CornerNet中定义的半径函数。</p><h3 id="loc"><a href="#loc" class="headerlink" title="loc"></a>loc</h3><p>sub-voxel location refinement $o$减少了backbone体素化和striding的量化误差</p><h3 id="height"><a href="#height" class="headerlink" title="height"></a>height</h3><p>地面上高度帮助定位目标的3D位置，并添加地图投影中丢失的海拔信息。</p><h3 id="Orientation-1"><a href="#Orientation-1" class="headerlink" title="Orientation"></a>Orientation</h3><p>使用角度的正弦和余弦进行回归。</p><h2 id="Stage-2"><a href="#Stage-2" class="headerlink" title="Stage 2"></a>Stage 2</h2><h3 id="class-agnostic-confidence-score-prediction"><a href="#class-agnostic-confidence-score-prediction" class="headerlink" title="class-agnostic confidence score prediction"></a>class-agnostic confidence score prediction</h3><p>使用目标框与相应真值框的3D IoU引导的目标分数$I$<br>$I=min(1,max(0,2\times IoU_t-0.5))$<br>使用BCE loss进行训练</p><script type="math/tex; mode=display">L_{score}=-I_t log(\hat{I}_t)-(1-I_t)log(1-\hat{I}_t)</script><h1 id="2020—MonoPair"><a href="#2020—MonoPair" class="headerlink" title="2020—MonoPair"></a>2020—MonoPair</h1><p><img src="https://cdn.nlark.com/yuque/0/2022/png/478741/1668651070095-af215080-895e-42d3-8b7c-a649d12e5e3f.png#averageHue=%23cce0bd&amp;clientId=u8cc0cffd-c905-4&amp;from=drop&amp;id=u1138dcb9&amp;originHeight=640&amp;originWidth=2532&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;size=749102&amp;status=done&amp;style=none&amp;taskId=u3ad3e383-df60-43c6-bb12-86e7ba082f6&amp;title=" alt="截屏2022-11-17 10.11.04.png"><br>一共11个分支，分为三个部分，2D检测、3D检测和成对约束。</p><ul><li>2D检测<ul><li>heatmap用于关键点的定位和分类，参考的是centernet</li><li>offset输出目标位置相对于目标框中心点的偏移<ul><li>L1 loss</li></ul></li><li>dimension输出2D目标框的size $[w_b,h_b]$<ul><li>L1 loss</li></ul></li></ul></li><li>3D检测<ul><li>depth<ul><li>参考Depth Map Prediction from a Single Image using a Multi-Scale Deep Network。</li><li>transforming the absolute depth by inverse sigmoid transformation z = 1/σ(ˆz) − 1</li><li>L1 loss</li></ul></li><li>depth uncertainty$\sigma$</li><li>offset<ul><li>L1 loss</li></ul></li><li>offset uncertainty$\sigma$</li><li>dimension<ul><li>直接回归$[w,h,l]$</li><li>L1 loss</li></ul></li><li>rotation<ul><li>参考centernet的local orientation 而非相机坐标系下的global orientation</li><li>represent the orientation using eight scalars, where the orientation branch is trained by M ultiBin loss.</li></ul></li></ul></li><li>成对约束<ul><li>distance<ul><li>预测了两目标之间的3D坐标距离，此时还有两个目标预测得到的3D坐标。（这里如何使用的没有看明白）</li></ul></li><li>distance uncertainty$\sigma$</li></ul></li></ul><p>在2D检测中得到的keypoints中两两临近的点构成的空间约束，以两个kepoints之间的距离作为直径去画圆，当圆中有包含有其他object点时就直接忽略该约束。<br>本文另一方面的贡献在于uncertainty。</p><h1 id="2021—CVPR—FCOS3D"><a href="#2021—CVPR—FCOS3D" class="headerlink" title="2021—CVPR—FCOS3D"></a>2021—CVPR—FCOS3D</h1><h1 id="2021—MonoFlex"><a href="#2021—MonoFlex" class="headerlink" title="2021—MonoFlex"></a>2021—MonoFlex</h1><p>作者将3D框分为4个子任务进行求解</p><ol><li>对每一类检测目标，均给出先验信息$[h,w,l]$，通过网络输出偏差得到最终的结果</li></ol><p><img src="https://cdn.nlark.com/yuque/0/2022/png/478741/1668604297904-60334d82-88ee-427f-b246-7551aedaa97f.png#averageHue=%23f2f2f2&amp;clientId=u5249b83e-cebf-4&amp;from=drop&amp;height=110&amp;id=u467074be&amp;originHeight=220&amp;originWidth=986&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;size=33395&amp;status=done&amp;style=none&amp;taskId=uffcec2bf-b248-4244-8985-5ade4640326&amp;title=&amp;width=493" alt="截屏2022-11-16 21.11.31.png"></p><ol><li>估计深度信息z</li></ol><p>深度信息z的最终值由两部分组成，一部分是由网络直接预测得到的z,一部分是由相机小孔成像相似三角形得到的z，两者通过各自的概率信息，融合得到最终的z值。</p><ol><li>利用深度信息z与长方体的中心在二维图像上的像素坐标结合相机投影公式得出长方体的中心在相机坐标系下的坐标值</li><li>利用网络直接回归得到角度信息</li></ol><p>通过在 2D 图像上设定 10 个关键点: 3D 框的 8 个角点、顶部中心和底部中心，将 10 个关键点分为 3 组，即中心垂直线和两个对角垂直边界线 (h1-h3 和 h2-h4) 的平均，每组可以独立产生 2D 的高度 h，再结合网络预测得到的物体实际高度 H ，根据上述的关系式独立估计中心深度。最终结合 3 组估计的深度获得更好的深度预测结果。<br><img src="https://cdn.nlark.com/yuque/0/2022/png/478741/1668589646762-d980a382-3af7-4200-a000-6dea00da801f.png#averageHue=%23e0ddd5&amp;clientId=uef77c357-290e-4&amp;from=drop&amp;id=u9678ef0d&amp;originHeight=1132&amp;originWidth=2178&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;size=1742968&amp;status=done&amp;style=none&amp;taskId=u49f01383-30c2-4102-ba7b-6be60d4e3ca&amp;title=" alt="截屏2022-11-16 17.07.22.png"></p>]]></content>
      
      
      <categories>
          
          <category> 系统梳理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Multi-view 3D </tag>
            
            <tag> Mono3D </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>BEVDepth论文解读</title>
      <link href="/2022/07/17/paper-bevdepth-lun-wen-yu-dai-ma-jie-du/"/>
      <url>/2022/07/17/paper-bevdepth-lun-wen-yu-dai-ma-jie-du/</url>
      
        <content type="html"><![CDATA[<p>BEVDepth论文解读<br><span id="more"></span></p><h2 id="BEVDepth整体结构"><a href="#BEVDepth整体结构" class="headerlink" title="BEVDepth整体结构"></a>BEVDepth整体结构</h2><p>BEVDepth认为深度估计对于基于相机的3D目标检测来说至关重要，因此尝试利用显式深度监督来解决这个问题。<br>BEVDepth实际上是在LSS的投影方法的基础上增加准确的深度估计模块和显式的深度监督。它指出了LSS的方案存在的3个问题：</p><ol><li>不准确的深度</li><li>深度模块过拟合</li><li>不准确的BEV语义</li></ol><p>BEVDepth的网络结构如图1所示<br><img src="/images/bevdepth.png"/></p><center>图1 BEVDepth网络结构</center><h3 id="显式深度监督"><a href="#显式深度监督" class="headerlink" title="显式深度监督"></a>显式深度监督</h3><p>以往的模型对于深度的监督几乎都只来自检测的损失，BEVDepth认为由于单目深度估计的困难，唯一的检测损失远远不足以监督深度模块。因此BEVDepth将LiDar点云投影到图像上来进行显式的监督。<br>这部分的原理并不难，但是代码实现上包含很多多传感器同步的细节，很多相关的博文并没有对此进行详细的介绍。BEVDepth主要是针对nuScenes数据集设计的，nuScenes中传感器采用的是<strong>硬同步方法</strong>。硬同步方法可以缓解查找时间戳造成的误差现象。该方法可以以激光雷达作为触发其它传感器的源头，当激光雷达转到某个角度时，才触发该角度的摄像头，这可以大大减少时间差的问题。这套时间同步方案可以做到硬件中，这样可以大大降低同步误差，提高数据对齐效果。nuScenes提供每张图像对应的时间戳，这个时间戳是根据惯导的频率进行同步。因此深度监督的数据生成包括以下几个部分</p><ol><li>点云数据转到lidar时间戳对应的自车坐标系（实际上就是IMU坐标系）</li><li>点云数据从IMU坐标系转到global坐标系（nuScenes自己定义的一个global坐标系）</li><li>点云数据从global坐标系转到各相机时间戳对应的自车坐标系</li><li>点云数据从各相机时间戳对应的自车坐标系转到相机坐标系</li><li>结合图像上的裁剪操作，将点云数据从相机坐标系转到像素坐标系</li></ol><p>由此，过滤到多余的点之后，就可以得到大致的像素对应的深度。由于nuScenes的LiDar是128线，实际上投影得到的监督比较稀疏，因此降采样之后实际上是针对的$16\times 44$特征图进行监督的。</p><h3 id="Camera-aware-深度估计"><a href="#Camera-aware-深度估计" class="headerlink" title="Camera-aware 深度估计"></a>Camera-aware 深度估计</h3><p>BEVDepth认为将相机内参也作为输入信息，一同编码进行深度估计可以得到更好的深度估计结果。BEVDepth的DepthNet模块设计如下<br><img src="/images/depthnet.png" width="50%" height="50%"/><br>具体来说，先使用MLP讲相机内参的维度扩展到特征维度，然后使用类似SE模块的方式re-weight图像特征，最后把相机外参和相机内参concate起来辅助感知在自车坐标系下的位置。</p><h3 id="深度细化模块"><a href="#深度细化模块" class="headerlink" title="深度细化模块"></a>深度细化模块</h3><blockquote><p>We first reshape $F^{3d}$ from $[C_F , C_D , H, W ]$ to $[C_F \times H, C_D , W]$, and stack several $3×3$ convolution layer on the $C_D × W$ plane. Its output is finally reshaped back and fed into the subsequent Voxel/Pillar Pooling operation.</p></blockquote><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://arxiv.org/pdf/2206.10092.pdf">BEVDepth Paper</a><br><a href="https://github.com/Megvii-BaseDetection/BEVDepth">BEVDepth Code</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LSS </tag>
            
            <tag> Multi-view 3D </tag>
            
            <tag> BEV Perception </tag>
            
            <tag> AAAI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>BEVDet论文解读</title>
      <link href="/2022/07/08/paper-bevdet-lun-wen-yu-dai-ma-jie-du/"/>
      <url>/2022/07/08/paper-bevdet-lun-wen-yu-dai-ma-jie-du/</url>
      
        <content type="html"><![CDATA[<p>BEVDet论文解读<br><span id="more"></span></p><h2 id="BEVDet整体结构"><a href="#BEVDet整体结构" class="headerlink" title="BEVDet整体结构"></a>BEVDet整体结构</h2><p>BEVDet是一个比较经典的在BEV视角下进行3D目标检测的范式，这种表示方式能够给下游的路径规划等任务带来极大的便利。BEVDet主要是重用现有的模块来构建其框架，但通过构建独占数据增强策略和升级非最大抑制策略，大大提高了其性能。此外，BEVDet的官方仓库跟进了很多包括深度估计、occupancy prediction在内的拓展版本，是初学者比较好的参考。<br>BEVDet的整体结构如图1所示<br><img src="/images/bevdet.png"/></p><center>图1 BEVDet网络结构</center><p>BEVDet由编码图像特征的image-view encoder，将特征从image-view转到BEV的view transformer，在BEV视角进一步编码特征的BEV encoder以及在BEV空间进行3D目标检测的特定任务头组成。遵循这一套范式，可以轻松的更换更适配自己应用场景的模块。</p><h3 id="image-view-encoder"><a href="#image-view-encoder" class="headerlink" title="image-view encoder"></a>image-view encoder</h3><blockquote><p>To exploit the power of multi-resolution features, the imageview encoder includes a backbone for high-level feature extraction and a neck for multi-resolution feature fusion.</p></blockquote><p>这个部分就是把输入的原始图像编码成特征，通常直接遵循2D检测的pipeline。为权衡精度和速度，常见的模块为ResNet/EfficientNet + FPN。</p><h3 id="view-transformer"><a href="#view-transformer" class="headerlink" title="view transformer"></a>view transformer</h3><p>这个部分直接使用LSS的视角转换方法，具体可参考<a href="https://massive11.github.io/2022/03/18/principle-fiery-yuan-li/">FIERY与LSS详解</a>。</p><h3 id="BEV-encoder"><a href="#BEV-encoder" class="headerlink" title="BEV encoder"></a>BEV encoder</h3><p>这个部分和image-view encoder比较类似，只是处理的特征变成了BEV视角的特征，仍然是用ResNet+FPN的方式。</p><h3 id="head"><a href="#head" class="headerlink" title="head"></a>head</h3><p>3D目标检测任务需要检测目标的位置、尺寸、朝向以及包括行人、车辆、障碍物等移动目标的速度。BEVDet的head直接使用的是CenterPoint的head（以下简称为CenterHead）的第一阶段。<br>CenterHead也是一个比较经典的3D目标检测头，原文中对其描述如下</p><blockquote><p>The first stage of CenterPoint predicts a class-specific heatmap, object size, a sub-voxel location refinement, rotation, and velocity. All outputs are dense predictions.</p></blockquote><p>它解耦了3D目标框的不同属性，具体来说，主要包括以下几个sub head：</p><ol><li>Center heatmap head<ul><li>由于中心点真值通常比较稀疏，真值的制作实际上对每个中心点渲染了高斯峰值以扩大面积</li></ul></li><li>Regression heads<ul><li>a sub-voxel location refinement</li><li>height-above-ground</li><li>3D size</li><li>yaw rotation angle<ul><li>使用的是sin和cos值</li></ul></li></ul></li></ol><p>在nusenes数据集中，目标的类别一共被分为了6个大类<br><pre class="line-numbers language-PYTHON" data-language="PYTHON"><code class="language-PYTHON">tasks&#x3D;[    dict(num_class&#x3D;1, class_names&#x3D;[&#39;car&#39;]),    dict(num_class&#x3D;2, class_names&#x3D;[&#39;truck&#39;, &#39;construction_vehicle&#39;]),    dict(num_class&#x3D;2, class_names&#x3D;[&#39;bus&#39;, &#39;trailer&#39;]),    dict(num_class&#x3D;1, class_names&#x3D;[&#39;barrier&#39;]),    dict(num_class&#x3D;2, class_names&#x3D;[&#39;motorcycle&#39;, &#39;bicycle&#39;]),    dict(num_class&#x3D;2, class_names&#x3D;[&#39;pedestrian&#39;, &#39;traffic_cone&#39;]),]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></p><p>网络给每一个大类都分配了一个head，装在headlist中，而每个head内部都为预测的参数，包括（center_x,y,z、dim、rot、vel ）分配了一个MLP，从而完成检测头的设计。所有的输出都使用L1 loss进行监督。（回归到对数大小了）</p><h2 id="BEVDet的数据增强策略"><a href="#BEVDet的数据增强策略" class="headerlink" title="BEVDet的数据增强策略"></a>BEVDet的数据增强策略</h2><p>BEVDet认为，训练阶段出现的过拟合问题可能是限制性能的主要因素。因此，BEVDet提出了数据增强策略来应对过拟合的问题。<br>应用于图像视图空间的增强策略不会改变 BEV 空间中特征的空间分布。但是过拟合的问题主要出现在BEV特征的学习方面。可以通过对BEV特征进行缩放、翻转、旋转等操作，这些操作在视图转换器的输出特征和3D对象检测目标上都需要进行，以保持它们的空间一致性。</p><h2 id="BEVDet的改进NMS策略"><a href="#BEVDet的改进NMS策略" class="headerlink" title="BEVDet的改进NMS策略"></a>BEVDet的改进NMS策略</h2><blockquote><p>. Scale-NMS scales the size of each object according to its category before performing the classical NMS algorithm.</p></blockquote><p>这个部分提出了Scale-NMS。主要是针对3D目标检测中，不同类别的目标本身的尺寸不一致。比如行人，预测得到的TP可能和真值框的IoU为0。因此，BEVDet对不同类别的目标进行不同尺度的缩放，来做更符合客观场景的目标框过滤。<br><img src="/images/Scale-NMS.png"/></p><h2 id="BEVPoolv2"><a href="#BEVPoolv2" class="headerlink" title="BEVPoolv2"></a>BEVPoolv2</h2><p>基于LSS的方案在实际部署上还存在着很多问题，如推理速度慢和显存占用多。虽然BEVFusion已经提出过该方案的加速思路，但是收益仍然十分有限。BEVDet在其v2版本中提出了BEVPoolv2的思路，如下图所示<br><img src="/images/bevpoolv2.png"/></p><p>BEVDet的作者指出Lift的步骤需要计算、存储和预处理多个超大的视锥特征，因此针对这一点进行了优化。采用的方法就是offline的方式预计算depth_score和frustum的index，然后只在线进行多线程的相乘和累积，从而显著提高了FPS。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://arxiv.org/pdf/2112.11790.pdf">BEVDet Paper</a><br><a href="https://github.com/HuangJunJie2017/BEVDet">BEVDet Code</a><br><a href="https://arxiv.org/pdf/2006.11275.pdf">CenterPoint Paper</a><br><a href="https://arxiv.org/pdf/2211.17111.pdf">BEVPoolv2 Paper</a><br><a href="https://zhuanlan.zhihu.com/p/586637783">https://zhuanlan.zhihu.com/p/586637783</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LSS </tag>
            
            <tag> Multi-view 3D </tag>
            
            <tag> BEV Perception </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Deformable DETR 与 Deformable Attention</title>
      <link href="/2022/06/03/principle-deformable-attention/"/>
      <url>/2022/06/03/principle-deformable-attention/</url>
      
        <content type="html"><![CDATA[<p>Deformable DETR介绍 与 Deformable Attention原理<br><span id="more"></span></p><h2 id="Deformable-DETR"><a href="#Deformable-DETR" class="headerlink" title="Deformable DETR"></a>Deformable DETR</h2><p>Deformable DETR是DETR的改进版，主要是针对DETR收敛时间长、对小目标检测性能差两个缺陷进行改进。主要思路是引入可变注意力模块，整体结构如下图所示</p><p><img src="/images/Deformable_DETR.png" width="80%" height="80%"/></p><h2 id="Deformable-Attention"><a href="#Deformable-Attention" class="headerlink" title="Deformable Attention"></a>Deformable Attention</h2><p>其思路主要来源于可变形卷积DCN，其对比如下图所示</p><p><img src="/images/Deformable_attention.png" width="60%" height="60%"/></p><p>上图展示了四种网络的特点：</p><ol><li>ViT 中所有 Q 的感受野是一样的，都针对全局所有位置特征</li><li>Swin 中是局部 Attention，因此处于不同窗口的两个 Q 针对的感受野区域是不一样的</li><li>DCN 是针对周围九个位置学习偏差，之后采样矫正过的特征位置，可以看到图中红点蓝点数量均为 9；</li><li>DAT 则结合了 ViT 和 DCN，所有的 Q 会共享相同的感受野，但这些感受野会有学出来的位置偏差；为了降低计算复杂度，针对的特征数量也会降采样，因此图中采样点一共 16 个，相比原来缩小了 1/4。</li></ol><p>总体结构就如下图所示，Q 保持不变，K / V 是经过位置偏差后的采样值</p><p><img src="/images/DAT_structure.png" width="80%" height="80%"/></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://arxiv.org/pdf/2201.00520.pdf">Deformable Attention Paper</a><br><a href="https://arxiv.org/pdf/2010.04159.pdf">Deformable DETR Paper</a><br><a href="https://zhuanlan.zhihu.com/p/454115736">https://zhuanlan.zhihu.com/p/454115736</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Multi-view 3D </tag>
            
            <tag> Attention </tag>
            
            <tag> DETR </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DETR 与 DETR3D</title>
      <link href="/2022/05/28/principle-detr-yuan-li/"/>
      <url>/2022/05/28/principle-detr-yuan-li/</url>
      
        <content type="html"><![CDATA[<p>DETR原理与DETR3D原理<br><span id="more"></span><br>前情提要：之前的<a href="https://massive11.github.io/2022/01/25/Principle-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/">文章</a>中，已经对注意力机制和原理Transformer的整体结构进行了介绍。本文将在其基础上介绍DETR原理和DETR3D原理。</p><h2 id="DETR-ECCV-2020"><a href="#DETR-ECCV-2020" class="headerlink" title="DETR (ECCV 2020)"></a>DETR (ECCV 2020)</h2><p>DETR总体思路是把目标检测看成一个set prediction的问题，使用Transformer来预测box的set。DETR 利用标准 Transformer 架构来执行传统上特定于目标检测的操作，从而简化了检测 pipeline，可以归结为：Backbone -&gt; Transformer -&gt; detect header。其结构图如下图所示</p><p><img src="/images/DETR.png"/></p><p>观察上图可以发现，DETR的整体结构与Transformer类似：Backbone得到的特征铺平，加上Position信息之后送到一堆Encoder里，得到一些candidates的特征。Candidates又被Decoder并行解码得到最后的检测框。</p><h3 id="DETR-Encoder"><a href="#DETR-Encoder" class="headerlink" title="DETR Encoder"></a>DETR Encoder</h3><p>网络一开始是使用Backbone（比如ResNet）提取一些feature，然后降维到d×HW。</p><p>Feature降维之后与Spatial Positional Encoding相加，然后被送到Encoder里。</p><p>为了体现图像在x和y维度上的信息，作者的代码里分别计算了两个维度的Positional Encoding，然后Cat到一起。</p><pre class="line-numbers language-Python" data-language="Python"><code class="language-Python">pos_x &#x3D; torch.stack((pos_x[:, :, :, 0::2].sin(), pos_x[:, :, :, 1::2].cos()), dim&#x3D;4).flatten(3)pos_y &#x3D; torch.stack((pos_y[:, :, :, 0::2].sin(), pos_y[:, :, :, 1::2].cos()), dim&#x3D;4).flatten(3)pos &#x3D; torch.cat((pos_y, pos_x), dim&#x3D;3).permute(0, 3, 1, 2)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>FFN、LN等操作也与Transformer类似。Encoder最后得到的结果是对N个物体编码后的特征。</p><h3 id="DETR-Decoder"><a href="#DETR-Decoder" class="headerlink" title="DETR Decoder"></a>DETR Decoder</h3><p>DETR Decoder的结构也与Transformer类似，区别在于Decoder并行解码N个object。每个Decoder有两个输入：一路是Object Query（或者是上一个Decoder的输出），另一路是Encoder的结果。Object Query是一组nn.Embedding的weight（就是一组学到的参数）。</p><p>与Transformer不同，DETR的Decoder也加了Positional Encoding。最后一个Decoder后面接了两个FFN，分别预测检测框及其类别。</p><h3 id="Bipartite-Matching"><a href="#Bipartite-Matching" class="headerlink" title="Bipartite Matching"></a>Bipartite Matching</h3><p>由于输出物体的顺序不一定与ground truth的序列相同，作者使用二元匹配将GT框与预测框进行匹配。这里可以参考之前写的匈牙利匹配的<a href="https://massive11.github.io/2022/05/27/Principle-%E5%8C%88%E7%89%99%E5%88%A9%E5%8C%B9%E9%85%8D/">文章</a>。</p><h3 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h3><p>目标框的L1 loss和IoU loss。</p><h2 id="DETR3D-CoRL-2021"><a href="#DETR3D-CoRL-2021" class="headerlink" title="DETR3D (CoRL 2021)"></a>DETR3D (CoRL 2021)</h2><p>DETR3D是DETR范式在多目3D视觉任务上的拓展版本。对于多目3D视觉任务，就不得不提到当前仍占据主流的BEV感知方案。</p><h3 id="BEV-Perception"><a href="#BEV-Perception" class="headerlink" title="BEV Perception"></a>BEV Perception</h3><p>自动驾驶是3D目标检测重要的落地场景之一，通过纯视觉的方法，在2D图像上预测车辆、行人等3维BBox信息天然存在诸多困难，而2D的目标检测方法相对成熟，因此更多的3D目标检测方法更加关注如何从现有的2D检测框架迁移到3D任务中。从Tesla公布新的俯瞰视角(Bird’s-eys-view, BEV)目标检测的方法开始，越来越多的工作从俯视角出发做Detection, Segmentation。在BEV视角下做3D Detection，其好处如下：</p><ol><li>基于视觉的检测属于前向视角，在成像的过程中天然存在近大远小的特点。例如车道线，在成像的图片上是一条相交的直线，而在俯视角下直接预测为直线更接近现实场景；同时在BEV视角做检测能更好避免截断问题</li><li>按照2D检测的pipeline，将2D中检测好的物体反投影回3D空间，需要预测深度、航向角等，引入了更多的误差</li><li>LiDAR的点云数据天然支持BEV视角的检测、识别，更方便融合多模态的数据，以及与下游任务Prediction和Planning结合。</li></ol><h3 id="DETR3D结构"><a href="#DETR3D结构" class="headerlink" title="DETR3D结构"></a>DETR3D结构</h3><p>DETR3D将DETR中基于Transformer的2D检测框架引入到了3D检测任务中：一次性生成N个bbox，采用set-to-set损失函数计算预测和GT的二分图匹配损失。这种方式避免了常规3D检测任务中所需的深度估计模块，因此无需集中算力进行冗余信息的处理，而只关注在目标的特征之上，速度得到了较大的提升，也避免了重建带来的误差。此外，DETR3D也无需NMS等后处理操作。</p><p><img src="/images/DETR3D.png"/></p><p>整个网络可大致分为四个部分：</p><ol><li><p>Encoder<br>输入车载环视的6张图片,每张图片通过ResNet等2D骨干网络提取特征；再通过FPN得到4个不同尺度特征图 </p></li><li><p>Decoder<br>Decoder的输入是Encoder输出的特征图，在特征层面实现2D到3D的转换，避免深度估计带来的误差，同时这种set prediction的方式可以避免NMS等耗时的后处理操作。具体实现步骤如下</p><ul><li><p>object queries的生成类似DETR，先随机生成$M$个bounding box(类似先生成一堆anchor box，只不过这里的box是会被最后的loss梯度回传的)。</p><pre class="line-numbers language-Python" data-language="Python"><code class="language-Python">self.query_embedding &#x3D; nn.Embedding(self.num_query, self.embed_dims * 2)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p>如图中蓝线所示，使用子网络预测query在三维空间中的一个参考点（通常是简单的linear）。</p><pre class="line-numbers language-Python" data-language="Python"><code class="language-Python">self.reference_points &#x3D; nn.Linear(self.embed_dims, 3)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p>如图中绿线所示，利用相机内外参，将这个参考点反投影回图像中，找到其在原始图像中对应的位置。</p></li><li>如图中黄线所示，将投影后的Projection point 对应到FPN中的每一个尺度的feature map上。由于投影点经过下采样后在不同尺度的特征图上很可能没有刚好对应的特征点，因此采用双线性插值的方法来获取得到在每个尺度上的特征。然后将不同尺度上和不同位置相机上的提取到的特征进行求和平均处理</li><li>如图中红线所示，利用多头注意力机制，将找出的特征映射部分对queries进行refine。这种refine过程是逐层进行的，理论上，更靠后的layer应该会吸纳更多的特征信息。</li><li>如图中黑线所示，得到新的queries之后，再通过两个子网络（Linear+ReLU）分别预测bounding box和类别。</li></ul></li></ol><h3 id="set-to-set-loss"><a href="#set-to-set-loss" class="headerlink" title="set-to-set loss"></a>set-to-set loss</h3><ol><li><p>二分匹配<br>将真值set与预测值set进行匹配，利用匈牙利匹配实现一对一的对应。在匹配过程中，当在两个集合元素数量不相同时，通常使用将数量较少的一个集合补一堆空元素的方法来让集合元素数量相等。<br>在匹配的过程中，一个关键的步骤是衡量两个集合不同元素之间的相似度，论文中使用的是bounding box之间的相似度。</p><pre class="line-numbers language-Python" data-language="Python"><code class="language-Python">bbox_cost &#x3D; torch,cdist(bbox_pred, gt_bboxes, p&#x3D;1)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p>损失函数<br> 损失函数部分保持和DETR一致，Decoder的每一层输出都计算loss。回归损失采用L1，分类损失使用focal loss。</p></li></ol><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://arxiv.org/pdf/2005.12872.pdf">DETR Paper</a><br><a href="https://arxiv.org/pdf/2110.06922.pdf">DETR3D Paper</a><br><a href="https://github.com/wangyueft/detr3d">DETR3D Code</a><br><a href="https://zhuanlan.zhihu.com/p/146065711">https://zhuanlan.zhihu.com/p/146065711</a><br><a href="https://zhuanlan.zhihu.com/p/587380480">https://zhuanlan.zhihu.com/p/587380480</a><br><a href="https://zhuanlan.zhihu.com/p/430198800">https://zhuanlan.zhihu.com/p/430198800</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Multi-view 3D </tag>
            
            <tag> BEV Perception </tag>
            
            <tag> Attention </tag>
            
            <tag> DETR </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>匈牙利匹配</title>
      <link href="/2022/05/27/principle-xiong-ya-li-pi-pei/"/>
      <url>/2022/05/27/principle-xiong-ya-li-pi-pei/</url>
      
        <content type="html"><![CDATA[<p>匈牙利匹配原理和实现<br><span id="more"></span></p><h1 id="匈牙利匹配概述"><a href="#匈牙利匹配概述" class="headerlink" title="匈牙利匹配概述"></a>匈牙利匹配概述</h1><p>分配问题是组合优化领域中的一个基本问题，其目标是确定最佳分配，如使总成本最小化或使团队效率最大化。匈牙利匹配是求解二分图最大匹配的经典算法，算法的核心是根据一个初始匹配不停的找增广路径，直到没有增广路径为止。</p><h2 id="二分图"><a href="#二分图" class="headerlink" title="二分图"></a>二分图</h2><p>图中所有顶点可以划分两个子集，任取一条边，边的两个顶点不在同一个子集中。</p><h2 id="最大匹配"><a href="#最大匹配" class="headerlink" title="最大匹配"></a>最大匹配</h2><p>一个图的所有匹配中，所含匹配边数最多的匹配，称为这个图的最大匹配。最大匹配数，就是最大匹配时边的数目。</p><h2 id="交替路径"><a href="#交替路径" class="headerlink" title="交替路径"></a>交替路径</h2><p>给定图G的一个匹配M，如果一条路径的边交替出现在M中和不出现在M中，我们称之为一条M-交错路径。</p><h2 id="增广路径"><a href="#增广路径" class="headerlink" title="增广路径"></a>增广路径</h2><p>从一个未匹配点出发，走交替路，如果途径另一个未匹配点（出发的点不算），则这条交替路径称为增广路径。<br>增广路径的特点：路径长度为奇数（因为起点和终点必须分属两个集合）</p><h1 id="匈牙利匹配原理"><a href="#匈牙利匹配原理" class="headerlink" title="匈牙利匹配原理"></a>匈牙利匹配原理</h1><p>通过寻找增广路径，把增广路径中的匹配边和非匹配边的相互交换，这样就会多出一条匹配边，直到找不到增广路径为止。</p><h1 id="匈牙利匹配实现"><a href="#匈牙利匹配实现" class="headerlink" title="匈牙利匹配实现"></a>匈牙利匹配实现</h1><h2 id="实现思路"><a href="#实现思路" class="headerlink" title="实现思路"></a>实现思路</h2><p>步骤1：减去行最小值<br>找到邻接矩阵中每一行的最小元素，并且将每一行中的元素都减去该行中的最小元素。<br>步骤2：减去列最小值<br>在剩下的矩阵中找到每一列的最小元素，并且将每一列中的元素都减去该列中的最小元素。<br>步骤3：用最少的横线/竖线覆盖所有零<br>使用最少数量的水平和垂直线覆盖结果矩阵中的所有零。如果需要n行，则零之间存在最佳分配。执行步骤5。<br>如果少于n行，继续执行步骤4。<br>步骤4：生成额外的零<br>在没有被覆盖的元素中找到最小值，并且将没有被覆盖的元素减去这个最小值，同时将不同线条交叉位置上的元素加上这个最小值。回到步骤3。<br>步骤5：得到结果<br>最后从剩余的矩阵元素中找到最合适的零元素作为匹配结果。这里每个零元素对应筛选后的两个节点之间的边。</p><h2 id="Python实现"><a href="#Python实现" class="headerlink" title="Python实现"></a>Python实现</h2><p>scipy提供了linear_sum_assignment函数可以直接调用求解匈牙利匹配问题。<br><pre class="line-numbers language-Python" data-language="Python"><code class="language-Python">cost &#x3D; np.array([[4, 1, 3], [2, 0, 5], [3, 2, 2]])from scipy.optimize import linear_sum_assignmentrow_ind, col_ind &#x3D; linear_sum_assignment(cost)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>这里有一个比较清楚的图示讲解：<a href="https://zhuanlan.zhihu.com/p/459758723">https://zhuanlan.zhihu.com/p/459758723</a></p>]]></content>
      
      
      <categories>
          
          <category> 原理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Algorithm </tag>
            
            <tag> Match </tag>
            
            <tag> Graph Theory </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>FIERY 后处理原理与代码解读</title>
      <link href="/2022/03/18/code-fiery-hou-chu-li-quan-zhi-quan-hui/"/>
      <url>/2022/03/18/code-fiery-hou-chu-li-quan-zhi-quan-hui/</url>
      
        <content type="html"><![CDATA[<p>FIERY 后处理原理与代码解读<br><span id="more"></span><br>本文主要是对FIERY原本Python版本的后处理代码进行介绍。此外，可以在<a href="https://github.com/massive11/fiery-post-process">FIERY 后处理</a>仓库查看C++版本的开源代码，仅供学习交流使用。</p><p>FIERY的后处理部分主要是将decoder输出的各个head的结果组合起来，通过匹配操作得到每个实例的轨迹信息和预测结果。具体来说，先分别计算每一帧中的实例分割结果和实例中心点。然后，根据多帧的结果进行帧间的匹配工作，保持同一个instance在不同帧的id保持一致。最后得到同一个实例的连续运动信息和轨迹预测结果。整个序列一共是5帧，包括2帧过去，1帧当前，2帧未来的预测。</p><h2 id="前景mask"><a href="#前景mask" class="headerlink" title="前景mask"></a>前景mask</h2><p>根据<em>segmentation</em>计算前景mask<br><pre class="line-numbers language-PYTHON" data-language="PYTHON"><code class="language-PYTHON">preds &#x3D; output[&#39;segmentation&#39;].detach()  # 1*5*2*200*200preds &#x3D; torch.argmax(preds, dim&#x3D;2, keepdims&#x3D;True)foreground_masks &#x3D; preds.squeeze(2) &#x3D;&#x3D; vehicles_id  # 1*5*200*200<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></p><h2 id="实例分割和实例中心点"><a href="#实例分割和实例中心点" class="headerlink" title="实例分割和实例中心点"></a>实例分割和实例中心点</h2><p>根据<em>instance_center</em>和<em>instance_offset</em>计算实例分割结果和各实例中心点<br><pre class="line-numbers language-PYTHON" data-language="PYTHON"><code class="language-PYTHON">batch_size, seq_len &#x3D; preds.shape[:2]pred_inst &#x3D; []for b in range(batch_size):    pred_inst_batch &#x3D; []    for t in range(seq_len):  # 逐帧处理        pred_instance_t, _ &#x3D; get_instance_segmentation_and_centers(            output[&#39;instance_center&#39;][b, t].detach(),            output[&#39;instance_offset&#39;][b, t].detach(),            foreground_masks[b, t].detach()        )        pred_inst_batch.append(pred_instance_t)    pred_inst.append(torch.stack(pred_inst_batch, dim&#x3D;0))pred_inst &#x3D; torch.stack(pred_inst).squeeze(2)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></p><p>其中，<em>get_instance_segmentation_and_centers</em>函数的实现细节如下<br><pre class="line-numbers language-PYTHON" data-language="PYTHON"><code class="language-PYTHON">def get_instance_segmentation_and_centers(    center_predictions: torch.Tensor,    offset_predictions: torch.Tensor,    foreground_mask: torch.Tensor,    conf_threshold: float &#x3D; 0.1,    nms_kernel_size: float &#x3D; 3,    max_n_instance_centers: int &#x3D; 100,) -&gt; Tuple[torch.Tensor, torch.Tensor]:    width, height &#x3D; center_predictions.shape[-2:]    center_predictions &#x3D; center_predictions.view(1, width, height)    offset_predictions &#x3D; offset_predictions.view(2, width, height)    foreground_mask &#x3D; foreground_mask.view(1, width, height)    # find_instance_centers是使用最大池化实现的NMS，计算得到各个车辆的center坐标    centers &#x3D; find_instance_centers(center_predictions, conf_threshold&#x3D;conf_threshold, nms_kernel_size&#x3D;nms_kernel_size)    if not len(centers):        return torch.zeros(center_predictions.shape, dtype&#x3D;torch.int64, device&#x3D;center_predictions.device), \               torch.zeros((0, 2), device&#x3D;centers.device)    if len(centers) &gt; max_n_instance_centers:        print(f&#39;There are a lot of detected instance centers: &#123;centers.shape&#125;&#39;)        centers &#x3D; centers[:max_n_instance_centers].clone()    # offset_predictions显示的是指示实例中心的vector field    # 根据其与各个中心点之间的距离，计算得到200*200的网格点各自属于哪一个实例    instance_ids &#x3D; group_pixels(centers, offset_predictions)    # 通过之前计算过的前景mask进行过滤    instance_seg &#x3D; (instance_ids * foreground_mask.float()).long()    # 上一步操作可能导致id编号中断，这个步骤只是重新赋值了id    instance_seg &#x3D; make_instance_seg_consecutive(instance_seg)    return instance_seg.long(), centers<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></p><h2 id="保持多帧instance-id的一致性"><a href="#保持多帧instance-id的一致性" class="headerlink" title="保持多帧instance id的一致性"></a>保持多帧instance id的一致性</h2><p>上一步中，是针对每一帧单独处理的，同一个实例在不同帧的id不一定相同。这部分通过进行多帧之间的匹配，根据instance_flow提供的信息统一多帧的实例信息，从而得到每个实例的时序运动信息。</p><pre class="line-numbers language-PYTHON" data-language="PYTHON"><code class="language-PYTHON">consistent_instance_seg &#x3D; []for b in range(batch_size):    consistent_instance_seg.append(        # 根据instance_flow提供的信息统一多帧的实例id        make_instance_id_temporally_consistent(pred_inst[b:b+1],                                                output[&#39;instance_flow&#39;][b:b+1].detach())    )consistent_instance_seg &#x3D; torch.cat(consistent_instance_seg, dim&#x3D;0)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>其中，<em>make_instance_id_temporally_consistent</em>函数的实现细节<br><pre class="line-numbers language-PYTHON" data-language="PYTHON"><code class="language-PYTHON">def make_instance_id_temporally_consistent(pred_inst, future_flow, matching_threshold&#x3D;3.0):    &quot;&quot;&quot;    Parameters        pred_inst: torch.Tensor (1, seq_len, h, w)        future_flow: torch.Tensor(1, seq_len, 2, h, w)        matching_threshold: distance threshold for a match to be valid.    Returns    consistent_instance_seg: torch.Tensor(1, seq_len, h, w)    1. time t. Loop over all detected instances. Use flow to compute new centers at time t+1.    2. Store those centers    3. time t+1. Re-identify instances by comparing position of actual centers, and flow-warped centers.        Make the labels at t+1 consistent with the matching    4. Repeat    &quot;&quot;&quot;    assert pred_inst.shape[0] &#x3D;&#x3D; 1, &#39;Assumes batch size &#x3D; 1&#39;    # Initialise instance segmentations with prediction corresponding to the present    consistent_instance_seg &#x3D; [pred_inst[0, 0]]    largest_instance_id &#x3D; consistent_instance_seg[0].max().item()    _, seq_len, h, w &#x3D; pred_inst.shape    device &#x3D; pred_inst.device    for t in range(seq_len - 1):        # Compute predicted future instance means        grid &#x3D; torch.stack(torch.meshgrid(            torch.arange(h, dtype&#x3D;torch.float, device&#x3D;device), torch.arange(w, dtype&#x3D;torch.float, device&#x3D;device)        ))        # 预定义的网格点与预测的future flow相加        grid &#x3D; grid + future_flow[0, t]  # [2, 200, 200] 分别是两个轴的坐标        warped_centers &#x3D; []        # 获取背景之外的所有id号        t_instance_ids &#x3D; torch.unique(consistent_instance_seg[-1])[1:].cpu().numpy()        if len(t_instance_ids) &#x3D;&#x3D; 0:            # No instance so nothing to update            consistent_instance_seg.append(pred_inst[0, t + 1])            continue        for instance_id in t_instance_ids:            instance_mask &#x3D; (consistent_instance_seg[-1] &#x3D;&#x3D; instance_id)            # 使用instance mask的均值作为中心点坐标            warped_centers.append(grid[:, instance_mask].mean(dim&#x3D;1))        warped_centers &#x3D; torch.stack(warped_centers)        # Compute actual future instance means        centers &#x3D; []        grid &#x3D; torch.stack(torch.meshgrid(            torch.arange(h, dtype&#x3D;torch.float, device&#x3D;device), torch.arange(w, dtype&#x3D;torch.float, device&#x3D;device)        ))        n_instances &#x3D; int(pred_inst[0, t + 1].max().item())        if n_instances &#x3D;&#x3D; 0:            # No instance, so nothing to update.            consistent_instance_seg.append(pred_inst[0, t + 1])            continue        for instance_id in range(1, n_instances + 1):            instance_mask &#x3D; (pred_inst[0, t + 1] &#x3D;&#x3D; instance_id)            centers.append(grid[:, instance_mask].mean(dim&#x3D;1))        centers &#x3D; torch.stack(centers)        # Compute distance matrix between warped centers and actual centers        distances &#x3D; torch.norm(centers.unsqueeze(0) - warped_centers.unsqueeze(1), dim&#x3D;-1).cpu().numpy()        # outputs (row, col) with row: index in frame t, col: index in frame t+1        # the missing ids in col must be added (correspond to new instances)        # 匈牙利匹配，计算当前帧与下一帧之间的匹配关系        ids_t, ids_t_one &#x3D; linear_sum_assignment(distances)        matching_distances &#x3D; distances[ids_t, ids_t_one]        # Offset by one as id&#x3D;0 is the background        ids_t +&#x3D; 1        ids_t_one +&#x3D; 1        # swap ids_t with real ids. as those ids correspond to the position in the distance matrix.        id_mapping &#x3D; dict(zip(np.arange(1, len(t_instance_ids) + 1), t_instance_ids))        ids_t &#x3D; np.vectorize(id_mapping.__getitem__, otypes&#x3D;[np.int64])(ids_t)        # Filter low quality match        ids_t &#x3D; ids_t[matching_distances &lt; matching_threshold]        ids_t_one &#x3D; ids_t_one[matching_distances &lt; matching_threshold]        # Elements that are in t+1, but weren&#39;t matched        remaining_ids &#x3D; set(torch.unique(pred_inst[0, t + 1]).cpu().numpy()).difference(set(ids_t_one))        # remove background        remaining_ids.remove(0)        #  Set remaining_ids to a new unique id        for remaining_id in list(remaining_ids):            largest_instance_id +&#x3D; 1            ids_t &#x3D; np.append(ids_t, largest_instance_id)            ids_t_one &#x3D; np.append(ids_t_one, remaining_id)        consistent_instance_seg.append(update_instance_ids(pred_inst[0, t + 1], old_ids&#x3D;ids_t_one, new_ids&#x3D;ids_t))    consistent_instance_seg &#x3D; torch.stack(consistent_instance_seg).unsqueeze(0)    return consistent_instance_seg<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></p><h2 id="计算每个instance的轨迹"><a href="#计算每个instance的轨迹" class="headerlink" title="计算每个instance的轨迹"></a>计算每个instance的轨迹</h2><pre class="line-numbers language-PYTHON" data-language="PYTHON"><code class="language-PYTHON">assert batch_size &#x3D;&#x3D; 1# Generate trajectoriesmatched_centers &#x3D; &#123;&#125;_, seq_len, h, w &#x3D; consistent_instance_seg.shapegrid &#x3D; torch.stack(torch.meshgrid(    torch.arange(h, dtype&#x3D;torch.float, device&#x3D;preds.device),    torch.arange(w, dtype&#x3D;torch.float, device&#x3D;preds.device)))for instance_id in torch.unique(consistent_instance_seg[0, 0])[1:].cpu().numpy():    # 得到每个instance id在各帧的位置    for t in range(seq_len):        instance_mask &#x3D; consistent_instance_seg[0, t] &#x3D;&#x3D; instance_id        if instance_mask.sum() &gt; 0:            matched_centers[instance_id] &#x3D; matched_centers.get(instance_id, []) + [                grid[:, instance_mask].mean(dim&#x3D;-1)]for key, value in matched_centers.items():    matched_centers[key] &#x3D; torch.stack(value).cpu().numpy()[:, ::-1]return consistent_instance_seg, matched_centers<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>得到上述结果之后，进行简单的坐标系转换就可以得到车载坐标系下各个障碍物的位置和轨迹预测结果，也可以进一步计算得到运动速度、加速度等信息传递给下游模块。</p>]]></content>
      
      
      <categories>
          
          <category> Code </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LSS </tag>
            
            <tag> Prediction </tag>
            
            <tag> Multi-view 3D </tag>
            
            <tag> BEV Perception </tag>
            
            <tag> Temporal </tag>
            
            <tag> Post Processing </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>FIERY论文与代码解读、LSS原理</title>
      <link href="/2022/03/18/principle-fiery-yuan-li/"/>
      <url>/2022/03/18/principle-fiery-yuan-li/</url>
      
        <content type="html"><![CDATA[<p>FIERY论文与代码解读，LSS原理与代码解读<br><span id="more"></span></p><h2 id="FIERY整体结构"><a href="#FIERY整体结构" class="headerlink" title="FIERY整体结构"></a>FIERY整体结构</h2><p>FIERY是时序的实例分割和预测网络，在不依赖高清地图的情况下，仅从摄像头驱动数据中以端到端的方式学习对未来固有的随机性进行建模，并预测未来的多时间轨迹。效果可视化如下图所示<br><img src="/images/FIERY_visual.png"/></p><p>FIERY的整体结构如下图所示<br><img src="/images/FIERY.png"/></p><p>FIERY的整体结构可以分为6个部分。</p><ol><li>根据相机的内外参和估计的深度概率分布，将图像特征转为3D特征</li><li>将上述得到的各个视角的3D特征，投影到BEV空间，然后基于惯导提供的运动信息，将过去两帧的BEV特征与当前帧的BEV特征对齐</li><li>使用一个基于3D卷积的时序模块学习时空状态${s_t}$</li><li>参数化两个概率分布，分别为当前分布 和 未来分布。当前分布以当前的时空状态$s_t$为条件，未来分布则同时以当前的时空状态$s_t$和未来的标签${(y_{t+1}, …, y_{t+H})}$为条件</li><li>我们在训练过程中从未来分布中采样潜在编码$\eta_t$，而在推理期间从当前分布中采样。当前状态$s_t$和潜在编码$\eta_t$是未来预测模型的输入，它递归地预测未来的状态${(\hat{s}_{t+1}, …, \hat{s}_{t+H})}$。</li><li>这些状态被解码为BEV视角的未来实例分割和未来的运动</li></ol><p>经过一系列编解码，最终得到的结果如下图所示<br><img src="/images/fiery_out.png"/><br>各模块具体的实现原理将拆解成单独的几部分进行介绍。</p><h3 id="LSS-ECCV-2020"><a href="#LSS-ECCV-2020" class="headerlink" title="LSS (ECCV 2020)"></a>LSS (ECCV 2020)</h3><p>FIERY从2D特征到3D BEV特征的转换过程采用的是LSS的方式（以下具体介绍部分还是以FIERY的设定为例，避免混淆）。LSS是NVIDIA在2020年提出的方案，是自动驾驶多目相机感知领域的非常重要的论文。其最大的贡献在于：提供了一个端到端的训练方法解决多个传感器融合的问题。传统的多个传感器单独检测后再进行后处理的方法无法将此过程损失传进行反向传播而调整相机输入，而LSS则省去了这一阶段的后处理，直接输出融合结果。<br>LSS分别对应Lift, Splat, Shoot三个步骤。原文中对其解释如下</p><blockquote><p>The core idea behind our approach is to “lift” each image individually into a frustum of features for each camera, then “splat” all frustums into a rasterized bird’s-eyeview grid.<br>…<br>The representations inferred by our model enable interpretable end-to-end motion planning by “shooting” template trajectories into a bird’s-eyeview cost map output by our network.</p></blockquote><h4 id="Lift"><a href="#Lift" class="headerlink" title="Lift"></a>Lift</h4><p>该步骤是将每个图像单独“lift”为每个相机的特征截锥体，该过程的可视化如下图所示<br><img src="/images/lift.png"/></p><p>根据相机的成像原理，每个像素实际上是空间中的一条射线经过相机中心在像平面上的投影。因此，对于每个像素，在已知相机的内外参的情况下，我们只能确定这条射线，但不能确定这个像素具体反映的是射线上哪个点的信息，也就是常说的depth。因此，LSS预先设定了相机的视锥，深度方向的范围是${[d_{min}, d_{max}]}$。在FIERY的实现版本中，深度范围是${[2, 50)}$，每隔1米进行采样，因此可以得到48个离散的深度值。在这个范围内进行概率估计，然后将深度概率估计与图像特征进行内积，从而能够得到每个相机视锥体。该部分的Tensor变换如下图所示<br><img src="/images/fiery_st1.png"/><br>在实现方面，实际上就直接使用卷积层实现。softmax之后得到深度估计的概率分布，然后将其与特征做外积，具体代码如下，<br><pre class="line-numbers language-Python" data-language="Python"><code class="language-Python">class Encoder(nn.Module):    def __init__(self, cfg, D):        super().__init__()        self.D &#x3D; D        self.C &#x3D; cfg.OUT_CHANNELS        # ...    def forward(self, x):        x &#x3D; self.depth_layer(x)  # feature and depth head        if self.use_depth_distribution:            depth &#x3D; x[:, : self.D].softmax(dim&#x3D;1)            x &#x3D; depth.unsqueeze(1) * x[:, self.D : (self.D + self.C)].unsqueeze(2)  # outer product depth and features        else:            x &#x3D; x.unsqueeze(2).repeat(1, 1, self.D, 1, 1)        return x<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></p><h4 id="Splat"><a href="#Splat" class="headerlink" title="Splat"></a>Splat</h4><p>Splat步骤是将各个相机的视锥体投影到BEV视角。该部分的Tensor变换如下图所示<br><img src="/images/fiery_st2.png"/><br>要实现这个投影，就要得到相机视锥体到BEV空间的坐标转换关系。此处首先依赖相机的内外参生成一个Geometry，它用于存储每个相机视锥体的各个特征到整体3D空间（后续会降维到BEV空间）的对应关系。首先，初始化一个$D\times W \times H$ 的frustum。按照图像坐标系 -&gt; 归一化相机坐标系 -&gt; 相机坐标系 -&gt; 车身坐标系的坐标转换顺序计算得到对应坐标。该部分代码实现如下（其中涉及到的坐标转换部分，可以参考这篇<a href="https://massive11.github.io/2022/03/18/principle-zuo-biao-zhuan-huan-yuan-li-yu-nuscenes-zhong-de-zuo-biao-zhuan-huan-shi-zhan/">自动驾驶中的常见坐标转换与Nuscenes中的坐标转换实战</a>）<br><pre class="line-numbers language-Python" data-language="Python"><code class="language-Python">def get_geometry(self, intrinsics, extrinsics):    &quot;&quot;&quot;    Calculate the (x, y, z) 3D position of the features.    &quot;&quot;&quot;    rotation, translation &#x3D; extrinsics[..., :3, :3], extrinsics[..., :3, 3]    B, N, _ &#x3D; translation.shape    # Add batch, camera dimension, and a dummy dimension at the end    # self.frustum是预定义的相机视锥体的三维网格    points &#x3D; self.frustum.unsqueeze(0).unsqueeze(0).unsqueeze(-1)    # Camera to ego reference frame    # 反归一化    points &#x3D; torch.cat((points[:, :, :, :, :, :2] * points[:, :, :, :, :, 2:3], points[:, :, :, :, :, 2:3]), 5)    combined_transformation &#x3D; rotation.matmul(torch.inverse(intrinsics))    points &#x3D; combined_transformation.view(B, N, 1, 1, 1, 3, 3).matmul(points).squeeze(-1)    points +&#x3D; translation.view(B, N, 1, 1, 1, 3)    # The 3 dimensions in the ego reference frame are: (forward, sides, height)    return points<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></p><p>得到Geometry之后，就可以利用Voxel Pooling构建BEV特征。具体来说，将相机点云投影到预先划定范围的3D空间中。此时，可能会有多个视觉特征对应同一个BEV网格，直接将能投影到同一个BEV网格的特征累加起来，就可以得到降维后的BEV特征。该部分代码实现如下<br><pre class="line-numbers language-Python" data-language="Python"><code class="language-Python">def projection_to_birds_eye_view(self, x, geometry):    # batch, n_cameras, depth, height, width, channels    batch, n, d, h, w, c &#x3D; x.shape    output &#x3D; torch.zeros(        (batch, c, self.bev_dimension[0], self.bev_dimension[1]), dtype&#x3D;torch.float, device&#x3D;x.device    )    # Number of 3D points    N &#x3D; n * d * h * w    for b in range(batch):        # 展平视觉点云        x_b &#x3D; x[b].reshape(N, c)        # 计算栅格坐标并取整        geometry_b &#x3D; ((geometry[b] - (self.bev_start_position - self.bev_resolution &#x2F; 2.0)) &#x2F; self.bev_resolution)        # 展平体素坐标        geometry_b &#x3D; geometry_b.view(N, 3).long()        # Mask out points that are outside the considered spatial extent.        mask &#x3D; (                (geometry_b[:, 0] &gt;&#x3D; 0)                &amp; (geometry_b[:, 0] &lt; self.bev_dimension[0])                &amp; (geometry_b[:, 1] &gt;&#x3D; 0)                &amp; (geometry_b[:, 1] &lt; self.bev_dimension[1])                &amp; (geometry_b[:, 2] &gt;&#x3D; 0)                &amp; (geometry_b[:, 2] &lt; self.bev_dimension[2])        )        x_b &#x3D; x_b[mask]        geometry_b &#x3D; geometry_b[mask]        # Sort tensors so that those within the same voxel are consecutives.        ranks &#x3D; (                geometry_b[:, 0] * (self.bev_dimension[1] * self.bev_dimension[2])                + geometry_b[:, 1] * (self.bev_dimension[2])                + geometry_b[:, 2]        )        ranks_indices &#x3D; ranks.argsort()        x_b, geometry_b, ranks &#x3D; x_b[ranks_indices], geometry_b[ranks_indices], ranks[ranks_indices]        # Project to bird&#39;s-eye view by summing voxels.        x_b, geometry_b &#x3D; VoxelsSumming.apply(x_b, geometry_b, ranks)        bev_feature &#x3D; torch.zeros((self.bev_dimension[2], self.bev_dimension[0], self.bev_dimension[1], c),                                    device&#x3D;x_b.device)        bev_feature[geometry_b[:, 2], geometry_b[:, 0], geometry_b[:, 1]] &#x3D; x_b        # Put channel in second position and remove z dimension        bev_feature &#x3D; bev_feature.permute((0, 3, 1, 2))        bev_feature &#x3D; bev_feature.squeeze(0)        output[b] &#x3D; bev_feature    return output<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></p><h3 id="时序特征对齐"><a href="#时序特征对齐" class="headerlink" title="时序特征对齐"></a>时序特征对齐</h3><p>经过上一步，我们已经得到多帧的BEV特征。由于车辆的运动，每一帧的BEV特征可能都处于不同的位置，因此需要将过去时刻的BEV特征warp到当前时刻以统一坐标系，也就是所谓的特征对齐（feature alignment）。这个步骤基本就是简单的坐标系转换，借助惯导信息来完成。该部分的Tensor变换如下<br><img src="/images/fiery_st3.png" width="70%" height="70%"/><br>惯导提供的信息通常是相对于起始位置的三轴平移量和三轴旋转量。根据两帧的惯导数据，可以很容易的计算出两帧之间的相对平移量和旋转量，即$[tx, ty, tz, rx, ry, rz]$。然后，基于这个运动信息，实现feature warp（由于涉及到连续多帧的变换，会先将这个6 DoF转成坐标转换矩阵的形式，然后再转回6 DoF的形式）。先进行Z轴上的旋转，然后进行X轴、Y轴的平移，最后使用仿射变换实现X轴、Y轴的旋转。</p><pre class="line-numbers language-Python" data-language="Python"><code class="language-Python">def warp_features(x, flow, mode&#x3D;&#39;nearest&#39;, spatial_extent&#x3D;None):    &quot;&quot;&quot; Applies a rotation and translation to feature map x.        Args:            x: (b, c, h, w) feature map            flow: (b, 6) 6DoF vector (only uses the xy poriton)            mode: use &#39;nearest&#39; when dealing with categorical inputs        Returns:            in plane transformed feature map        &quot;&quot;&quot;    if flow is None:        return x    b, c, h, w &#x3D; x.shape    # z-rotation    angle &#x3D; flow[:, 5].clone()  # torch.atan2(flow[:, 1, 0], flow[:, 0, 0])    # x-y translation    translation &#x3D; flow[:, :2].clone()  # flow[:, :2, 3]    # Normalise translation. Need to divide by how many meters is half of the image.    # because translation of 1.0 correspond to translation of half of the image.    translation[:, 0] &#x2F;&#x3D; spatial_extent[0]    translation[:, 1] &#x2F;&#x3D; spatial_extent[1]    # forward axis is inverted    translation[:, 0] *&#x3D; -1    cos_theta &#x3D; torch.cos(angle)    sin_theta &#x3D; torch.sin(angle)    # output &#x3D; Rot.input + translation    # tx and ty are inverted as is the case when going from real coordinates to numpy coordinates    # translation_pos_0 -&gt; positive value makes the image move to the left    # translation_pos_1 -&gt; positive value makes the image move to the top    # Angle -&gt; positive value in rad makes the image move in the trigonometric way    transformation &#x3D; torch.stack([cos_theta, -sin_theta, translation[:, 1],                                  sin_theta, cos_theta, translation[:, 0]], dim&#x3D;-1).view(b, 2, 3)    # Note that a rotation will preserve distances only if height &#x3D; width. Otherwise there&#39;s    # resizing going on. e.g. rotation of pi&#x2F;2 of a 100x200 image will make what&#39;s in the center of the image    grid &#x3D; torch.nn.functional.affine_grid(transformation, size&#x3D;x.shape, align_corners&#x3D;False)    warped_x &#x3D; torch.nn.functional.grid_sample(x, grid.float(), mode&#x3D;mode, padding_mode&#x3D;&#39;zeros&#39;, align_corners&#x3D;False)    return warped_xdef cumulative_warp_features(x, flow, mode&#x3D;&#39;nearest&#39;, spatial_extent&#x3D;None):    &quot;&quot;&quot; Warps a sequence of feature maps by accumulating incremental 2d flow.    x[:, -1] remains unchanged    x[:, -2] is warped using flow[:, -2]    x[:, -3] is warped using flow[:, -3] @ flow[:, -2]    ...    x[:, 0] is warped using flow[:, 0] @ ... @ flow[:, -3] @ flow[:, -2]    Args:        x: (b, t, c, h, w) sequence of feature maps        flow: (b, t, 6) sequence of 6 DoF pose            from t to t+1 (only uses the xy poriton)    &quot;&quot;&quot;    sequence_length &#x3D; x.shape[1]    if sequence_length &#x3D;&#x3D; 1:        return x    # Convert 6DoF parameters to transformation matrix.    flow &#x3D; pose_vec2mat(flow)    out &#x3D; [x[:, -1]]    cum_flow &#x3D; flow[:, -2]    for t in reversed(range(sequence_length - 1)):        out.append(warp_features(x[:, t], mat2pose_vec(cum_flow), mode&#x3D;mode, spatial_extent&#x3D;spatial_extent))        # @ is the equivalent of torch.bmm        cum_flow &#x3D; flow[:, t - 1] @ cum_flow    return torch.stack(out[::-1], 1)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="时序模块"><a href="#时序模块" class="headerlink" title="时序模块"></a>时序模块</h3><p>在送入时序模块之前，FIERY将过去时刻的运动信息拓展维度之后与上一步中得到的对齐之后的BEV特征concate起来，作为时序模块的输入。时序模块实际上是一个带有局部时空卷积和全局3D池化的3D卷积网络。该模块的Tensor变换如下图所示<br><img src="/images/fiery_st4.png" width="60%" height="60%"/></p><p>这部分的代码就不做展开了，主要就是时序block和bottleneck3D block的叠加。</p><h3 id="当前-未来分布估计模块"><a href="#当前-未来分布估计模块" class="headerlink" title="当前/未来分布估计模块"></a>当前/未来分布估计模块</h3><p>这歌步骤是采用条件变分方法对未来预测的固有随机性进行建模。当前分布和未来分布都被参数化为对角高斯（diagonal Gaussians）。然后使用一个KL散度的loss来监督分布的学习。这一部分的Tensor变换如下图所示<br><img src="/images/fiery_st5.png"/></p><p>具体的代码实现如下<br><pre class="line-numbers language-Python" data-language="Python"><code class="language-Python">if self.n_future &gt; 0:    present_state &#x3D; states[:, :1].contiguous()    if self.cfg.PROBABILISTIC.ENABLED:        # Do probabilistic computation        sample, output_distribution &#x3D; self.distribution_forward(            present_state, future_distribution_inputs, noise        )        output &#x3D; &#123;**output, **output_distribution&#125;    # Prepare future prediction input    b, _, _, h, w &#x3D; present_state.shape    hidden_state &#x3D; present_state[:, 0]    if self.cfg.PROBABILISTIC.ENABLED:        future_prediction_input &#x3D; sample.expand(-1, self.n_future, -1, -1, -1)    else:        future_prediction_input &#x3D; hidden_state.new_zeros(b, self.n_future, self.latent_dim, h, w)    # Recursively predict future states    future_states &#x3D; self.future_prediction(future_prediction_input, hidden_state)    # Concatenate present state    future_states &#x3D; torch.cat([present_state, future_states], dim&#x3D;1)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></p><h3 id="实例分割和运动预测"><a href="#实例分割和运动预测" class="headerlink" title="实例分割和运动预测"></a>实例分割和运动预测</h3><p>这部分是模型的Decoder部分，包括语义分割、实例中心点、实例偏移和未来实例流在内的多个head。该部分的Tensor变换如下图<br><img src="/images/fiery_st6.png" width="60%" height="60%"/></p><p>这些head都是由两层卷积得到结果，具体代码也不做展开。多个输出结果融合得到最后的轨迹可视化，这些部分都在后处理中进行。本博客在<a href="https://massive11.github.io/2022/03/18/code-fiery-hou-chu-li-quan-zhi-quan-hui/">FIERY后处理原理与代码解读</a> 中对FIERY的后处理部分进行了详细的讲解，并开源了Fiery后处理的代码（C++版），有兴趣的读者可以参考<a href="https://github.com/massive11/fiery-post-process">GitHub仓库</a>。</p><h2 id="FIERY-Loss"><a href="#FIERY-Loss" class="headerlink" title="FIERY Loss"></a>FIERY Loss</h2><h3 id="distribution"><a href="#distribution" class="headerlink" title="distribution"></a>distribution</h3><blockquote><p>During training, we use samples $\eta_t \sim N(\mu_{t, future}, \sigma^2_{t, future})$ from the future distribution to enforce predictions consistent with the observed future, and a mode covering Kullback-Leibler divergence loss to encourage the present distribution to cover the observed futures:</p><script type="math/tex; mode=display">L_{probabilistic} = D_{KL}(F(\cdot|s_t, y_{t+1}, ..., y_{t+H})||P(\cdot|s_t))</script></blockquote><p>对于分布估计，使用的是KL散度进行监督。KL散度可以用来衡量两个概率分布之间的相似性，两个概率分布越相近，KL散度越小。论文中将present distribution和future distribution都参数化为均值为$\mu$，方差为$\sigma^2$的对角高斯。在loss中，KL散度的定义具体表示为</p><script type="math/tex; mode=display">var\_future = e^{2*future\_log\_sigma}</script><script type="math/tex; mode=display">var\_present = e^{2*present\_log\_sigma}</script><script type="math/tex; mode=display">kl\_div = \frac{present\_log\_sigma-future\_log\_sigma-0.5+(var\_future+(future\_mu-present\_mu)^2)}{2 * var\_present}</script><p>KL散度损失的具体定义可以参考<a href="https://massive11.github.io/2023/04/08/principle-bev-mo-xing-chang-yong-loss-zong-jie/">BEV模型常用LOSS总结</a></p><h3 id="segmentation"><a href="#segmentation" class="headerlink" title="segmentation"></a>segmentation</h3><blockquote><p>For semantic segmentation, we use a top-k cross-entropy loss. As the bird’s-eye view image is largely dominated by the background, we only backpropagate the top-k hardest pixels.</p></blockquote><p>对于语义分割的结果，使用的是<strong>cross-entropy</strong>进行监督。交叉熵主要刻画的是实际输出（概率）与期望输出（概率）的距离，也就是交叉熵的值越小，两个概率分布就越接近。此外，原文提到只对top-k个像素进行，因为针对整个BEV Tensor进行的分类监督，背景占绝大多数，loss较小可认为学会了，既然学会了就没有必要再学，也就不需要bp了。top-k 定义的比例是25%。交叉熵损失的具体定义可以参考<a href="https://massive11.github.io/2023/04/08/principle-bev-mo-xing-chang-yong-loss-zong-jie/">BEV模型常用LOSS总结</a></p><h3 id="centerness"><a href="#centerness" class="headerlink" title="centerness"></a>centerness</h3><p>对于中心点回归，使用的是 l2 loss</p><h3 id="offset-amp-flow"><a href="#offset-amp-flow" class="headerlink" title="offset &amp; flow"></a>offset &amp; flow</h3><p>offset和flow的回归都使用的l1 loss</p><h2 id="FIERY-指标"><a href="#FIERY-指标" class="headerlink" title="FIERY 指标"></a>FIERY 指标</h2><p>针对语义分割结果的指标使用的是IoU（阈值为0.5）。<br><img src="/images/fiery_iou.png" width="60%" height="60%"/></p><p>针对时序分割的指标使用的是FIERY自己定义的VPQ(Video Panoptic Quality)，具体来说这个指标希望能够衡量识别质量RQ和分割质量SQ方面的性能，其中<br>RQ：衡量随着时间的推移，实例被检测到的一致性<br>SQ：实例分割的准确性<br>VPQ的定义如下</p><script type="math/tex; mode=display">VPQ = \sum_{t=0}^{H}\frac{\sum_{(p_t, q_t)\in Tp_t}IoU(p_t, q_t)}{|TP_t|+\frac{1}{2}|FP_t|+\frac{1}{2}|FN_t|}</script><p>此处对于真正例的定义是1.IoU超过0.5；2.实例id在不同时刻保持一致</p><h2 id="FIERY缺点"><a href="#FIERY缺点" class="headerlink" title="FIERY缺点"></a>FIERY缺点</h2><ol><li>在网络架构中，没有进行帧间的跟踪，是在后处理过程中经过匈牙利匹配得到的；</li><li>非常耗时，3帧处理，加速后在orin平台部署也只能达到11hz；</li><li>强依赖精准内外参，行车过程中的参数变化会对结果产生比较大的影响。</li></ol><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://arxiv.org/pdf/2104.10490.pdf">Fiery Paper</a><br><a href="https://arxiv.org/pdf/2008.05711.pdf">LSS Paper</a><br><a href="https://developer.aliyun.com/article/1173815">https://developer.aliyun.com/article/1173815</a><br><a href="https://zhuanlan.zhihu.com/p/589146284">https://zhuanlan.zhihu.com/p/589146284</a><br><a href="https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html">https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html</a><br><a href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html">https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LSS </tag>
            
            <tag> Prediction </tag>
            
            <tag> Multi-view 3D </tag>
            
            <tag> BEV Perception </tag>
            
            <tag> Temporal </tag>
            
            <tag> ICCV </tag>
            
            <tag> ECCV </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>自动驾驶常见坐标转换与nuScenes中的坐标转换实战</title>
      <link href="/2022/03/18/principle-zuo-biao-zhuan-huan-yuan-li-yu-nuscenes-zhong-de-zuo-biao-zhuan-huan-shi-zhan/"/>
      <url>/2022/03/18/principle-zuo-biao-zhuan-huan-yuan-li-yu-nuscenes-zhong-de-zuo-biao-zhuan-huan-shi-zhan/</url>
      
        <content type="html"><![CDATA[<p>自动驾驶常见坐标转换与Nuscenes中的坐标转换实战<br><span id="more"></span></p><h2 id="自动驾驶常见坐标转换"><a href="#自动驾驶常见坐标转换" class="headerlink" title="自动驾驶常见坐标转换"></a>自动驾驶常见坐标转换</h2><p>自动驾驶系统涉及很多传感器，每个传感器处于不同的坐标系统，要统一处理来自不同传感器的数据，就涉及到传感器之间的坐标转换。以下将主要围绕视觉系统涉及到的坐标转换进行展开。</p><h3 id="相机相关的基础知识"><a href="#相机相关的基础知识" class="headerlink" title="相机相关的基础知识"></a>相机相关的基础知识</h3><p><strong>光轴</strong>：一条垂直穿过理想透镜中心的光线。<br><strong>焦点</strong>：与光轴平行的光线射入凸透镜时，理想的凸镜应该是所有的光线会聚在透镜后面一点上，这个会聚所有光线的一点，就叫做焦点。<br><strong>焦距</strong>：一般指镜片的中心到焦点为止的光轴上的距离，入射平行光线（或其延长线）与出射会聚光线（或其延长线）相交，就能确定折射主面，这个想象的平面与镜头光轴相交处就是主点。<br><strong>光圈</strong>：光圈对照片的影响主要有两大项，一是镜头的进光量，二是相片的景深。光圈越大（f 值越小），景深越浅（前景/背景看上去就像化掉一样）；光圈越小（f 值越大），景深越深（前景/背景看上去比较清楚）</p><h3 id="相机坐标系统"><a href="#相机坐标系统" class="headerlink" title="相机坐标系统"></a>相机坐标系统</h3><p>为了清晰描述整个相机坐标系统，首先需要明确将要涉及到的坐标系的定义</p><ol><li>像素坐标系<br> 像素坐标系是二维坐标系，它用于描述像素点在图像中的位置。像素坐标系一般以左上角为原点，向右为u轴正方向，向下为v轴正方向，用$(u, v)$表示其坐标值。</li><li>图像坐标系<br> 以光心在图像平面投影为原点的坐标系 ，X轴和Y轴分别平行于图像平面的两条垂直边，用$(x, y)$表示其坐标值。图像坐标系是用物理单位表示像素在图像中的位置。</li><li>相机坐标系<br>相机坐标系是一个三维空间坐标系，以相机光心为原点的坐标系。X轴和Y轴分别平行于图像坐标系的X轴和Y轴，相机的光轴为Z轴，z轴垂直于像平面且朝向像平面，Z轴和像平面的交点正是图像坐标系的像主点。用$(x_c, y_c, z_c)$表示其坐标值。</li><li>世界坐标系<br>世界坐标系是一个三维直角坐标系，它是一个绝对坐标系，旨在将空间中的所有点都统一到同一个坐标系下表达。在不同的应用场景中，世界坐标系的定义并不一样，可以根据实际情况自由确定。在自动驾驶场景中，常将各传感器数据统一到LiDar坐标系或者IMU坐标系下。用$(x_w, y_w, z_w)$表示其坐标值。</li></ol><h4 id="相机-gt-像素（相机内参）"><a href="#相机-gt-像素（相机内参）" class="headerlink" title="相机-&gt;像素（相机内参）"></a>相机-&gt;像素（相机内参）</h4><p>相机将三维世界中的坐标点映射到二维图像平面的过程可以用几何模型进行描述，其中比较经典又简单有效的是针孔模型。针孔模型描述了一束光线通过针孔之后，在针孔背面投影成像的关系。由于镜头上存在透镜，光线投影到成像平面的过程中会产生畸变，将在<a href="https://massive11.github.io/2022/09/12/principle-xiang-ji-cheng-xiang-ji-bian-mo-xing/">相机畸变</a>这篇文章中具体介绍。针孔成像模型如下图所示<br><img src="/images/pinhole_model.png"  width="60%" height="60%"/><br>设${O-x-y-z}$为相机坐标系，$O$为相机的光心。空间中一点$P$经过小孔$O$投影后落在物理成像平面${O’-x’-y’}$上，成像点为$P’$。设$P$的坐标为$[X, Y, Z]^T$，$P’$为$[X’, Y’, Z’]^T$。设物理成像平面到小孔的距离为$f$（焦距）。根据三角形相似关系，</p><script type="math/tex; mode=display">\frac{Z}{f}=-\frac{X}{X'}=-\frac{Y}{Y'}</script><p>负号表示成的是倒立的像，但是实际成像并不是倒像。将成像平面对称地放到相机前面，和三维空间点一起放在相机坐标系的同一侧，上式中的负号就可以去掉。</p><script type="math/tex; mode=display">\frac{Z}{f}=\frac{X}{X'}=\frac{Y}{Y'}</script><p>把$X’$、$Y’$放在等式左侧，整理得到</p><script type="math/tex; mode=display">X'=f\frac{X}{Z}</script><script type="math/tex; mode=display">Y'=f\frac{Y}{Z}</script><p>由此，我们得到了点$P$和它的像之间的空间关系，目前所有的单位都可理解成米。由于最终获得的是一个个的像素，还需要在成像平面上对像进行采样和量化。为了描述传感器将感受到的光线转换成像素图像的过程，设在物理成像平面上固定着一个像素平面${O-u-v}$。在像素平面上得到了$P’$的像素坐标$[u, v]^T$。<br>我们在上文中已经明确了像素坐标系的定义，像素坐标系与成像平面之间，相差了一个缩放和一个原点的平移。设像素坐标在$u$轴上缩放了$\alpha$倍， 在$v$轴上缩放了$\beta$倍。同时，原点平移了$[c_x, c_y]^T$。那么，$P’$的坐标与像素坐标$[u, v]^T$的关系为</p><script type="math/tex; mode=display">u=\alpha X' + c_x</script><script type="math/tex; mode=display">v=\beta Y' + c_y</script><p>合并$\alpha f$为$f_x$，$\beta f$为$f_y$，可得</p><script type="math/tex; mode=display">u=f_x \frac{X}{Z} + c_x</script><script type="math/tex; mode=display">v=f_y \frac{Y}{Z} + c_y</script><p>其中，$f$的单位为米，$\alpha$，$\beta$的单位为像素/米，因此$f_x$，$f_y$，$c_x$，$c_y$的单位为像素。将上式写成矩阵形式为</p><script type="math/tex; mode=display">\begin{pmatrix}u\\v\\1\\\end{pmatrix}=\frac{1}{Z}\begin{pmatrix}f_x&0&c_x\\0&f_y&c_y\\0&0&1\\\end{pmatrix}\begin{pmatrix}x\\y\\z\\\end{pmatrix}\overset{def}{=}\frac{1}{Z}KP</script><p>将$Z$移到左边，可以得到</p><script type="math/tex; mode=display">Z\begin{pmatrix}u\\v\\1\\\end{pmatrix}=\begin{pmatrix}f_x&0&c_x\\0&f_y&c_y\\0&0&1\\\end{pmatrix}\begin{pmatrix}x\\y\\z\\\end{pmatrix}\overset{def}{=}KP</script><p>矩阵$K$通常称为相机内参（Camera Intrinsics）。通常认为相机内参在出厂之后是固定的。也可以通过相机标定自行确定相机内参。</p><h4 id="世界-gt-相机（相机外参）"><a href="#世界-gt-相机（相机外参）" class="headerlink" title="世界-&gt;相机（相机外参）"></a>世界-&gt;相机（相机外参）</h4><p>上式中，我们使用的是$P$在相机坐标系下的坐标，由于相机实际上是运动的，$P$的相机坐标应该是它的世界坐标$P_w$根据相机的当前位置变换到相机坐标系下的结果。相机的位姿由其旋转矩阵$R$和平移向量$t$描述，因此有</p><script type="math/tex; mode=display">ZP_{uv}=Z\begin{bmatrix}u\\v\\1\\\end{bmatrix}=K(RP_w+t)=KTP_w</script><p>上式描述的是$P$的世界坐标到像素坐标的投影关系。其中，相机的位姿成为相机的外参（Camera Extrinsics）。外参会随着相机的运动而发生变化。<br>需要注意的是，点的深度在投影过程中会丢失，这也是多目3D感知中的难点所在。</p><h2 id="三维空间刚体运动"><a href="#三维空间刚体运动" class="headerlink" title="三维空间刚体运动"></a>三维空间刚体运动</h2><p>上文中，我们使用内外参实现了相机系统相关的坐标转换。在实际使用中，描述刚体在三维空间中的运动可以使用多种方式。这一部分，将对相应知识点进行补充讲解。</p><h3 id="坐标系间的欧式变换"><a href="#坐标系间的欧式变换" class="headerlink" title="坐标系间的欧式变换"></a>坐标系间的欧式变换</h3><p>两个坐标系之间的运动由一个旋转加上一个平移组成，这种运动称为刚体运动。两个坐标系之间的转换通过欧式变换进行描述。欧式变换由旋转和平移组成。<br>首先考虑旋转，设某个单位正交基$(e_1, e_2, e_3)$经过一次旋转变成了$(e_1’, e_2’, e_3’)$。对于同一个向量$a$，它在两个坐标系下的坐标为$[a_1, a_2, a_3]^T$和$[a_1’, a_2’, a_3’]^T$。因为向量本身并没有改变，根据坐标的定义有</p><script type="math/tex; mode=display">[e_1, e_2, e_3]\begin{bmatrix}a_1\\a_2\\a_3\\\end{bmatrix}=[e_1', e_2', e_3']\begin{bmatrix}a_1'\\a_2'\\a_3'\\\end{bmatrix}</script><p>为了描述两个坐标之间的关系，对上述等式的左右两边同时左乘</p><script type="math/tex; mode=display">\begin{bmatrix}e_1^T\\e_2^T\\e_3^T\\\end{bmatrix}</script><p>等式左边的系数就变成了单位矩阵，所以</p><script type="math/tex; mode=display">\begin{bmatrix}a_1\\a_2\\a_3\\\end{bmatrix}=\begin{bmatrix}e_1^T e_1' & e_1^T e_2' & e_1^T e_3'\\e_2^T e_1' & e_2^T e_2' & e_2^T e_3'\\e_3^T e_1' & e_3^T e_2' & e_3^T e_3'\\\end{bmatrix}\begin{bmatrix}a_1'\\a_2'\\a_3'\\\end{bmatrix}\overset{def}{=}Ra'</script><p>将中间的矩阵定义为旋转矩阵$R$，该矩阵由两组基之间的内积组成，描述的是旋转前后同一个向量的坐标转换关系。由于基的长度为1，实际上是各基向量夹角的余弦值，因此该矩阵也称为方向余弦矩阵。</p><h3 id="旋转矩阵"><a href="#旋转矩阵" class="headerlink" title="旋转矩阵"></a>旋转矩阵</h3><h3 id="旋转向量和欧拉角"><a href="#旋转向量和欧拉角" class="headerlink" title="旋转向量和欧拉角"></a>旋转向量和欧拉角</h3><h3 id="四元数"><a href="#四元数" class="headerlink" title="四元数"></a>四元数</h3><h2 id="Nuscenes中的坐标转换"><a href="#Nuscenes中的坐标转换" class="headerlink" title="Nuscenes中的坐标转换"></a>Nuscenes中的坐标转换</h2><p>nuScenes的传感器坐标系设置如图所示<br><img src="/images/nuscenes.png"/></p><p>由于在nuScenes数据集中，相机的外参是LiDar到相机的转换矩阵，因此基于nuScenes的BEV研究，通常将LiDar坐标系的XY轴定义为BEV坐标系的XY轴。</p><h3 id="图像-gt-LiDar"><a href="#图像-gt-LiDar" class="headerlink" title="图像-&gt;LiDar"></a>图像-&gt;LiDar</h3><p>根据上文，此处主要涉及像素-图像、图像-相机、相机-LiDar共三个步骤。</p><h3 id="LiDar-gt-惯导"><a href="#LiDar-gt-惯导" class="headerlink" title="LiDar-&gt;惯导"></a>LiDar-&gt;惯导</h3><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.nuscenes.org/nuscenes">https://www.nuscenes.org/nuscenes</a><br>《视觉SLAM十四讲》—高翔<br><a href="https://zhuanlan.zhihu.com/p/340751743">https://zhuanlan.zhihu.com/p/340751743</a><br><a href="https://zhuanlan.zhihu.com/p/374269239">https://zhuanlan.zhihu.com/p/374269239</a><br><a href="https://zhuanlan.zhihu.com/p/453081175">https://zhuanlan.zhihu.com/p/453081175</a></p>]]></content>
      
      
      <categories>
          
          <category> 实战 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Coordinate system </tag>
            
            <tag> Camera imaging </tag>
            
            <tag> nuScenes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transformer详解</title>
      <link href="/2022/01/25/principle-zhu-yi-li-ji-zhi/"/>
      <url>/2022/01/25/principle-zhu-yi-li-ji-zhi/</url>
      
        <content type="html"><![CDATA[<p>Transformer详解<br><span id="more"></span></p><h2 id="Transformer的提出"><a href="#Transformer的提出" class="headerlink" title="Transformer的提出"></a>Transformer的提出</h2><p>机器翻译领域主流的方案基于复杂的循环或卷积神经网络，其中包括编码器和解码器。以目前效果较好的RNN系列算法为例，这种架构存在并行度不够、计算效率低的问题。因为RNN要维护一个隐状态，该隐状态取决于上一时刻的隐状态。这种内在的串行计算特质阻碍了训练时的并行计算。特别是训练序列较长时，每一个句子占用的存储更多，batch size变小，并行度降低。有许多研究都在尝试解决这一问题，但是串行计算的本质无法改变。</p><p>在Transformer提出之前，已经有文章提出了基于注意力的架构。这种架构依然使用了编码器和解码器，只不过解码器的输入是编码器的状态的加权和，而不再是一个简单的中间状态。每一个输出对每一个输入的权重叫做注意力，注意力的大小取决于输出和输入的相关关系。这种架构优化了编码器和解码器之间的信息交流方式，在处理长文章时更加有效。但几乎所有的注意力机制都用在RNN上的。</p><p>既然注意力机制能够无视序列的先后顺序，捕捉序列间的关系，为什么不只用这种机制来构造一个适用于并行计算的模型呢？因此，作者提出了Transformer架构。这一架构规避了RNN的使用，完全使用注意力机制来捕捉输入输出序列之间的依赖关系。这种架构不仅训练得更快了，表现也更强了。</p><h2 id="注意力机制"><a href="#注意力机制" class="headerlink" title="注意力机制"></a>注意力机制</h2><p>在熟悉Transformer的整体结构之前，先来看注意力机制的细节。</p><blockquote><p>An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. (Attention is all you need, 2017)</p></blockquote><p>注意力机制可以理解成利用query去找对应的keys然后计算这些keys对应的value的加权和。由于query可能是比较模糊的描述，最通用的方法是把query和key各建模成一个向量。对query和key之间算一个相似度（比如向量内积），以这个相似度为权重，算value的加权和。</p><p>从本质上来说，注意力是从大量信息中有筛选出少量重要信息，并聚焦到这些重要信息上，忽略大多不重要的信息。权重越大越聚焦于其对应的Value值上，即权重代表信息的重要性，Value是对应的信息。</p><h2 id="Scaled-Dot-Product-Attention"><a href="#Scaled-Dot-Product-Attention" class="headerlink" title="Scaled Dot-Product Attention"></a>Scaled Dot-Product Attention</h2><p>原论文称其提出的注意力机制为Scaled Dot-Product Attention，其结构如下</p><p><img src="/images/Attention_scheme.png" width="30%" height="30%"/></p><p>如图所示，注意力机制的核心过程就是通过Q和K计算得到注意力权重，然后再作用于V得到整个权重和输出。</p><p>对于输入Q、K和V来说，其输出向量的计算公式为：</p><script type="math/tex; mode=display">{Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V}</script><p>此处$d_k$是query和key向量的长度，${\frac{1}{\sqrt{d_k}}}$就是上图中的scale，这一步是因为作者通过实验发现，softmax在绝对值较大的区域梯度较小，梯度下降的速度比较慢。较大的${d_k}$在完成${QK^T}$后会得到很大的值，这导致在经过sofrmax操作后产生非常小的梯度，不利于网络的训练。</p><p>注意力机制的计算过程从本质上来讲归纳为两个过程，第一个过程是根据Query和Key计算权重系数，第二个过程根据权重系数对Value进行加权求和。第一个过程可以继续细分为两个阶段：第一阶段根据Query和Key计算两者的相似性；第二阶段对第一阶段的原始分值进行归一化处理。</p><p>在第一个阶段，可以引入不同的函数和计算机制，根据Query和Key，计算两者的相似性或者相关性。算相似度并不只有求点乘这一种方式。另一种常用的注意力函数叫做加性注意力，它用一个单层神经网络来计算两个向量的相似度。相比之下，点积注意力在实践中要快得多，空间效率更高，因为它可以使用高度优化的矩阵乘法代码来实现。</p><p>在第二阶段引入类似SoftMax的计算方式对第一阶段的得分进行数值转换，一方面可以进行归一化，将原始计算分值整理成所有元素权重之和为1的概率分布；另一方面也可以通过SoftMax的内在机制更加突出重要元素的权重。</p><h2 id="多头自注意力"><a href="#多头自注意力" class="headerlink" title="多头自注意力"></a>多头自注意力</h2><p>在自注意力中，每一个单词的query, key, value应该只和该单词本身有关。因此，这三个向量都应该由单词的词嵌入得到。另外，每个单词的query, key, value不应该是人工指定的，而应该是可学习的。因此，我们可以用可学习的参数来描述从词嵌入到query, key, value的变换过程。综上，自注意力的输入<br>应该用下面这个公式计算：</p><script type="math/tex; mode=display">Q = EW^Q</script><script type="math/tex; mode=display">K = EW^K</script><script type="math/tex; mode=display">V = EW^V</script><p>其中，$E$是词嵌入矩阵，也就是每个单词的词嵌入的数组；$W^Q$、$W^K$、$W^V$是可学习的参数矩阵。在Transformer中，大部分中间向量的长度都用$d_{model}$表示，词嵌入的长度也是$d_{model}$。因此，设输入的句子长度为n，则$E$的size为$n \times d_{model}$，$W^Q$、$W^K$的size为$d_{model} \times d_k$，$W^V$的size为$d_{model} \times d_v$。</p><p>多头注意力允许模型共同关注不同位置的不同表示子空间的信息。仅使用单个注意力头的话，则会抑制这一点。多头注意力机制的结构图如下<br><img src="/images/multi_head_attention.png" width="30%" height="30%"/></p><p>就像卷积层能够用多个卷积核生成多个通道的特征一样，我们也用多组$W^Q$、$W^K$、$W^V$生成多组自注意力结果。这样，每个单词的自注意力表示会更丰富一点。这种机制就叫做多头注意力，它是将原始的输入序列进行多组的自注意力处理过程，然后再将每一组自注意力的结果拼接起来进行一次线性变换得到最终的输出结果。以$h$表示多头注意力的“头数”，其计算公式为：</p><script type="math/tex; mode=display">{MultiHead(Q,K,V)=Concate(head_1, ..., head_h)W^o \   where \   head_i=Attention(QW_i^Q, kW_i^K, VW_i^V)}</script><p>多头注意力机制其实就是将一个大的高维单头拆分成了h个多头，每个头可以关注输入的不同部分。因此能够表示比简单加权平均值更复杂的函数。由于每个头的维数降低，总计算成本与全维单头注意相似。</p><h2 id="Transformer架构"><a href="#Transformer架构" class="headerlink" title="Transformer架构"></a>Transformer架构</h2><p>Transformer 是基于 encoder-decoder 的架构，它使用注意力来编码，其整体结构如下图所示<br><img src="/images/Transformer.png" width="50%" height="50%"/><br>模型主干中，除了上文提到注意力模块，还有Add &amp; Norm、Feed Forward，此外，decoder中的一个多头注意力前面加了Masked。</p><ol><li><p>Add &amp; Norm<br>Transformer使用了和ResNet类似的残差连接，即设模块本身的映射为$F(x)$，则模块输出为$Normalization(F(x)+x)$。和ResNet不同，Transformer使用的归一化方法是LayerNorm。</p></li><li><p>Feed Forward<br>架构图中的前馈网络（Feed Forward）其实就是一个全连接网络。具体来说，这个子网络由两个线性层组成，中间用ReLU作为激活函数。</p><script type="math/tex; mode=display">FFN(x) = max(0, xW_1+b_1)W_2 + b_2</script></li></ol><h3 id="编码器"><a href="#编码器" class="headerlink" title="编码器"></a>编码器</h3><p>Transformer 的编码阶段概括如下：</p><ol><li>为每个单词生成初始表达或 embeddings</li><li>对于每一个词，使用自注意力聚合来自所有其他上下文单词的信息，生成参考了整个上下文的每个单词的新表达</li><li>基于前面生成的表达，连续地构建新的表达对每个单词并行地重复多次这种处理。</li></ol><p>Encoder 的 self-attention 中，所有 Key, Value 和 Query 都来自同一位置，即上一层 encoder 的输出。Decoder的设计类似，所有 Key, Value 和 Query 都来自同一位置，即上一层 decoder 的输出，不过只能看到上一层对应当前 query 位置之前的部分。生成 Query 时，不仅关注前一步的输出，还参考编码器的最后一层输出。</p><h3 id="解码器"><a href="#解码器" class="headerlink" title="解码器"></a>解码器</h3><p>解码器的输入是当前已经生成的序列，该序列会经过一个掩码（masked）多头自注意力子层。我们先不管这个掩码是什么意思，暂且把它当成普通的多头自注意力层。它的作用和编码器中的一样，用于提取出更有意义的表示。<br>接下来，数据还会经过一个多头注意力层。这个层比较特别，它的K，V来自<br>，Q来自上一层的输出。为什么会有这样的设计呢？这种设计来自于早期的注意力模型。如下图所示，在早期的注意力模型中，每一个输出单词都会与每一个输入单词求一个注意力，以找到每一个输出单词最相关的某几个输入单词。用注意力公式来表达的话，Q就是输出单词，K, V就是输入单词。</p><p>这种并行计算有一个要注意的地方。在输出第$t+1$个单词时，模型不应该提前知道$t+1$时刻之后的信息。因此，应该只保留$t$时刻之前的信息，遮住后面的输入。这可以通过添加掩码实现。</p><p>这就是为什么解码器的多头自注意力层前面有一个masked。在论文中，mask是通过令注意力公式的softmax的输入为$-\infty$来实现的（softmax的输入为$-\infty$<br>，注意力权重就几乎为0，被遮住的输出也几乎全部为0）。每个mask都是一个上三角矩阵。</p><h3 id="嵌入层"><a href="#嵌入层" class="headerlink" title="嵌入层"></a>嵌入层</h3><p>和其他大多数序列转换任务一样，Transformer主干结构的输入输出都是词嵌入序列。词嵌入，其实就是一个把one-hot向量转换成有意义的向量的转换矩阵。在Transformer中，解码器的嵌入层和输出线性层是共享权重的——输出线性层表示的线性变换是嵌入层的逆变换，其目的是把网络输出的嵌入再转换回one-hot向量。如果某任务的输入和输出是同一种语言，那么编码器的嵌入层和解码器的嵌入层也可以共享权重。</p><h3 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h3><p>现在，Transformer的结构图还剩一个模块没有读——位置编码。无论是RNN还是CNN，都能自然地利用到序列的先后顺序这一信息。然而，Transformer的主干网络并不能利用到序列顺序信息。因此，Transformer使用了一种叫做“位置编码”的机制，对编码器和解码器的嵌入输入做了一些修改，以向模型提供序列顺序信息。</p><h2 id="细节重温"><a href="#细节重温" class="headerlink" title="细节重温"></a>细节重温</h2><ol><li><p>Transformer为何使用多头注意力机制？为什么不使用一个头</p><blockquote><p>Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.</p></blockquote><p>多头可以使模型共同关注不同位置的不同表示子空间的信息，这是单头注意力无法达到的效果，且矩阵的整体size不变，计算量和单个head差不多。</p></li><li>为什么Q和K使用不同的权重矩阵生成，为何不能使用同一个值进行自身的点乘？<ul><li>K和Q的点乘是为了得到一个attention score 矩阵。自身点乘可以表示在本空间内的相似度，但这个通常来说是不够的，泛化能力很差。为了使得模型有更强的表征能力，我们需要其在其他空间映射的相似度</li><li>直接拿K和K点乘的话，attention score 矩阵是一个对称矩阵。这种对称自反其实是不必要的</li></ul></li><li>计算Attention为何选择点乘而不是加法？两者计算复杂度和效果上有什么区别？<blockquote><p>While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.</p></blockquote></li><li>为什么在进行softmax之前需要对attention进行scaled（为什么除以$d_k$的平方根），并使用公式推导进行讲解<br>假设 Q 和 K 的均值为0，方差为1。它们的矩阵乘积$q\cdot k=\sum_{i=1}^{d_k}q_ik_i$将有均值为0，方差为$d_k$，因此使用$d_k$的平方根被用于缩放，因为，Q 和 K 的矩阵乘积的均值本应该为 0，方差本应该为1，这样可以获得更平缓的softmax。当维度很大时，点积结果会很大，会导致softmax的梯度很小。为了减轻这个影响，对点积进行缩放。</li><li>在计算attention score的时候如何对padding做mask操作？<br>对需要mask的位置设为负无穷，再对attention score进行相加</li><li>为什么在进行多头注意力的时候需要对每个head进行降维？<br>将原有的高维空间转化为多个低维空间并再最后进行拼接，形成同样维度的输出，借此丰富特性信息，降低了计算量</li><li>Transformer的Encoder模块？<br>输入嵌入-加上位置编码-多个编码器层（每个编码器层包含全连接层，多头注意力层和点式前馈网络层（包含激活函数层））</li><li>为何在获取输入词向量之后需要对矩阵乘以embedding size的开方？<br>embedding matrix的初始化方式是xavier init，这种方式的方差是1/embedding size，因此乘以embedding size的开方使得embedding matrix的方差是1，在这个scale下可能更有利于embedding matrix的收敛。</li><li>简单介绍一下Transformer的位置编码？有什么意义和优缺点？<br>因为self-attention是位置无关的，无论句子的顺序是什么样的，通过self-attention计算的token的hidden embedding都是一样的，这显然不符合人类的思维。因此要有一个办法能够在模型中表达出一个token的位置信息，transformer使用了固定的positional encoding来表示token在句子中的绝对位置信息。</li><li>还了解哪些关于位置编码的技术，各自的优缺点是什么？<br>相对位置编码（RPE）1.在计算attention score和weighted value时各加入一个可训练的表示相对位置的参数。2.在生成多头注意力时，把对key来说将绝对位置转换为相对query的位置3.复数域函数，已知一个词在某个位置的词向量表示，可以计算出它在任何位置的词向量表示。前两个方法是词向量+位置编码，属于亡羊补牢，复数域是生成词向量的时候即生成对应的位置信息。</li></ol><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://arxiv.org/pdf/1706.03762.pdf">Attention is all you need</a><br><a href="https://zhouyifan.net/2022/11/12/20220925-Transformer/">https://zhouyifan.net/2022/11/12/20220925-Transformer/</a><br><a href="https://www.ylkz.life/deeplearning/p10553832/">https://www.ylkz.life/deeplearning/p10553832/</a><br><a href="https://zhuanlan.zhihu.com/p/265108616">https://zhuanlan.zhihu.com/p/265108616</a><br><a href="https://congchan.github.io/NLP-attention-03-self-attention/">https://congchan.github.io/NLP-attention-03-self-attention/</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Attention </tag>
            
            <tag> Transformer </tag>
            
            <tag> NeurIPS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PAT - [1147] Heaps (30分)</title>
      <link href="/2020/04/07/pat-1147-heaps-30-fen/"/>
      <url>/2020/04/07/pat-1147-heaps-30-fen/</url>
      
        <content type="html"><![CDATA[<p>从后往前检查所有节点（除了根节点）和它的父节点的关系，判断是否破坏最大堆或者最小堆的性质<br><span id="more"></span></p><h1 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h1><p>In computer science, a heap is a specialized tree-based data structure that satisfies the heap property: if P is a parent node of C, then the key (the value) of P is either greater than or equal to (in a max heap) or less than or equal to (in a min heap) the key of C. A common implementation of a heap is the binary heap, in which the tree is a complete binary tree. (Quoted from Wikipedia at <a href="https://en.wikipedia.org/wiki/Heap_(data_structure">https://en.wikipedia.org/wiki/Heap_(data_structure</a>))</p><p>Your job is to tell if a given complete binary tree is a heap.</p><h1 id="输入说明"><a href="#输入说明" class="headerlink" title="输入说明"></a>输入说明</h1><p>Each input file contains one test case. For each case, the first line gives two positive integers: M (≤ 100), the number of trees to be tested; and N (1 &lt; N ≤ 1,000), the number of keys in each tree, respectively. Then M lines follow, each contains N distinct integer keys (all in the range of int), which gives the level order traversal sequence of a complete binary tree.</p><h1 id="输出说明"><a href="#输出说明" class="headerlink" title="输出说明"></a>输出说明</h1><p>For each given tree, print in a line Max Heap if it is a max heap, or Min Heap for a min heap, or Not Heap if it is not a heap at all. Then in the next line print the tree’s postorder traversal sequence. All the numbers are separated by a space, and there must no extra space at the beginning or the end of the line.</p><h1 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h1><h2 id="输入"><a href="#输入" class="headerlink" title="输入"></a>输入</h2><p>3 8<br>98 72 86 60 65 12 23 50<br>8 38 25 58 52 82 70 60<br>10 28 15 12 34 9 8 56</p><h2 id="输出"><a href="#输出" class="headerlink" title="输出"></a>输出</h2><p>Max Heap<br>50 60 65 72 12 23 86 98<br>Min Heap<br>60 58 52 38 82 70 25 8<br>Not Heap<br>56 12 34 28 9 8 15 10</p><h1 id="Vocabulary"><a href="#Vocabulary" class="headerlink" title="Vocabulary"></a>Vocabulary</h1><ul><li>heap 堆</li></ul><h1 id="自己的想法"><a href="#自己的想法" class="headerlink" title="自己的想法"></a>自己的想法</h1><p>非常蠢的企图先把树建出来再挨个判断是大根堆还是小根堆，然后再将其转化为后序遍历输出。其实也有想过要不要直接根据二叉树的规律进行判断，因为对树相关的题目不够熟练所以打算先按自己的方法尝试一下，果然失败了。看了别人的答案之后才觉得自己的想法真是绕了太多的弯，且没能解决问题。</p><h1 id="参考题解："><a href="#参考题解：" class="headerlink" title="参考题解："></a>参考题解：</h1><p>从后往前检查所有节点（除了根节点）和它的父节点的关系，判断是否破坏最大堆或者最小堆的性质，如果有不满足的情况将maxn或minn置为0，以此排除最大堆或者最小堆～</p><p>然后后序遍历，对于index结点分别遍历孩子index<em>2和右孩子index</em>2+1，遍历完左右子树后输出根结点～</p><h1 id="参考代码"><a href="#参考代码" class="headerlink" title="参考代码"></a>参考代码</h1><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;&#x2F;https:&#x2F;&#x2F;www.liuchuo.net&#x2F;archives&#x2F;4667#include &lt;iostream&gt;using namespace std;int a[1005], m, n;void postOrder(int index) &#123;    if (index &gt; n) return;    postOrder(index * 2);    postOrder(index * 2 + 1);    printf(&quot;%d%s&quot;, a[index], index &#x3D;&#x3D; 1 ? &quot;\n&quot; : &quot; &quot;);&#125;int main() &#123;    scanf(&quot;%d %d&quot;, &amp;m, &amp;n);    while (m--) &#123;        int minn &#x3D; 1, maxn &#x3D; 1;        for (int i &#x3D; 1; i &lt;&#x3D; n; i++) scanf(&quot;%d&quot;, &amp;a[i]);        for (int i &#x3D; 2; i &lt;&#x3D; n; i++) &#123;            if (a[i] &gt; a[i &#x2F; 2]) maxn &#x3D; 0;            if (a[i] &lt; a[i &#x2F; 2]) minn &#x3D; 0;        &#125;        if (maxn &#x3D;&#x3D; 1) printf(&quot;Max Heap\n&quot;);        else if (minn &#x3D;&#x3D; 1) printf(&quot;Min Heap\n&quot;);        else printf(&quot;Not Heap\n&quot;);        postOrder(1);    &#125;    return 0;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 刷题笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PAT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PAT - [1145] Hashing - Average Search Time (25分)</title>
      <link href="/2020/03/25/pat-1145-hashing-average-search-time-25-fen/"/>
      <url>/2020/03/25/pat-1145-hashing-average-search-time-25-fen/</url>
      
        <content type="html"><![CDATA[<p>本题其实并不难，只是很多hash相关的基础知识容易遗忘，需要多复习巩固一下。<br><span id="more"></span></p><h1 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h1><p>The task of this problem is simple: insert a sequence of distinct positive integers into a hash table first. Then try to find another sequence of integer keys from the table and output the average search time (the number of comparisons made to find whether or not the key is in the table). The hash function is defined to be H(key)=key%TSize where TSize is the maximum size of the hash table. Quadratic probing (with positive increments only) is used to solve the collisions.</p><p>Note that the table size is better to be prime. If the maximum size given by the user is not prime, you must re-define the table size to be the smallest prime number which is larger than the size given by the user.</p><h1 id="输入说明"><a href="#输入说明" class="headerlink" title="输入说明"></a>输入说明</h1><p>Each input file contains one test case. For each case, the first line contains 3 positive numbers: MSize, N, and M, which are the user-defined table size, the number of input numbers, and the number of keys to be found, respectively. All the three numbers are no more than $10^4$.Then N distinct positive integers are given in the next line, followed by M positive integer keys in the next line. All the numbers in a line are separated by a space and are no more than $10^5$.</p><h1 id="输出说明"><a href="#输出说明" class="headerlink" title="输出说明"></a>输出说明</h1><p>For each test case, in case it is impossible to insert some number, print in a line X cannot be inserted. where X is the input number. Finally print in a line the average search time for all the M keys, accurate up to 1 decimal place.</p><h1 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h1><h3 id="input"><a href="#input" class="headerlink" title="input:"></a>input:</h3><p>4 5 4<br>10 6 4 15 11<br>11 4 15 2</p><h3 id="output"><a href="#output" class="headerlink" title="output:"></a>output:</h3><p>15 cannot be inserted.<br>2.8</p><h1 id="Vocabulary"><a href="#Vocabulary" class="headerlink" title="Vocabulary"></a>Vocabulary</h1><ul><li>quadratic 二次方的</li><li>probing 调查，探究</li><li>quadratic probing 平方探测法</li><li>collision 冲突</li></ul><h1 id="解题思路"><a href="#解题思路" class="headerlink" title="解题思路"></a>解题思路</h1><p>先找到大于tsize的最小的素数为真正的tsize，然后建立一个tsize长度的数组。首先用平方探测法插入数字a，每次pos = (a + j <em> j) % tsize，j是从0～tsize-1的数字，如果当前位置可以插入就将a赋值给v[pos]，如果一次都没有能够插入成功就输出”X cannot be inserted.”。其次计算平均查找时间，每次计算pos = (a + j </em> j) % tsize，其中j &lt;= tsize，如果v[pos]处正是a则查找到了，则退出循环，如果v[pos]处不存在数字表示没查找到，那么也要退出循环。每次查找的时候，退出循环之前的j就是这个数字的查找长度。最后ans除以m得到平均查找时间然后输出。<br>本题其实并不难，只是很多hash相关的基础知识容易遗忘，需要多复习巩固一下。</p><h1 id="参考代码"><a href="#参考代码" class="headerlink" title="参考代码"></a>参考代码</h1><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;&#x2F;代码链接：https:&#x2F;&#x2F;blog.csdn.net&#x2F;liuchuo&#x2F;article&#x2F;details&#x2F;79819316#include&lt;cstdio&gt;$#include&lt;cmath&gt;#include&lt;vector&gt;using namespace std;int Msize,n,m,t;bool isPrime(int n)&#123;if(n&lt;2) return false;int sqrtN&#x3D;(int)sqrt(1.0*n);for(int i&#x3D;2;i&lt;&#x3D;sqrtN;i++)&#123;if(n%i&#x3D;&#x3D;0) return false;&#125;return true;&#125;int main()&#123;scanf(&quot;%d%d%d&quot;,&amp;Msize,&amp;n,&amp;m);while(!isPrime(Msize)) Msize++;vector&lt;int&gt; ht(Msize,0);for(int i&#x3D;0;i&lt;n;i++)&#123;scanf(&quot;%d&quot;,&amp;t);bool flag&#x3D;false;for(int j&#x3D;0;j&lt;Msize&amp;&amp;!flag;j++)&#123;int pos&#x3D;(t+j*j)%Msize;if(ht[pos]&#x3D;&#x3D;0)&#123;ht[pos]&#x3D;t;flag&#x3D;true;&#125;&#125;if(!flag) printf(&quot;%d cannot be inserted.\n&quot;,t);&#125;int total&#x3D;0;for(int i&#x3D;0;i&lt;m;i++)&#123;scanf(&quot;%d&quot;,&amp;t);for(int j&#x3D;0;j&lt;&#x3D;Msize;j++)&#123;total++;int pos&#x3D;(t+j*j)%Msize;if(ht[pos]&#x3D;&#x3D;t||ht[pos]&#x3D;&#x3D;0)&#123;break;&#125;&#125;&#125;printf(&quot;%.1f\n&quot;,(float)total&#x2F;m);return 0;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 刷题笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PAT </tag>
            
            <tag> hash </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LeetCode-181场周赛</title>
      <link href="/2020/03/22/leetcode-181-chang-zhou-sai/"/>
      <url>/2020/03/22/leetcode-181-chang-zhou-sai/</url>
      
        <content type="html"><![CDATA[<p>按既定顺序创建目标数组、四因数、检查网格中是否存在有效路径、最长快乐前缀<br><span id="more"></span></p><h1 id="1-按既定顺序创建目标数组"><a href="#1-按既定顺序创建目标数组" class="headerlink" title="1. 按既定顺序创建目标数组"></a>1. 按既定顺序创建目标数组</h1><p>给你两个整数数组 nums 和 index。你需要按照以下规则创建目标数组：</p><ul><li>目标数组 target 最初为空。</li><li>按从左到右的顺序依次读取 nums[i] 和 index[i]，在 target 数组中的下标 index[i] 处插入值 nums[i] 。</li><li>重复上一步，直到在 nums 和 index 中都没有要读取的元素。</li><li>请你返回目标数组。</li></ul><p>题目保证数字插入位置总是存在。</p><p>示例：<br>输入：nums = [0,1,2,3,4], index = [0,1,2,2,1]<br>输出：[0,4,1,3,2]<br>解释：<br>nums       index     target<br>0            0        [0]<br>1            1        [0,1]<br>2            2        [0,1,2]<br>3            2        [0,1,3,2]<br>4            1        [0,4,1,3,2]</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">class Solution &#123;public:    vector&lt;int&gt; createTargetArray(vector&lt;int&gt;&amp; nums, vector&lt;int&gt;&amp; index) &#123;        vector&lt;int&gt; result(index.size(),0);        int f[index.size()];        memset(f,0,sizeof(f));        for(int i&#x3D;0;i&lt;index.size();++i)&#123;            if(f[result[i]]&#x3D;&#x3D;0)&#123;                result[index[i]] &#x3D; nums[i];                f[index[i]] &#x3D; 1;            &#125;else&#123;                for(int j&#x3D;index.size()-1;j&gt;index[i];--j)&#123;                    result[j] &#x3D; result[j-1];                &#125;                result[index[i]] &#x3D; nums[i];            &#125;        &#125;        return result;    &#125;&#125;;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>代码优化<br>使用库方法</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;&#x2F;代码链接：https:&#x2F;&#x2F;leetcode-cn.com&#x2F;problems&#x2F;create-target-array-in-the-given-order&#x2F;solution&#x2F;di-181chang-zhou-sai-jie-ti-bao-gao-by-time-limit&#x2F;class Solution &#123;public:    vector&lt;int&gt; createTargetArray(vector&lt;int&gt;&amp; nums, vector&lt;int&gt;&amp; index) &#123;        vector&lt;int&gt; infoVec;        for(int i &#x3D; 0, n &#x3D; nums.size(); i &lt; n; i++) &#123;            infoVec.insert(infoVec.begin() + index[i], nums[i]);        &#125;        return infoVec;    &#125;&#125;;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="2-四因数"><a href="#2-四因数" class="headerlink" title="2. 四因数"></a>2. 四因数</h1><p>给你一个整数数组 nums，请你返回该数组中恰有四个因数的这些整数的各因数之和。</p><p>如果数组中不存在满足题意的整数，则返回 0 。</p><p>示例：<br>输入：nums = [21,4,7]<br>输出：32<br>解释：<br>21 有 4 个因数：1, 3, 7, 21<br>4 有 3 个因数：1, 2, 4<br>7 有 2 个因数：1, 7<br>答案仅为 21 的所有因数的和。</p><pre class="line-numbers language-c" data-language="c"><code class="language-c">class Solution <span class="token punctuation">&#123;</span>public<span class="token operator">:</span>    <span class="token keyword">int</span> <span class="token function">sumFourDivisors</span><span class="token punctuation">(</span>vector<span class="token operator">&lt;</span><span class="token keyword">int</span><span class="token operator">></span><span class="token operator">&amp;</span> nums<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>        <span class="token keyword">int</span> result <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>        <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>i<span class="token operator">&lt;</span>nums<span class="token punctuation">.</span><span class="token function">size</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token operator">++</span>i<span class="token punctuation">)</span><span class="token punctuation">&#123;</span>            <span class="token keyword">int</span> count <span class="token operator">=</span> <span class="token number">2</span><span class="token punctuation">;</span>            vector<span class="token operator">&lt;</span><span class="token keyword">int</span><span class="token operator">></span> add<span class="token punctuation">;</span>            <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> j<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">;</span>j<span class="token operator">*</span>j<span class="token operator">&lt;=</span>nums<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span><span class="token operator">++</span>j<span class="token punctuation">)</span><span class="token punctuation">&#123;</span>                <span class="token keyword">if</span><span class="token punctuation">(</span>nums<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">%</span>j<span class="token operator">==</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">&#123;</span>                    <span class="token keyword">if</span><span class="token punctuation">(</span>j<span class="token operator">!=</span>nums<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">/</span>j<span class="token punctuation">)</span><span class="token punctuation">&#123;</span>                        count<span class="token operator">+=</span><span class="token number">2</span><span class="token punctuation">;</span>                        add<span class="token punctuation">.</span><span class="token function">emplace_back</span><span class="token punctuation">(</span>j<span class="token punctuation">)</span><span class="token punctuation">;</span>                        add<span class="token punctuation">.</span><span class="token function">emplace_back</span><span class="token punctuation">(</span>nums<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">/</span>j<span class="token punctuation">)</span><span class="token punctuation">;</span>                    <span class="token punctuation">&#125;</span><span class="token keyword">else</span><span class="token punctuation">&#123;</span>                        <span class="token operator">++</span>count<span class="token punctuation">;</span>                        add<span class="token punctuation">.</span><span class="token function">emplace_back</span><span class="token punctuation">(</span>j<span class="token punctuation">)</span><span class="token punctuation">;</span>                    <span class="token punctuation">&#125;</span>                <span class="token punctuation">&#125;</span>                <span class="token keyword">if</span><span class="token punctuation">(</span>count<span class="token operator">></span><span class="token number">4</span><span class="token punctuation">)</span> <span class="token keyword">break</span><span class="token punctuation">;</span>            <span class="token punctuation">&#125;</span>            <span class="token keyword">if</span><span class="token punctuation">(</span>count<span class="token operator">==</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">&#123;</span>                result <span class="token operator">+=</span><span class="token number">1</span><span class="token punctuation">;</span>                result <span class="token operator">+=</span> nums<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>                <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> j <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>j<span class="token operator">&lt;</span>add<span class="token punctuation">.</span><span class="token function">size</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token operator">++</span>j<span class="token punctuation">)</span> result <span class="token operator">+=</span> add<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">;</span>            <span class="token punctuation">&#125;</span>        <span class="token punctuation">&#125;</span>        <span class="token keyword">return</span> result<span class="token punctuation">;</span>    <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>代码优化<br>思路上没有发现较好的解法，只是代码量能减少很多</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">class Solution &#123;public:    int sumFourDivisors(vector&lt;int&gt;&amp; nums) &#123;        int ans&#x3D;0,i,j,k;        for(auto c:nums)        &#123;            for(i&#x3D;1,j&#x3D;k&#x3D;0;i*i&lt;&#x3D;c;i++)if(c%i&#x3D;&#x3D;0)            &#123;                j++;                k+&#x3D;i;                if(i*i&lt;c)                &#123;                    j++;                    k+&#x3D;c&#x2F;i;                &#125;            &#125;            if(j&#x3D;&#x3D;4)ans+&#x3D;k;        &#125;        return ans;    &#125;&#125;;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="3-检查网格中是否存在有效路径"><a href="#3-检查网格中是否存在有效路径" class="headerlink" title="3. 检查网格中是否存在有效路径"></a>3. 检查网格中是否存在有效路径</h1><p>给你一个 m x n 的网格 grid。网格里的每个单元都代表一条街道。grid[i][j] 的街道可以是：</p><p>1 表示连接左单元格和右单元格的街道。<br>2 表示连接上单元格和下单元格的街道。<br>3 表示连接左单元格和下单元格的街道。<br>4 表示连接右单元格和下单元格的街道。<br>5 表示连接左单元格和上单元格的街道。<br>6 表示连接右单元格和上单元格的街道。<br><img src="/img/181-1.png" alt=""><br>你最开始从左上角的单元格 (0,0) 开始出发，网格中的「有效路径」是指从左上方的单元格 (0,0) 开始、一直到右下方的 (m-1,n-1) 结束的路径。该路径必须只沿着街道走。</p><p>注意：你不能变更街道。<br>如果网格中存在有效的路径，则返回 true，否则返回 false 。</p><p>示例：<br><img src="/img/181-2.png" alt=""></p><p>将该问题转化为DFS问题</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;&#x2F;参考代码：https:&#x2F;&#x2F;leetcode-cn.com&#x2F;problems&#x2F;check-if-there-is-a-valid-path-in-a-grid&#x2F;solution&#x2F;cdfsjie-fa-rong-yi-li-jie-dai-ma-duan-zhu-shi-duo-&#x2F;class Solution &#123;    int m,n,dx[4]&#x3D;&#123;1,0,-1,0&#125;,dy[4]&#x3D;&#123;0,1,0,-1&#125;;&#x2F;&#x2F;0下、1右、2上、3左    int pipe[7][4]&#x3D;&#123;&#123;-1,-1,-1,-1&#125;,&#123;-1,1,-1,3&#125;,&#123;0,-1,2,-1&#125;,&#123;-1,0,3,-1&#125;,&#123;-1,-1,1,0&#125;,&#123;3,2,-1,-1&#125;,&#123;1,-1,-1,2&#125;&#125;;    &#x2F;&#x2F;记录各个拼图块路径的方向，0、1、2、3代表方向，-1代表不可走。    bool dfs(int x,int y,int dir,vector&lt;vector&lt;int&gt;&gt;&amp; grid)&#123;&#x2F;&#x2F;(x,y,当前方向,地图)        if(x&#x3D;&#x3D;m-1&amp;&amp;y&#x3D;&#x3D;n-1) return 1;&#x2F;&#x2F;到达终点        int xx&#x3D;x+dx[dir];        int yy&#x3D;y+dy[dir];&#x2F;&#x2F;得到下一个准备走的坐标        if(xx&lt;0||yy&lt;0||xx&gt;&#x3D;m||yy&gt;&#x3D;n)return 0;&#x2F;&#x2F;越界        int nxt&#x3D;grid[xx][yy];&#x2F;&#x2F;得到下一块拼图的编号        if(pipe[nxt][dir]!&#x3D;-1)return dfs(xx,yy,pipe[nxt][dir],grid);&#x2F;&#x2F;如果当前方向可走，则方向改变，继续走。        return 0;&#x2F;&#x2F;无法走，返回0    &#125;    public:    bool hasValidPath(vector&lt;vector&lt;int&gt;&gt;&amp; grid) &#123;            m&#x3D;grid.size();        n&#x3D;grid[0].size();        int sta&#x3D;grid[0][0];&#x2F;&#x2F;起点的拼图编号        for(int i&#x3D;0;i&lt;4;++i)&#x2F;&#x2F;朝着四个方向都试一下            if(pipe[sta][i]!&#x3D;-1)&#x2F;&#x2F;当前方向可以走                if(dfs(0,0,pipe[sta][i],grid))&#x2F;&#x2F;沿着当前方向搜索                    return 1;&#x2F;&#x2F;拼图都有两个方向可以走，只要沿着一个初始方向走通就可以。        return 0;    &#125;&#125;;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="4-最长快乐前缀"><a href="#4-最长快乐前缀" class="headerlink" title="4. 最长快乐前缀"></a>4. 最长快乐前缀</h1><p>「快乐前缀」是在原字符串中既是 非空 前缀也是后缀（不包括原字符串自身）的字符串。</p><p>给你一个字符串 s，请你返回它的 最长快乐前缀。</p><p>如果不存在满足题意的前缀，则返回一个空字符串。</p><p>示例：<br>输入：s = “level”<br>输出：”l”<br>解释：不包括 s 自己，一共有 4 个前缀（”l”, “le”, “lev”, “leve”）和 4 个后缀（”l”, “el”, “vel”, “evel”）。最长的既是前缀也是后缀的字符串是 “l” 。</p><p>自己写的代码最后超时了😭</p><pre class="line-numbers language-c" data-language="c"><code class="language-c">class Solution <span class="token punctuation">&#123;</span>public<span class="token operator">:</span>    string <span class="token function">longestPrefix</span><span class="token punctuation">(</span>string s<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>        string str <span class="token operator">=</span> <span class="token string">""</span><span class="token punctuation">;</span>        <span class="token keyword">int</span> length <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>        <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> i<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">;</span>i<span class="token operator">&lt;</span>s<span class="token punctuation">.</span><span class="token function">size</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token operator">++</span>i<span class="token punctuation">)</span><span class="token punctuation">&#123;</span>            <span class="token keyword">if</span><span class="token punctuation">(</span>s<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token operator">==</span>s<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">&#123;</span>                <span class="token keyword">int</span> count <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>                <span class="token keyword">char</span> st<span class="token punctuation">[</span>s<span class="token punctuation">.</span><span class="token function">size</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">+</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">;</span>                bool f <span class="token operator">=</span> false<span class="token punctuation">;</span>                <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> j <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>i<span class="token operator">+</span>j<span class="token operator">&lt;</span>s<span class="token punctuation">.</span><span class="token function">size</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token operator">++</span>j<span class="token punctuation">)</span><span class="token punctuation">&#123;</span>                    <span class="token keyword">if</span><span class="token punctuation">(</span>s<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token operator">==</span>s<span class="token punctuation">[</span>i<span class="token operator">+</span>j<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">&#123;</span>                        <span class="token operator">++</span>count<span class="token punctuation">;</span>                        st<span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">=</span> s<span class="token punctuation">[</span>i<span class="token operator">+</span>j<span class="token punctuation">]</span><span class="token punctuation">;</span>                        <span class="token keyword">if</span><span class="token punctuation">(</span>i<span class="token operator">+</span>j<span class="token operator">+</span><span class="token number">1</span><span class="token operator">==</span>s<span class="token punctuation">.</span><span class="token function">size</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">&#123;</span>                            f <span class="token operator">=</span> true<span class="token punctuation">;</span>                            <span class="token keyword">break</span><span class="token punctuation">;</span>                        <span class="token punctuation">&#125;</span>                    <span class="token punctuation">&#125;</span><span class="token keyword">else</span><span class="token punctuation">&#123;</span>                        <span class="token keyword">break</span><span class="token punctuation">;</span>                    <span class="token punctuation">&#125;</span>                <span class="token punctuation">&#125;</span>                <span class="token keyword">if</span><span class="token punctuation">(</span>count<span class="token operator">></span>length<span class="token operator">&amp;&amp;</span>f <span class="token operator">==</span> true<span class="token punctuation">)</span><span class="token punctuation">&#123;</span>                    str<span class="token punctuation">.</span><span class="token function">resize</span><span class="token punctuation">(</span>count<span class="token punctuation">)</span><span class="token punctuation">;</span>                    <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> p <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>p<span class="token operator">&lt;</span>count<span class="token punctuation">;</span><span class="token operator">++</span>p<span class="token punctuation">)</span> str<span class="token punctuation">[</span>p<span class="token punctuation">]</span> <span class="token operator">=</span> st<span class="token punctuation">[</span>p<span class="token punctuation">]</span><span class="token punctuation">;</span>                    length <span class="token operator">=</span> count<span class="token punctuation">;</span>                    <span class="token keyword">if</span><span class="token punctuation">(</span>length<span class="token operator">==</span>s<span class="token punctuation">.</span><span class="token function">size</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token keyword">break</span><span class="token punctuation">;</span>                <span class="token punctuation">&#125;</span>            <span class="token punctuation">&#125;</span>        <span class="token punctuation">&#125;</span>        <span class="token keyword">return</span> str<span class="token punctuation">;</span>    <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token comment">//参考代码</span>class Solution <span class="token punctuation">&#123;</span>    <span class="token keyword">int</span> ne<span class="token punctuation">[</span><span class="token number">100005</span><span class="token punctuation">]</span><span class="token punctuation">;</span>    string ans<span class="token punctuation">;</span>public<span class="token operator">:</span>    string <span class="token function">longestPrefix</span><span class="token punctuation">(</span>string s<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>        <span class="token keyword">int</span> n<span class="token operator">=</span>s<span class="token punctuation">.</span><span class="token function">size</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>i<span class="token punctuation">,</span>j<span class="token punctuation">;</span>        <span class="token keyword">for</span><span class="token punctuation">(</span>ne<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">=</span>j<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>i<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">;</span>i<span class="token operator">&lt;=</span>n<span class="token punctuation">;</span>i<span class="token operator">++</span><span class="token punctuation">)</span>        <span class="token punctuation">&#123;</span>            <span class="token keyword">while</span><span class="token punctuation">(</span>j<span class="token operator">&amp;&amp;</span>s<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token operator">!=</span>s<span class="token punctuation">[</span>i<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>j<span class="token operator">=</span>ne<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">;</span>            <span class="token keyword">if</span><span class="token punctuation">(</span>s<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token operator">==</span>s<span class="token punctuation">[</span>i<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>j<span class="token operator">++</span><span class="token punctuation">;</span>            ne<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">=</span>j<span class="token punctuation">;</span>        <span class="token punctuation">&#125;</span>        ans<span class="token punctuation">.</span><span class="token function">clear</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token keyword">for</span><span class="token punctuation">(</span>i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>i<span class="token operator">&lt;</span>ne<span class="token punctuation">[</span>n<span class="token punctuation">]</span><span class="token punctuation">;</span>i<span class="token operator">++</span><span class="token punctuation">)</span>ans<span class="token operator">+=</span>s<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>        <span class="token keyword">return</span> ans<span class="token punctuation">;</span>    <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 刷题笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DFS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PAT - [1148] Werewolf - Simple Version (20分)</title>
      <link href="/2020/03/11/pat-1148-werewolf-simple-version-20-fen/"/>
      <url>/2020/03/11/pat-1148-werewolf-simple-version-20-fen/</url>
      
        <content type="html"><![CDATA[<p>进行模拟，假定两个人为狼人看是否符合条件<br><span id="more"></span></p><h2 id="Vocabulary"><a href="#Vocabulary" class="headerlink" title="Vocabulary"></a>Vocabulary</h2><ol><li>ascending 升序的</li></ol><h2 id="自己的想法"><a href="#自己的想法" class="headerlink" title="自己的想法"></a>自己的想法</h2><p>进行模拟，假定两个人为狼人看是否符合条件<br>冷静可以解决一切问题！<br>思路没有问题，但是代码可以精简很多</p><h2 id="自己的代码"><a href="#自己的代码" class="headerlink" title="自己的代码"></a>自己的代码</h2><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include &lt;iostream&gt;#include &lt;vector&gt;#include &lt;algorithm&gt;#include &lt;set&gt;using namespace std;int main() &#123;    int n;    scanf(&quot;%d&quot;,&amp;n);    vector&lt;int&gt; in;    for(int i&#x3D;0;i&lt;n;++i)&#123;        int t;        scanf(&quot;%d&quot;,&amp;t);        in.emplace_back(t);    &#125;    int w1 &#x3D; 0,w2 &#x3D; 1;&#x2F;&#x2F;假设狼人为w1和w2    for(;;)&#123;        bool flag &#x3D; true;        int countL &#x3D; 0;        &#x2F;&#x2F;1.先判断两个狼人是否全说的真话，若全说的真话则为假，若全说的假话则为假        if(in[w1]&gt;0)&#123;            &#x2F;&#x2F;w1说某人是人，判断这个人是不是两个狼人之一，若是则为假话，否则为真话            if(in[w1]&#x3D;&#x3D;(w1+1)||in[w1]&#x3D;&#x3D;(w2+1)) ++countL;        &#125;else&#123;            &#x2F;&#x2F;w1说某人是狼人，如果这个人不是二者之一，则说的假话            if((-in[w1])!&#x3D;(w1+1)&amp;&amp;(-in[w1])!&#x3D;(w2+1)) ++countL;        &#125;        &#x2F;&#x2F;判断w2        if(in[w2]&gt;0)&#123;            if(in[w2]&#x3D;&#x3D;(w1+1)||in[w2]&#x3D;&#x3D;(w2+1)) ++countL;        &#125;else&#123;            if((-in[w2])!&#x3D;(w1+1)&amp;&amp;(-in[w2])!&#x3D;(w2+1)) ++countL;        &#125;        &#x2F;&#x2F;2.判断共有几个说谎的人，如果不为2则为假        if(countL&#x3D;&#x3D;0||countL&#x3D;&#x3D;2)&#123;            flag &#x3D; false;        &#125;else&#123;            &#x2F;&#x2F;剩下的人中必须只能有一个说谎的人            int count &#x3D; 0;            for(int j&#x3D;0;j&lt;n;++j)&#123;                if(j&#x3D;&#x3D;w1||j&#x3D;&#x3D;w2) continue;                if(in[j]&gt;0)&#123;                    &#x2F;&#x2F;这个人类说某人是人，若某人是w1或w2则为说谎                    if(in[j]&#x3D;&#x3D;(w1+1)||in[j]&#x3D;&#x3D;(w2+1)) ++count;                &#125;else&#123;                    &#x2F;&#x2F;这个人说某人是狼，若这个人不是w1且不是w2则为说谎                    if(-in[j]!&#x3D;(w1+1)&amp;&amp;-in[j]!&#x3D;(w2+1)) ++count;                &#125;                if(count&gt;1)&#123;                    flag &#x3D; false;                    break;                &#125;            &#125;            if(count&#x3D;&#x3D;0) flag &#x3D; false;        &#125;        if(!flag)&#123;            if(w1&#x3D;&#x3D;in.size()-2&amp;&amp;w2&#x3D;&#x3D;in.size()-1)&#123;                cout&lt;&lt;&quot;No Solution&quot;;                break;            &#125;else if(w2&#x3D;&#x3D;in.size()-1)&#123;                ++w1;                w2 &#x3D; w1+1;            &#125;else&#123;                ++w2;            &#125;        &#125;else&#123;            cout&lt;&lt;w1+1&lt;&lt;&quot; &quot;&lt;&lt;w2+1;            break;        &#125;    &#125;    return 0;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><p>每个人说的数字保存在v数组中，i从1～n、j从i+1～n遍历，分别假设i和j是狼人，a数组表示该人是狼人还是好人，等于1表示是好人，等于-1表示是狼人。k从1～n分别判断k所说的话是真是假，k说的话和真实情况不同（即v[k] * a[abs(v[k])] &lt; 0）则表示k在说谎，则将k放在lie数组中；遍历完成后判断lie数组，如果说谎人数等于2并且这两个说谎的人一个是好人一个是狼人（即a[lie[0]] + a[lie[1]] == 0）表示满足题意，此时输出i和j并return，否则最后的时候输出No Solution～</p><h2 id="代码优化"><a href="#代码优化" class="headerlink" title="代码优化"></a>代码优化</h2><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;&#x2F;代码链接 https:&#x2F;&#x2F;blog.csdn.net&#x2F;liuchuo&#x2F;article&#x2F;details&#x2F;82560876#include &lt;iostream&gt;#include &lt;vector&gt;#include &lt;cmath&gt;using namespace std;int main() &#123;    int n;    cin &gt;&gt; n;    vector&lt;int&gt; v(n+1);    for (int i &#x3D; 1; i &lt;&#x3D; n; i++) cin &gt;&gt; v[i];    for (int i &#x3D; 1; i &lt;&#x3D; n; i++) &#123;        for (int j &#x3D; i + 1; j &lt;&#x3D; n; j++) &#123;            vector&lt;int&gt; lie, a(n + 1, 1);            a[i] &#x3D; a[j] &#x3D; -1;            for (int k &#x3D; 1; k &lt;&#x3D; n; k++)                if (v[k] * a[abs(v[k])] &lt; 0) lie.push_back(k);            if (lie.size() &#x3D;&#x3D; 2 &amp;&amp; a[lie[0]] + a[lie[1]] &#x3D;&#x3D; 0) &#123;                cout &lt;&lt; i &lt;&lt; &quot; &quot; &lt;&lt; j;                return 0;            &#125;        &#125;    &#125;    cout &lt;&lt; &quot;No Solution&quot;;    return 0;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 刷题笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PAT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PAT - [1152] Google Recruitment (20分)</title>
      <link href="/2020/03/05/pat-1152-google-recruitment-20-fen/"/>
      <url>/2020/03/05/pat-1152-google-recruitment-20-fen/</url>
      
        <content type="html"><![CDATA[<p>本题使用暴力解的方法成功AC，本来担心会不会超时，但是并没有。只是实现的思路略复杂，也导致了在判断的时候遗漏了部分关键信息。<br><span id="more"></span></p><h2 id="Vocabulary"><a href="#Vocabulary" class="headerlink" title="Vocabulary"></a>Vocabulary</h2><ol><li>prime number 素数</li><li>consequence 连续的</li><li>transcendental number 超越数</li><li>in bold 黑体的，加粗的</li></ol><h2 id="个人解题思路"><a href="#个人解题思路" class="headerlink" title="个人解题思路"></a>个人解题思路</h2><p>本题使用暴力解的方法成功AC，本来担心会不会超时，但是并没有。只是实现的思路略复杂，也导致了在判断的时候遗漏了部分关键信息。</p><p>自己的复杂思路：采用枚举法，从左至右一依次寻找k位数，为了缩短时间，首先判断最后一位是否是偶数。在这里隐含了两种情况。第一种情况是是偶数，但这个数为2，2为素数；第二种情况是是除2以外的偶数，可以直接判断为非素数进入下一次的循环。<br>如果最后一位为奇数，且9位以下的数在int范围内，则将其该k位数转为int型，通过isPrime函数进行判断是否为素数，如果是素数则按位输出，谨防丢掉前面的几位0。<br>最后如果没有找到则输出404。</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include &lt;iostream&gt;#include &lt;algorithm&gt;#include &lt;vector&gt;#include &lt;cmath&gt;using namespace std;bool isPrime(int q)&#123;    bool flag &#x3D; true;    if(q&#x3D;&#x3D;1||q&#x3D;&#x3D;2)&#123;        return flag;    &#125;else&#123;        for(int i&#x3D;2;i&lt;&#x3D;sqrt(q);++i)&#123;            if(q%i&#x3D;&#x3D;0)&#123;                flag &#x3D; false;                break;            &#125;        &#125;        return flag;    &#125;&#125;int main() &#123;    int l,k;    scanf(&quot;%d%d&quot;,&amp;l,&amp;k);    char s[l];    scanf(&quot;%s&quot;,s);    bool f &#x3D; false;    for(int i&#x3D;0;i+k-1&lt;l;++i)&#123;        &#x2F;&#x2F;此种情况表示这个k位数为偶数，偶数不是素数，除2外        if((s[i+k-1]-&#39;0&#39;)%2&#x3D;&#x3D;0)&#123;            bool flag &#x3D; true;            &#x2F;&#x2F;如果不是00002，那么一定是偶数            if((s[i+k-1]-&#39;0&#39;)!&#x3D;2)&#123;                flag &#x3D; false;            &#125;else&#123;                &#x2F;&#x2F;如果flag为false，表示不是素数，为true表示是素数                for(int j &#x3D; i;j&lt;i+k-1;++j)&#123;                    if(s[j]!&#x3D;&#39;0&#39;)&#123;                        flag &#x3D; false;                        break;                    &#125;                &#125;            &#125;            if(flag)&#123;                f &#x3D; true;                for(int j &#x3D; i;j&lt;i+k;++j)&#123;                    cout&lt;&lt;s[j];                &#125;                break;            &#125;else&#123;                continue;&#x2F;&#x2F;向后寻找素数            &#125;        &#125;else&#123;&#x2F;&#x2F;非偶数的情况            int iP &#x3D; 0;            int su &#x3D; 1;            int t &#x3D; i+k-1;            while(t&gt;&#x3D;i)&#123;                iP +&#x3D; (s[t]-&#39;0&#39;)*su;                --t;                su*&#x3D;10;            &#125;            if(isPrime(iP))&#123;                f &#x3D;true;                for(int j &#x3D; i;j&lt;i+k;++j)&#123;                    cout&lt;&lt;s[j];                &#125;                break;            &#125;        &#125;    &#125;    if(!f)&#123;        cout&lt;&lt;&quot;404&quot;;    &#125;    return 0;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="代码优化"><a href="#代码优化" class="headerlink" title="代码优化"></a>代码优化</h2><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;&#x2F;代码链接 https:&#x2F;&#x2F;www.liuchuo.net&#x2F;archives&#x2F;category&#x2F;code&#x2F;page&#x2F;5#include &lt;iostream&gt;#include &lt;string&gt;using namespace std;bool isPrime(int n) &#123;    if (n &#x3D;&#x3D; 0 || n &#x3D;&#x3D; 1) return false;    for (int i &#x3D; 2; i * i &lt;&#x3D; n; i++)        if (n % i &#x3D;&#x3D; 0) return false;    return true;&#125;int main() &#123;    int l, k;    string s;    cin &gt;&gt; l &gt;&gt; k &gt;&gt; s;    for (int i &#x3D; 0; i &lt;&#x3D; l - k; i++) &#123;        string t &#x3D; s.substr(i, k);        int num &#x3D; stoi(t);        if (isPrime(num)) &#123;            cout &lt;&lt; t;            return 0;        &#125;    &#125;    cout &lt;&lt; &quot;404\n&quot;;    return 0;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>学习心得：<br>该题考虑了多种情况提高代码速度反而带来了更多的漏洞，可以先直接尝试暴力解是否得解，超时的话再考虑代码优化问题。</p><h3 id="C-stoi函数使用："><a href="#C-stoi函数使用：" class="headerlink" title="C++ stoi函数使用："></a>C++ stoi函数使用：</h3><p>引入头文件<br><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include &lt;string&gt;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><br>作用：将n进制的字符串转为十进制</p><p>用法</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;&#x2F;stoi（字符串，起始位置，n进制），将 n 进制的字符串转化为十进制stoi(str, 0, 2); &#x2F;&#x2F;将字符串 str 从 0 位置开始到末尾的 2 进制转换为十进制<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h3 id="C-substr函数使用"><a href="#C-substr函数使用" class="headerlink" title="C++ substr函数使用"></a>C++ substr函数使用</h3><p>引入头文件<br><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include &lt;string&gt;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><br>作用：返回一个从指定位置开始的指定长度的子字符串</p><p>用法</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;&#x2F;只有一个数字5表示从下标为5开始一直到结尾：sub1 &#x3D; &quot;56789&quot;string sub1 &#x3D; s.substr(5);&#x2F;&#x2F;从下标为5开始截取长度为3位：sub2 &#x3D; &quot;567&quot;string sub2 &#x3D; s.substr(5, 3); <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 刷题笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PAT </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
